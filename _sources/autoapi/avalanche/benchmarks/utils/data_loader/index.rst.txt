:mod:`avalanche.benchmarks.utils.data_loader`
=============================================

.. py:module:: avalanche.benchmarks.utils.data_loader


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.benchmarks.utils.data_loader.MultiTaskDataLoader
   avalanche.benchmarks.utils.data_loader.MultiTaskMultiBatchDataLoader
   avalanche.benchmarks.utils.data_loader.MultiTaskJoinedBatchDataLoader



.. py:class:: MultiTaskDataLoader(data_dict: Dict, oversample_small_tasks: bool = False, **kwargs)

   Custom data loader for multi-task training.
   The dictionary `data_dict` maps task ids into their
   corresponding datasets.

   When iterating over the data, it returns sequentially a different
   batch for each task (i.e. first a batch for task 1, then task 2,
   and so on). If `oversample_small_tasks == True` smaller tasks are
   oversampled to match the largest task.

   It is suggested to use this loader only if tasks have approximately the
   same length.

   :param data_dict: a dictionary with task ids as keys and Datasets
       as values.
   :param oversample_small_tasks: whether smaller tasks should be
       oversampled to match the largest one.
   :param kwargs: data loader arguments used to instantiate the loader for
       each task separately. See pytorch :class:`DataLoader`.

   .. method:: __iter__(self)


   .. method:: __len__(self)



.. py:class:: MultiTaskMultiBatchDataLoader(data_dict: Dict, oversample_small_tasks: bool = False, **kwargs)

   Custom data loader for multi-task training.
   The dictionary `data_dict` maps task ids into their
   corresponding datasets.

   mini-batches emitted by this dataloader are dictionaries with task
   labels as keys and mini-batches as values. Therefore, each mini-batch
   contains separate data for each task (i.e. key 1 batch for task 1).
   If `oversample_small_tasks == True` smaller tasks are oversampled to
   match the largest task.

   It is suggested to use this loader only if tasks have approximately the
   same length.

   :param data_dict: a dictionary with task ids as keys and Datasets
       as values.
   :param oversample_small_task: whether smaller tasks should be
       oversampled to match the largest one.
   :param kwargs: data loader arguments used to instantiate the loader for
       each task separately. See pytorch :class:`DataLoader`.

   .. method:: __iter__(self)


   .. method:: __len__(self)



.. py:class:: MultiTaskJoinedBatchDataLoader(data_dict: Dict, memory_dict: Dict = None, oversample_small_tasks: bool = False, batch_size: int = 32, **kwargs)

   Custom data loader for multi-task training.
   The dictionary `data_dict` maps task ids into their corresponding
   training datasets. 
   The dictionary `memory_dict` maps task ids into their corresponding
   datasets of memories. 

   When iterating over the data, it returns a single batch containing
   example from different tasks (i.e. a batch containing a balanced number
   of examples from all the tasks in the `data_dict` and `memory_dict`). 

   If `oversample_small_tasks == True` smaller tasks are oversampled to
   match the largest task.

   :param data_dict: a dictionary with task ids as keys and Datasets
       (training data) as values.
   :param memory_dict: a dictionary with task ids as keys and Datasets 
       (patterns in memory) as values.
   :param oversample_small_tasks: whether smaller tasks should be
       oversampled to match the largest one.
   :param batch_size: the size of the batch. It must be greater than or
       equal to the number of tasks.
   :param kwargs: data loader arguments used to instantiate the loader for
       each task separately. See pytorch :class:`DataLoader`.

   .. method:: __iter__(self)


   .. method:: __len__(self)


   .. method:: _get_mini_batch_from_data_dict(self, data_dict, iter_dataloaders, loaders_dict, oversample_small_tasks, mb_curr)


   .. method:: _create_dataloaders(self, data_dict, single_exp_batch_size, remaining_example, **kwargs)



