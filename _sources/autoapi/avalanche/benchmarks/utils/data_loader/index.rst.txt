:mod:`avalanche.benchmarks.utils.data_loader`
=============================================

.. py:module:: avalanche.benchmarks.utils.data_loader


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.benchmarks.utils.data_loader.MultiTaskDataLoader
   avalanche.benchmarks.utils.data_loader.MultiTaskMultiBatchDataLoader
   avalanche.benchmarks.utils.data_loader.MultiTaskJoinedBatchDataLoader



.. py:class:: MultiTaskDataLoader(data: AvalancheDataset, oversample_small_tasks: bool = False, **kwargs)

   Custom data loader for Avalanche's datasets.

   When iterating over the data, it returns sequentially a different
   batch for each task (i.e. first a batch for task 1, then task 2,
   and so on). If `oversample_small_tasks == True` smaller tasks are
   oversampled to match the largest task.

   It is suggested to use this loader only if tasks have approximately the
   same length.

   :param data: an instance of `AvalancheDataset`.
   :param oversample_small_tasks: whether smaller tasks should be
       oversampled to match the largest one.
   :param kwargs: data loader arguments used to instantiate the loader for
       each task separately. See pytorch :class:`DataLoader`.

   .. method:: __iter__(self)


   .. method:: __len__(self)



.. py:class:: MultiTaskMultiBatchDataLoader(data: AvalancheDataset, oversample_small_tasks: bool = False, **kwargs)

   Custom data loader for task-balanced multi-task training.

   Mini-batches emitted by this dataloader are dictionaries with task
   labels as keys and mini-batches as values. Therefore, each mini-batch
   contains separate data for each task (i.e. key 1 batch for task 1).
   If `oversample_small_tasks == True` smaller tasks are oversampled to
   match the largest task.

   :param data: an instance of `AvalancheDataset`.
   :param oversample_small_task: whether smaller tasks should be
       oversampled to match the largest one.
   :param kwargs: data loader arguments used to instantiate the loader for
       each task separately. See pytorch :class:`DataLoader`.

   .. method:: __iter__(self)


   .. method:: __len__(self)



.. py:class:: MultiTaskJoinedBatchDataLoader(data: AvalancheDataset, memory: AvalancheDataset = None, oversample_small_tasks: bool = False, batch_size: int = 32, **kwargs)

   Custom data loader for rehearsal strategies.

   The current experience `data` and rehearsal `memory` are used to create
   the mini-batches by concatenating them together. Each mini-batch
   contains examples from each task (i.e. a batch containing a balanced
   number of examples from all the tasks in the `data` and `memory`).

   If `oversample_small_tasks == True` smaller tasks are oversampled to
   match the largest task.

   :param data: a dictionary with task ids as keys and Datasets
       (training data) as values.
   :param memory: a dictionary with task ids as keys and Datasets
       (patterns in memory) as values.
   :param oversample_small_tasks: whether smaller tasks should be
       oversampled to match the largest one.
   :param batch_size: the size of the batch. It must be greater than or
       equal to the number of tasks.
   :param kwargs: data loader arguments used to instantiate the loader for
       each task separately. See pytorch :class:`DataLoader`.

   .. method:: __iter__(self)


   .. method:: __len__(self)


   .. method:: _get_mini_batch_from_data_dict(self, data, iter_dataloaders, loaders_dict, oversample_small_tasks, mb_curr)


   .. method:: _create_dataloaders(self, data_dict, single_exp_batch_size, remaining_example, **kwargs)



