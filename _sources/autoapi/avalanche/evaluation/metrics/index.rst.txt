:mod:`avalanche.evaluation.metrics`
===================================

.. py:module:: avalanche.evaluation.metrics


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   _any_event_metric/index.rst
   accuracy/index.rst
   confusion_matrix/index.rst
   cpu_usage/index.rst
   disk_usage/index.rst
   forgetting/index.rst
   gpu_usage/index.rst
   loss/index.rst
   mac/index.rst
   mean/index.rst
   ram_usage/index.rst
   timing/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.Mean
   avalanche.evaluation.metrics.Sum
   avalanche.evaluation.metrics.Accuracy
   avalanche.evaluation.metrics.MinibatchAccuracy
   avalanche.evaluation.metrics.EpochAccuracy
   avalanche.evaluation.metrics.RunningEpochAccuracy
   avalanche.evaluation.metrics.ExperienceAccuracy
   avalanche.evaluation.metrics.StreamAccuracy
   avalanche.evaluation.metrics.ConfusionMatrix
   avalanche.evaluation.metrics.StreamConfusionMatrix
   avalanche.evaluation.metrics.CPUUsage
   avalanche.evaluation.metrics.MinibatchCPUUsage
   avalanche.evaluation.metrics.EpochCPUUsage
   avalanche.evaluation.metrics.RunningEpochCPUUsage
   avalanche.evaluation.metrics.ExperienceCPUUsage
   avalanche.evaluation.metrics.StreamCPUUsage
   avalanche.evaluation.metrics.DiskUsage
   avalanche.evaluation.metrics.DiskUsageMonitor
   avalanche.evaluation.metrics.ExperienceForgetting
   avalanche.evaluation.metrics.GpuUsage
   avalanche.evaluation.metrics.GpuUsageMonitor
   avalanche.evaluation.metrics.Loss
   avalanche.evaluation.metrics.MinibatchLoss
   avalanche.evaluation.metrics.EpochLoss
   avalanche.evaluation.metrics.RunningEpochLoss
   avalanche.evaluation.metrics.ExperienceLoss
   avalanche.evaluation.metrics.StreamLoss
   avalanche.evaluation.metrics.MAC
   avalanche.evaluation.metrics.RamUsage
   avalanche.evaluation.metrics.RamUsageMonitor
   avalanche.evaluation.metrics.ElapsedTime
   avalanche.evaluation.metrics.MinibatchTime
   avalanche.evaluation.metrics.EpochTime
   avalanche.evaluation.metrics.RunningEpochTime
   avalanche.evaluation.metrics.ExperienceTime
   avalanche.evaluation.metrics.StreamTime



Functions
~~~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.accuracy_metrics
   avalanche.evaluation.metrics.cpu_usage_metrics
   avalanche.evaluation.metrics.loss_metrics
   avalanche.evaluation.metrics.timing_metrics


.. py:class:: Mean

   Bases: :class:`Metric[float]`

   The mean metric.

   This utility metric is a general purpose metric that can be used to keep
   track of the mean of a sequence of values.

   Creates an instance of the mean metric.

   This metric in its initial state will return a mean value of 0.
   The metric can be updated by using the `update` method while the mean
   can be retrieved using the `result` method.

   .. method:: update(self, value: SupportsFloat, weight: SupportsFloat = 1.0) -> None

      Update the running mean given the value.

      The value can be weighted with a custom value, defined by the `weight`
      parameter.

      :param value: The value to be used to update the mean.
      :param weight: The weight of the value. Defaults to 1.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the mean.

      Calling this method will not change the internal state of the metric.

      :return: The mean, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: Sum

   Bases: :class:`Metric[float]`

   The sum metric.

   This utility metric is a general purpose metric that can be used to keep
   track of the sum of a sequence of values.

   Beware that this metric only supports summing numbers and the result is
   always a float value, even when `update` is called by passing `int`s only.

   Creates an instance of the sum metric.

   This metric in its initial state will return a sum value of 0.
   The metric can be updated by using the `update` method while the sum
   can be retrieved using the `result` method.

   .. method:: update(self, value: SupportsFloat) -> None

      Update the running sum given the value.

      :param value: The value to be used to update the sum.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the sum.

      Calling this method will not change the internal state of the metric.

      :return: The sum, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: Accuracy

   Bases: :class:`Metric[float]`

   The Accuracy metric. This is a general metric
   used to compute more specific ones.

   Instances of this metric keeps the running average accuracy
   over multiple <prediction, target> pairs of Tensors,
   provided incrementally.
   The "prediction" and "target" tensors may contain plain labels or
   one-hot/logit vectors.

   Each time `result` is called, this metric emits the average accuracy
   across all predictions made since the last `reset`.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an accuracy value of 0.

   Creates an instance of the Accuracy metric.

   By default this metric in its initial state will return an accuracy
   value of 0. The metric can be updated by using the `update` method
   while the running accuracy can be retrieved using the `result` method.

   .. attribute:: _mean_accuracy
      

      The mean utility that will be used to store the running accuracy.


   .. method:: update(self, predicted_y: Tensor, true_y: Tensor) -> None

      Update the running accuracy given the true and predicted labels.

      :param predicted_y: The model prediction. Both labels and logit vectors
          are supported.
      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the running accuracy.

      Calling this method will not change the internal state of the metric.

      :return: The running accuracy, as a float value between 0 and 1.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchAccuracy

   Bases: :class:`PluginMetric[float]`

   The minibatch accuracy metric.
   This metric only works at training time.

   This metric computes the average accuracy over patterns
   from a single minibatch.
   It reports the result after each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochAccuracy` instead.

   Creates an instance of the MinibatchAccuracy metric.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: EpochAccuracy

   Bases: :class:`PluginMetric[float]`

   The average accuracy over a single training epoch.
   This metric only works at training time.

   The accuracy will be logged after each training epoch by computing
   the number of correctly predicted patterns during the epoch divided by
   the overall number of patterns encountered in that epoch.

   Creates an instance of the EpochAccuracy metric.

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: before_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: RunningEpochAccuracy

   Bases: :class:`avalanche.evaluation.metrics.accuracy.EpochAccuracy`

   The average accuracy across all minibatches up to the current
   epoch iteration.
   This metric only works at training time.

   At each iteration, this metric logs the accuracy averaged over all patterns
   seen so far in the current epoch.
   The metric resets its state after each training epoch.

   Creates an instance of the RunningEpochAccuracy metric.

   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy)


   .. method:: __str__(self)

      Return str(self).



.. py:class:: ExperienceAccuracy

   Bases: :class:`PluginMetric[float]`

   At the end of each experience, this metric reports
   the average accuracy over all patterns seen in that experience.
   This metric only works at eval time.

   Creates an instance of ExperienceAccuracy metric

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: before_eval_exp(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_exp(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StreamAccuracy

   Bases: :class:`PluginMetric[float]`

   At the end of the entire stream of experiences, this metric reports the
   average accuracy over all patterns seen in all experiences.
   This metric only works at eval time.

   Creates an instance of StreamAccuracy metric

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: before_eval(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. function:: accuracy_metrics(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of metric.

   :param minibatch: If True, will return a metric able to log
       the minibatch accuracy at training time.
   :param epoch: If True, will return a metric able to log
       the epoch accuracy at training time.
   :param epoch_running: If True, will return a metric able to log
       the running epoch accuracy at training time.
   :param experience: If True, will return a metric able to log
       the accuracy on each evaluation experience.
   :param stream: If True, will return a metric able to log
       the accuracy averaged over the entire evaluation stream of experiences.

   :return: A list of plugin metrics.


.. py:class:: ConfusionMatrix(num_classes: int = None)

   Bases: :class:`Metric[Tensor]`

   The confusion matrix metric.

   Instances of this metric keep track of the confusion matrix by receiving a
   pair of "ground truth" and "prediction" Tensors describing the labels of a
   minibatch. Those two tensors can both contain plain labels or
   one-hot/logit vectors.

   The result is the unnormalized running confusion matrix.

   Beware that by default the confusion matrix size will depend on the value of
   the maximum label as detected by looking at both the ground truth and
   predictions Tensors. When passing one-hot/logit vectors, this
   metric will try to infer the number of classes from the vector sizes.
   Otherwise, the maximum label value encountered in the truth/prediction
   Tensors will be used. It is recommended to set the (initial) number of
   classes in the constructor.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an empty Tensor.

   Creates an instance of the confusion matrix metric.

   By default this metric in its initial state will return an empty Tensor.
   The metric can be updated by using the `update` method while the running
   confusion matrix can be retrieved using the `result` method.

   :param num_classes: The initial number of classes. Defaults to None,
       which means that the number of classes will be inferred from
       ground truth and prediction Tensors (see class description for more
       details).

   .. attribute:: _cm_tensor
      :annotation: :Optional[Tensor]

      The Tensor where the running confusion matrix is stored.


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor) -> None

      Update the running confusion matrix given the true and predicted labels.

      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :param predicted_y: The ground truth. Both labels and logit vectors
          are supported.
      :return: None.


   .. method:: result(self) -> Tensor

      Retrieves the unnormalized confusion matrix.

      Calling this method will not change the internal state of the metric.

      :return: The running confusion matrix, as a Tensor.


   .. method:: reset(self) -> None

      Resets the metric.

      Calling this method will *not* reset the default number of classes
      optionally defined in the constructor optional parameter.

      :return: None.



.. py:class:: StreamConfusionMatrix(*, num_classes: Union[int, Mapping[int, int]] = None, normalize: Literal['true', 'pred', 'all'] = None, save_image: bool = True, image_creator: Callable[[Tensor], Image] = default_cm_image_creator)

   Bases: :class:`PluginMetric[Tensor]`

   The Stream Confusion Matrix metric.
   This metric only works on the eval phase.

   At the end of the eval phase, this metric logs the confusion matrix
   relative to all the patterns seen during eval.

   The metric can log either a Tensor or a PIL Image representing the
   confusion matrix.

   Creates an instance of the Stream Confusion Matrix metric.

   :param num_classes: When not None, is used to properly define the
       amount of rows/columns in the confusion matrix. When None, the
       matrix will have many rows/columns as the maximum value of the
       predicted and true pattern labels. Can be either an int, in which
       case the same value will be used across all experiences, or a
       dictionary defining the amount of classes for each experience (key =
        experience label, value = amount of classes). Defaults to None.
   :param normalize: Normalizes confusion matrix over the true (rows),
       predicted (columns) conditions or all the population. If None,
       confusion matrix will not be normalized. Valid values are: 'true',
       'pred' and 'all' or None.
   :param save_image: If True, a graphical representation of the confusion
       matrix will be logged, too. If False, only the Tensor representation
       will be logged. Defaults to True.
   :param image_creator: A callable that, given the tensor representation
       of the confusion matrix, returns a graphical representation of the
       matrix as a PIL Image. Defaults to `default_cm_image_creator`.

   .. method:: reset(self) -> None


   .. method:: result(self) -> Tensor


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor) -> None


   .. method:: before_eval(self, strategy) -> None


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _class_num_for_exp(self, exp_label: int) -> Optional[int]


   .. method:: _normalize_cm(cm: Tensor, normalization: Literal['true', 'pred', 'all'])
      :staticmethod:


   .. method:: nan_to_num(matrix: Tensor) -> Tensor
      :staticmethod:


   .. method:: __str__(self)

      Return str(self).



.. py:class:: CPUUsage

   Bases: :class:`Metric[float]`

   The CPU usage metric.

   Instances of this metric compute the average CPU usage as a float value.
   The metric starts tracking the CPU usage when the `update` method is called
   for the first time. That is, the tracking does not start at the time the
   constructor is invoked.

   Calling the `update` method more than twice will update the metric to the
   average usage between the first and the last call to `update`.

   The result, obtained using the `result` method, is the usage computed
   as stated above.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an usage value of 0.

   Creates an instance of the CPU usage metric.

   By default this metric in its initial state will return a CPU usage
   value of 0. The metric can be updated by using the `update` method
   while the average CPU usage can be retrieved using the `result` method.

   .. attribute:: _mean_usage
      

      The mean utility that will be used to store the average usage.


   .. attribute:: _process_handle
      :annotation: :Optional[Process]

      The process handle, lazily initialized.


   .. attribute:: _first_update
      :annotation: = True

      An internal flag to keep track of the first call to the `update` method.


   .. attribute:: _timer
      :annotation: :Callable[[], float]

      The timer implementation (aligned with the one used by psutil).


   .. method:: update(self) -> None

      Update the running CPU usage.

      For more info on how to set the starting moment see the class
      description.

      :return: None.


   .. method:: result(self) -> float

      Retrieves the average CPU usage.

      Calling this method will not change the internal state of the metric.

      :return: The average CPU usage, as a float value.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchCPUUsage

   Bases: :class:`PluginMetric[float]`

   The minibatch CPU usage metric.
   This metric only works at training time.

   This metric "logs" the CPU usage for each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochCPUUsage`.

   Creates an instance of the minibatch CPU usage metric.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: before_training_iteration(self, strategy) -> MetricResult


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: EpochCPUUsage

   Bases: :class:`PluginMetric[float]`

   The Epoch CPU usage metric.
   This metric only works at training time.

   The average usage will be logged after each epoch.

   Creates an instance of the epoch CPU usage metric.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: RunningEpochCPUUsage

   Bases: :class:`PluginMetric[float]`

   The running epoch CPU usage metric.
   This metric only works at training time

   After each iteration, the metric logs the average CPU usage up
   to the current epoch iteration.

   Creates an instance of the average epoch cpu usage metric.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: before_training_iteration(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: ExperienceCPUUsage

   Bases: :class:`PluginMetric[float]`

   The average experience CPU usage metric.
   This metric works only at eval time.

   After each experience, this metric emits the average CPU usage on that
   experienc.

   Creates an instance of the experience CPU usage metric.

   .. method:: before_eval_exp(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_exp(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StreamCPUUsage

   Bases: :class:`PluginMetric[float]`

   The average stream CPU usage metric.
   This metric works only at eval time.

   After the entire evaluation stream, this metric emits
   the average CPU usage on all experiences.

   Creates an instance of the stream CPU usage metric.

   .. method:: before_eval(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. function:: cpu_usage_metrics(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of metric.

   :param minibatch: If True, will return a metric able to log the minibatch
       CPU usage
   :param epoch: If True, will return a metric able to log the epoch
       CPU usage
   :param epoch_running: If True, will return a metric able to log the running
       epoch CPU usage.
   :param experience: If True, will return a metric able to log the experience
       CPU usage.
   :param stream: If True, will return a metric able to log the evaluation
       stream CPU usage.

   :return: A list of plugin metrics.


.. py:class:: DiskUsage(paths_to_monitor: Union[PathAlike, Sequence[PathAlike]] = None, monitor_disk_io: bool = False)

   Bases: :class:`Metric[DiskUsageResult]`

   The disk usage metric.

   This metric can be used to monitor the size of a set of directories. This
   can be useful to monitor the size of a replay buffer,

   This metric can also be used to get info regarding the overall amount of
   other system-wide disk stats (see the constructor for more details).

   Creates an instance of the disk usage metric.

   By default invoking the `result` method will return the sum of the size
   of the directories specified as the first parameter. By passing
   `monitor_disk_io` as true the `result` method will return a 5 elements
   tuple containing 1) the sum of the size of the directories,
   the system-wide 2) read count, 3) write count, 4) read bytes and
   5) written bytes.

   :param paths_to_monitor: a path or a list of paths to monitor. If None,
       the current working directory is used. Defaults to None.
   :param monitor_disk_io: If True enables monitoring of I/O operations on
       disk. WARNING: Reports are system-wide, grouping all disks. Defaults
       to False.

   .. method:: update(self)

      Updates the disk usage statistics.

      :return None.


   .. method:: result(self) -> Optional[DiskUsageResult]

      Retrieves the disk usage as computed during the last call to the
      `update` method.

      Calling this method will not change the internal state of the metric.

      The info returned may vary depending on whether the constructor was
      invoked with `monitor_disk_io` to True. See the constructor for more
      details.

      :return: The disk usage or None if `update` was not invoked yet.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.


   .. method:: get_dir_size(path: str)
      :staticmethod:



.. py:class:: DiskUsageMonitor(*paths: PathAlike, timeout: float = 5.0, train=True, eval=False)

   Bases: :class:`AnyEventMetric[float]`

   The disk usage metric.

   This metric logs the disk usage (directory size) of the given list of paths.

   The logged value is in MiB.

   The metric can be either configured to log after a certain timeout or
   at each event.

   Disk usage is logged separately for the train and eval phases.

   Creates an instance of the disk usage metric.

   The train and eval parameters can be True at the same time. However,
   at least one of them must be True.

   :param paths: A list of paths to monitor. If no paths are defined,
       the current working directory is used.
   :param timeout: The timeout between each disk usage check, in seconds.
       If None, the disk usage is checked at every possible event (not
       recommended). Defaults to 5 seconds.
   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to False.

   .. method:: on_event(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: result(self) -> Optional[float]


   .. method:: reset(self) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy) -> 'MetricResult'



.. py:class:: ExperienceForgetting

   Bases: :class:`PluginMetric[Dict[int, float]]`

   The Forgetting metric, describing the accuracy loss detected for a
   certain experience.

   This metric, computed separately for each experience,
   is the difference between the accuracy result obtained after
   first training on a experience and the accuracy result obtained
   on the same experience at the end of successive experiences.

   This metric is computed during the eval phase only.

   Creates an instance of the ExperienceForgetting metric.

   .. attribute:: _initial_accuracy
      :annotation: :Dict[int, float]

      The initial accuracy of each experience.


   .. attribute:: _current_accuracy
      :annotation: :Dict[int, Accuracy]

      The current accuracy of each experience.


   .. attribute:: eval_exp_id
      

      The current evaluation experience id


   .. method:: reset(self) -> None

      Resets the metric.

      Beware that this will also reset the initial accuracy of each
      experience!

      :return: None.


   .. method:: reset_current_accuracy(self) -> None

      Resets the current accuracy.

      This will preserve the initial accuracy value of each experience.
      To be used at the beginning of each eval experience.

      :return: None.


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor, label: int) -> None

      Updates the running accuracy of a experience given the ground truth and
      predicted labels of a minibatch.

      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :param predicted_y: The ground truth. Both labels and logit vectors
          are supported.
      :param label: The experience label.
      :return: None.


   .. method:: before_eval(self, strategy) -> None


   .. method:: before_eval_exp(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_exp(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: result(self) -> float

      Return the amount of forgetting for the eval experience
      associated to `eval_label`.

      The forgetting is computed as the accuracy difference between the
      initial experience accuracy (when first encountered
      in the training stream) and the current accuracy.
      A positive value means that forgetting occurred. A negative value
      means that the accuracy on that experience increased.

      :param eval_label: integer label describing the eval experience
              of which measuring the forgetting
      :return: The amount of forgetting on `eval_exp` experience
               (as float in range [-1, 1]).


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: GpuUsage(gpu_id, every=2.0)

   GPU usage metric measured as average usage percentage over time.

   This metric will actively poll the system to get the GPU usage over time
   starting from the first call to `update`. Subsequent calls to the `update`
   method will consolidate the values gathered since the last call.

   The `result` method will return `None` until the `update` method is invoked
   at least two times.

   Invoking the `reset` method will stop the measurement and reset the metric
   to its initial state.

   Creates an instance of the GPU usage metric.

   For more info about the usage see the class description.

   :param gpu_id: GPU device ID.
   :param every: time delay (in seconds) between measurements.

   .. attribute:: MAX_BUFFER
      :annotation: = 10000

      

   .. attribute:: SMI_NOT_FOUND_MSG
      :annotation: = No GPU available: nvidia-smi command not found. Gpu Usage logging will be disabled.

      

   .. method:: update(self) -> None

      Consolidates the values got from the GPU sensor.

      This will store the average for retrieval through the `update` method.

      The previously consolidated value will be discarded.

      :return: None


   .. method:: _start_watch(self)


   .. method:: _push_lines(self) -> None


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.


   .. method:: result(self) -> Optional[float]

      Returns the last consolidated GPU usage value.

      For more info about the returned value see the class description.

      :return: The percentage GPU usage as a float value in range [0, 1].
          Returns None if the `update` method was invoked less than twice.


   .. method:: gpu_found(self) -> bool

      Checks if nvidia-smi could me executed.

      This method is experimental. Please use at you own risk.

      :return: True if nvidia-smi could be launched, False otherwise.



.. py:class:: GpuUsageMonitor(gpu_id: int, *, timeout: int = 2, train=True, eval=False)

   Bases: :class:`AnyEventMetric[float]`

   The GPU usage metric.

   This metric logs the percentage GPU usage.

   The metric can be either configured to log after a certain timeout or
   at each event.

   GPU usage is logged separately for the train and eval phases.

   Creates an instance of the GPU usage metric.

   The train and eval parameters can be True at the same time. However,
   at least one of them must be True.

   :param gpu_id: The GPU to monitor.
   :param timeout: The timeout between each GPU usage log, in seconds.
        Defaults to 2 seconds. Must be an int.
   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to False.

   .. method:: on_event(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: before_training(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: before_eval(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: result(self) -> Optional[float]


   .. method:: reset(self) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy) -> 'MetricResult'



.. py:class:: Loss

   Bases: :class:`Metric[float]`

   The Loss metric. This is a general metric
   used to compute more specific ones.

   Instances of this metric keeps the running average loss
   over multiple <prediction, target> pairs of Tensors,
   provided incrementally.
   The "prediction" and "target" tensors may contain plain labels or
   one-hot/logit vectors.

   Each time `result` is called, this metric emits the average loss
   across all predictions made since the last `reset`.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return a loss value of 0.

   Creates an instance of the loss metric.

   By default this metric in its initial state will return a loss
   value of 0. The metric can be updated by using the `update` method
   while the running loss can be retrieved using the `result` method.

   .. method:: update(self, loss: Tensor, patterns: int) -> None

      Update the running loss given the loss Tensor and the minibatch size.

      :param loss: The loss Tensor. Different reduction types don't affect
          the result.
      :param patterns: The number of patterns in the minibatch.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the running average loss per pattern.

      Calling this method will not change the internal state of the metric.

      :return: The running loss, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchLoss

   Bases: :class:`PluginMetric[float]`

   The minibatch loss metric.
   This metric only works at training time.

   This metric computes the average loss over patterns
   from a single minibatch.
   It reports the result after each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochLoss` instead.

   Creates an instance of the MinibatchLoss metric.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: EpochLoss

   Bases: :class:`PluginMetric[float]`

   The average loss over a single training epoch.
   This metric only works at training time.

   The loss will be logged after each training epoch by computing
   the loss on the predicted patterns during the epoch divided by
   the overall number of patterns encountered in that epoch.

   Creates an instance of the EpochLoss metric.

   .. method:: before_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: RunningEpochLoss

   Bases: :class:`avalanche.evaluation.metrics.loss.EpochLoss`

   The average loss across all minibatches up to the current
   epoch iteration.
   This metric only works at training time.

   At each iteration, this metric logs the loss averaged over all patterns
   seen so far in the current epoch.
   The metric resets its state after each training epoch.

   Creates an instance of the RunningEpochLoss metric.

   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: ExperienceLoss

   Bases: :class:`PluginMetric[float]`

   At the end of each experience, this metric reports
   the average loss over all patterns seen in that experience.
   This metric only works at eval time.

   Creates an instance of ExperienceLoss metric

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: before_eval_exp(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_exp(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StreamLoss

   Bases: :class:`PluginMetric[float]`

   At the end of the entire stream of experiences, this metric reports the
   average loss over all patterns seen in all experiences.
   This metric only works at eval time.

   Creates an instance of StreamLoss metric

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: before_eval(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. function:: loss_metrics(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of metric.

   :param minibatch: If True, will return a metric able to log
       the minibatch loss at training time.
   :param epoch: If True, will return a metric able to log
       the epoch loss at training time.
   :param epoch_running: If True, will return a metric able to log
       the running epoch loss at training time.
   :param experience: If True, will return a metric able to log
       the loss on each evaluation experience.
   :param stream: If True, will return a metric able to log
       the loss averaged over the entire evaluation stream of experiences.

   :return: A list of plugin metrics.


.. py:class:: MAC

   Bases: :class:`Metric[int]`

   Multiply-and-accumulate metric. Provides a lower bound of the
   computational cost of a model in a hardware-independent way by
   computing the number of multiplications. Currently supports only
   Linear or Conv2d modules. Other operations are ignored.

   Creates an instance of the MAC metric.

   .. method:: update(self, model: Module, dummy_input: Tensor)

      Computes the MAC metric.

      :param model: current model.
      :param dummy_input: A tensor of the correct size to feed as input
          to model.
      :return: MAC metric.


   .. method:: result(self) -> Optional[int]

      Return the number of MAC operations as computed in the previous call
      to the `update` method.

      :return: The number of MAC operations or None if `update` has not been
          called yet.


   .. method:: update_compute_cost(self, module, dummy_input, output)


   .. method:: is_recognized_module(mod)
      :staticmethod:



.. py:class:: RamUsage(two_read_average=False)

   Bases: :class:`Metric[float]`

   The RAM usage metric.

   Instances of this metric compute the punctual RAM usage as a float value.
   The metric updates the value each time the `update` method is called.

   The result, obtained using the `result` method, is the usage in bytes.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an usage value of `None`.

   Creates an instance of the RAM usage metric.

   By default this metric in its initial state will return a RAM usage
   value of `None`. The metric can be updated by using the `update` method
   while the average usage value can be retrieved using the `result`
   method.

   :param two_read_average: If True, the value resulting from calling
       `update` more than once will set the result to the average between
       the last read and the current RAM usage value.

   .. attribute:: _process_handle
      :annotation: :Optional[Process]

      The process handle, lazily initialized.


   .. attribute:: _last_values
      

      The last detected RAM usage.


   .. attribute:: _first_update
      :annotation: = True

      An internal flag to keep track of the first call to the `update` method.


   .. attribute:: _two_read_average
      

      If True, the value resulting from calling `update` more than once will
      set the result to the average between the last read and the current RAM
      usage value.


   .. method:: update(self) -> None

      Update the RAM usage.

      :return: None.


   .. method:: result(self) -> Optional[float]

      Retrieves the RAM usage.

      Calling this method will not change the internal state of the metric.

      :return: The average RAM usage in bytes, as a float value.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: RamUsageMonitor(*, timeout: float = 5.0, train=True, eval=False)

   Bases: :class:`AnyEventMetric[float]`

   The RAM usage metric.

   This metric logs the RAM usage.

   The logged value is in MiB.

   The metric can be either configured to log after a certain timeout or
   at each event.

   RAM usage is logged separately for the train and eval phases.

   Creates an instance of the RAM usage metric.

   The train and eval parameters can be True at the same time. However,
   at least one of them must be True.

   :param timeout: The timeout between each RAM usage check, in seconds.
       If None, the RAM usage is checked at every possible event (not
       recommended). Defaults to 5 seconds.
   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to False.

   .. method:: on_event(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: result(self) -> Optional[float]


   .. method:: reset(self) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy) -> 'MetricResult'



.. py:class:: ElapsedTime

   Bases: :class:`Metric[float]`

   The elapsed time metric.

   Instances of this metric keep track of the time elapsed between calls to the
   `update` method. The starting time is set when the `update` method is called
   for the first time. That is, the starting time is *not* taken at the time
   the constructor is invoked.

   Calling the `update` method more than twice will update the metric to the
   elapsed time between the first and the last call to `update`.

   The result, obtained using the `result` method, is the time, in seconds,
   computed as stated above.

   The `reset` method will set the metric to its initial state, thus resetting
   the initial time. This metric in its initial state (or if the `update`
   method was invoked only once) will return an elapsed time of 0.

   Creates an instance of the accuracy metric.

   This metric in its initial state (or if the `update` method was invoked
   only once) will return an elapsed time of 0. The metric can be updated
   by using the `update` method while the running accuracy can be retrieved
   using the `result` method.

   .. method:: update(self) -> None

      Update the elapsed time.

      For more info on how to set the initial time see the class description.

      :return: None.


   .. method:: result(self) -> float

      Retrieves the elapsed time.

      Calling this method will not change the internal state of the metric.

      :return: The elapsed time, in seconds, as a float value.


   .. method:: reset(self) -> None

      Resets the metric, including the initial time.

      :return: None.



.. py:class:: MinibatchTime

   Bases: :class:`PluginMetric[float]`

   The minibatch time metric.
   This metric only works at training time.

   This metric "logs" the elapsed time for each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochTime`.

   Creates an instance of the minibatch time metric.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: before_training_iteration(self, strategy) -> MetricResult


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: EpochTime

   Bases: :class:`PluginMetric[float]`

   The epoch elapsed time metric.
   This metric only works at training time.

   The elapsed time will be logged after each epoch.

   Creates an instance of the epoch time metric.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: RunningEpochTime

   Bases: :class:`PluginMetric[float]`

   The running epoch time metric.
   This metric only works at training time.

   For each iteration, this metric logs the average time
   between the start of the
   epoch and the current iteration.

   Creates an instance of the running epoch time metric..

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: before_training_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: ExperienceTime

   Bases: :class:`PluginMetric[float]`

   The experience time metric.
   This metric only works at eval time.

   After each experience, this metric emits the average time of that
   experience.

   Creates an instance of the experience time metric.

   .. method:: before_eval_exp(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_eval_exp(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StreamTime

   Bases: :class:`PluginMetric[float]`

   The stream time metric.
   This metric only works at eval time.

   After the entire evaluation stream,
   this metric emits the average time of that stream.

   Creates an instance of the stream time metric.

   .. method:: before_eval(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_eval(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. function:: timing_metrics(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of metric.

   :param minibatch: If True, will return a metric able to log the train
       minibatch elapsed time.
   :param epoch: If True, will return a metric able to log the train epoch
       elapsed time.
   :param epoch_running: If True, will return a metric able to log the running
       train epoch elapsed time.
   :param experience: If True, will return a metric able to log the eval
       experience elapsed time.
   :param stream: If True, will return a metric able to log the eval stream
       elapsed time.

   :return: A list of plugin metrics.


