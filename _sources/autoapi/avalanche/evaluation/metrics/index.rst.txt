:mod:`avalanche.evaluation.metrics`
===================================

.. py:module:: avalanche.evaluation.metrics


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   accuracy/index.rst
   confusion_matrix/index.rst
   forgetting/index.rst
   loss/index.rst
   mean/index.rst
   sum/index.rst
   timing/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.Accuracy
   avalanche.evaluation.metrics.MinibatchAccuracy
   avalanche.evaluation.metrics.EpochAccuracy
   avalanche.evaluation.metrics.RunningEpochAccuracy
   avalanche.evaluation.metrics.TaskAccuracy
   avalanche.evaluation.metrics.ConfusionMatrix
   avalanche.evaluation.metrics.TaskConfusionMatrix
   avalanche.evaluation.metrics.TaskForgetting
   avalanche.evaluation.metrics.Loss
   avalanche.evaluation.metrics.MinibatchLoss
   avalanche.evaluation.metrics.EpochLoss
   avalanche.evaluation.metrics.RunningEpochLoss
   avalanche.evaluation.metrics.TaskLoss
   avalanche.evaluation.metrics.Mean
   avalanche.evaluation.metrics.Sum
   avalanche.evaluation.metrics.ElapsedTime
   avalanche.evaluation.metrics.MinibatchTime
   avalanche.evaluation.metrics.EpochTime
   avalanche.evaluation.metrics.AverageEpochTime
   avalanche.evaluation.metrics.StepTime



.. py:class:: Accuracy

   Bases: :class:`Metric[float]`

   The accuracy metric.

   Instances of this metric compute the average accuracy by receiving a pair
   of "ground truth" and "prediction" Tensors describing the labels of a
   minibatch. Those two tensors can both contain plain labels or
   one-hot/logit vectors.

   The result is the running accuracy computed as the number of correct
   patterns divided by the overall amount of patterns.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an accuracy value of 0.

   Creates an instance of the accuracy metric.

   By default this metric in its initial state will return an accuracy
   value of 0. The metric can be updated by using the `update` method
   while the running accuracy can be retrieved using the `result` method.

   .. attribute:: _mean_accuracy
      

      The mean utility that will be used to store the running accuracy.


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor) -> None

      Update the running accuracy given the true and predicted labels.

      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :param predicted_y: The ground truth. Both labels and logit vectors
          are supported.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the running accuracy.

      Calling this method will not change the internal state of the metric.

      :return: The running accuracy, as a float value between 0 and 1.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchAccuracy(*, train=True, test=False)

   Bases: :class:`PluginMetric[float]`

   The minibatch accuracy metric.

   This metric "logs" the accuracy value after each iteration. Beware that this
   metric will not average the accuracy across minibatches!

   If a more coarse-grained logging is needed, consider using
   :class:`EpochAccuracy` and/or :class:`TaskAccuracy` instead.

   Creates an instance of the MinibatchAccuracy metric.

   The train and test parameters are used to control if this metric should
   compute and log values referred to the train phase, test phase or both.
   At least one of them must be True!

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _on_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: EpochAccuracy(*, train=True, test=False)

   Bases: :class:`PluginMetric[float]`

   The average epoch accuracy metric.

   The accuracy will be logged after each epoch by computing the accuracy
   as the number of correctly predicted patterns divided by the overall
   number of patterns encountered in that epoch, which means that having
   unbalanced minibatch sizes will not affect the metric.

   Creates an instance of the EpochAccuracy metric.

   The train and test parameters are used to control if this metric should
   compute and log values referred to the train phase, test phase or both.
   At least one of them must be True!

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: before_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: before_test_step(self, strategy: PluggableStrategy) -> None


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: RunningEpochAccuracy(*, train=True, test=False)

   Bases: :class:`avalanche.evaluation.metrics.accuracy.EpochAccuracy`

   The running average accuracy metric.

   This metric behaves like :class:`EpochAccuracy` but, differently from it,
   this metric will log the running accuracy value after each iteration.

   Creates an instance of the RunningEpochAccuracy metric.

   The train and test parameters are used to control if this metric should
   compute and log values referred to the train phase, test phase or both.
   At least one of them must be True!

   Beware that the test parameter defaults to False because logging
   the running test accuracy it's and uncommon practice.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy)



.. py:class:: TaskAccuracy

   Bases: :class:`PluginMetric[Dict[int, float]]`

   The task accuracy metric.

   This is the most common metric used in the evaluation of a Continual
   Learning algorithm.

   Can be safely used when evaluation task-free scenarios, in which case the
   default task label "0" will be used.

   The task accuracies will be logged at the end of the test phase. This metric
   doesn't apply to the training phase.

   Creates an instance of the TaskAccuracy metric.

   .. attribute:: _task_accuracy
      :annotation: :Dict[int, Accuracy]

      A dictionary used to store the accuracy for each task.


   .. method:: reset(self) -> None


   .. method:: result(self) -> Dict[int, float]


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor, task_label: int) -> None


   .. method:: before_test(self, strategy) -> None


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_test(self, strategy) -> MetricResult


   .. method:: _package_result(self) -> MetricResult



.. py:class:: ConfusionMatrix(num_classes: int = None)

   Bases: :class:`Metric[Tensor]`

   The confusion matrix metric.

   Instances of this metric keep track of the confusion matrix by receiving a
   pair of "ground truth" and "prediction" Tensors describing the labels of a
   minibatch. Those two tensors can both contain plain labels or
   one-hot/logit vectors.

   The result is the unnormalized running confusion matrix.

   Beware that by default the confusion matrix size will depend on the value of
   the maximum label as detected by looking at both the ground truth and
   predictions Tensors. When passing one-hot/logit vectors, this
   metric will try to infer the number of classes from the vector sizes.
   Otherwise, the maximum label value encountered in the truth/prediction
   Tensors will be used. It is recommended to set the (initial) number of
   classes in the constructor.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an empty Tensor.

   Creates an instance of the confusion matrix metric.

   By default this metric in its initial state will return an empty Tensor.
   The metric can be updated by using the `update` method while the running
   confusion matrix can be retrieved using the `result` method.

   :param num_classes: The initial number of classes. Defaults to None,
       which means that the number of classes will be inferred from
       ground truth and prediction Tensors (see class description for more
       details).

   .. attribute:: _cm_tensor
      :annotation: :Optional[Tensor]

      The Tensor where the running confusion matrix is stored.


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor) -> None

      Update the running confusion matrix given the true and predicted labels.

      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :param predicted_y: The ground truth. Both labels and logit vectors
          are supported.
      :return: None.


   .. method:: result(self) -> Tensor

      Retrieves the unnormalized confusion matrix.

      Calling this method will not change the internal state of the metric.

      :return: The running confusion matrix, as a Tensor.


   .. method:: reset(self) -> None

      Resets the metric.

      Calling this method will *not* reset the default number of classes
      optionally defined in the constructor optional parameter.

      :return: None.



.. py:class:: TaskConfusionMatrix(*, train: bool = False, test: bool = True, num_classes: Union[int, Mapping[int, int]] = None, normalize: Literal['true', 'pred', 'all'] = None, save_image: bool = True, image_creator: Callable[[Tensor], Image] = default_cm_image_creator)

   Bases: :class:`PluginMetric[Tensor]`

   The Confusion Matrix metric.

   This metric logs the confusion matrix for each task at the end of
   each phase. By default this metric computes the matrix on the test phase
   (on the test set) only but this behaviour can be changed by passing
   `train=True` in the constructor.

   The metric will log both a Tensor and PIL Image both representing the
   confusion matrices. The Logger will decide which one to use depending on its
   internal implementation.

   Creates an instance of the Confusion Matrix metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to False.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to True.
   :param num_classes: When not None, is used to properly define the
       amount of rows/columns in the confusion matrix. When None, the
       matrix will have many rows/columns as the maximum value of the
       predicted and true pattern labels. Can be either an int, in which
       case the same value will be used across all tasks, or a dictionary
       defining the amount of classes for each task (key = task label,
       value = amount of classes). Defaults to None.
   :param normalize: Normalizes confusion matrix over the true (rows),
       predicted (columns) conditions or all the population. If None,
       confusion matrix will not be normalized. Valid values are: 'true',
       'pred' and 'all'.
   :param save_image: If True, a graphical representation of the confusion
       matrix will be logged, too. If False, only the Tensor representation
       will be logged. Defaults to True.
   :param image_creator: A callable that, given the tensor representation
       of the confusion matrix, returns a graphical representation of the
       matrix as a PIL Image. Defaults to `default_cm_image_creator`.

   .. method:: reset(self) -> None


   .. method:: result(self) -> Dict[int, Tensor]


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor, task_label: int) -> None


   .. method:: before_training(self, strategy) -> None


   .. method:: before_test(self, strategy) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_training(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: after_test(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _class_num_for_task(self, task_label: int) -> Optional[int]


   .. method:: _normalize_cm(cm: Tensor, normalization: Literal['true', 'pred', 'all'])
      :staticmethod:


   .. method:: nan_to_num(matrix: Tensor) -> Tensor
      :staticmethod:



.. py:class:: TaskForgetting

   Bases: :class:`PluginMetric[Dict[int, float]]`

   The TaskForgetting metric, describing the accuracy loss detected for a
   certain task.

   This metric, computed separately for each task, is the difference between
   the accuracy result obtained after first training on a task and the accuracy
   result obtained on the same task at the end of successive steps.

   This metric is computed during the test phase only.

   Creates an instance of the Catastrophic TaskForgetting metric.

   .. attribute:: _initial_task_accuracy
      :annotation: :Dict[int, float]

      The initial accuracy of each task.


   .. attribute:: _current_task_accuracy
      :annotation: :Dict[int, Accuracy]

      The current accuracy of each task.


   .. method:: reset(self) -> None

      Resets the metric.

      Beware that this will also reset the initial accuracy of each task!

      :return: None.


   .. method:: reset_current_accuracy(self) -> None

      Resets the current accuracy.

      This will preserve the initial accuracy value of each task. To be used
      at the beginning of each test step.

      :return: None.


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor, task_label: int) -> None

      Updates the running accuracy of a task given the ground truth and
      predicted labels of a minibatch.

      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :param predicted_y: The ground truth. Both labels and logit vectors
          are supported.
      :param task_label: The task label.
      :return: None.


   .. method:: before_test(self, strategy) -> None


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_test(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: result(self) -> Dict[int, float]

      Return the amount of forgetting for each task.

      The forgetting is computed as the accuracy difference between the
      initial task accuracy (when first encountered in the training stream)
      and the current accuracy. A positive value means that forgetting
      occurred. A negative value means that the accuracy on that task
      increased.

      :return: A dictionary in which keys are task labels and the values are
          the forgetting measures (as floats in range [-1, 1]).


   .. method:: _package_result(self, train_task: int) -> MetricResult



.. py:class:: Loss

   Bases: :class:`Metric[float]`

   The average loss metric.

   Instances of this metric compute the running average loss by receiving a
   Tensor describing the loss of a minibatch. This metric then uses that tensor
   to computes the average loss per pattern.

   The Tensor passed to the `update` method are averaged to obtain a
   minibatch average loss. In order to compute the per-pattern running loss,
   the users should must pass the number of patterns in that minibatch as the
   second parameter of the `update` method. The number of patterns can't be
   usually obtained by analyzing the shape of the loss Tensor, which usually
   consists of a single float value.

   The result is the running loss computed as the accumulated average loss.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an accuracy value of 0.

   Creates an instance of the loss metric.

   By default this metric in its initial state will return a loss
   value of 0. The metric can be updated by using the `update` method
   while the running loss can be retrieved using the `result` method.

   .. method:: update(self, loss: Tensor, patterns: int) -> None

      Update the running loss given the loss Tensor and the minibatch size.

      :param loss: The loss Tensor. Different reduction types don't affect
          the result.
      :param patterns: The number of patterns in the minibatch.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the running average loss per pattern.

      Calling this method will not change the internal state of the metric.

      :return: The running loss, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchLoss(*, train=True, test=True)

   Bases: :class:`PluginMetric[float]`

   The minibatch loss metric.

   The logged loss value is the per-pattern loss obtained by averaging the loss
   of patterns contained in the minibatch.

   This metric "logs" the loss value after each iteration. Beware that this
   metric will not average the loss across minibatches!

   If a more coarse-grained logging is needed, consider using
   :class:`EpochLoss` and/or :class:`TaskLoss` instead.

   Creates an instance of the MinibatchLoss metric.

   The train and test parameters are used to control if this metric should
   compute and log values referred to the train phase, test phase or both.
   At least one of them must be True!

   Beware that the test parameter defaults to False because logging
   the test minibatch loss it's and uncommon practice.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _on_iteration(self, strategy: PluggableStrategy)


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: EpochLoss(*, train=True, test=True)

   Bases: :class:`PluginMetric[float]`

   The average epoch loss metric.

   The logged loss value is the per-pattern loss obtained by averaging the loss
   of all patterns encountered in that epoch, which means that having
   unbalanced minibatch sizes will not affect the metric.

   Creates an instance of the EpochLoss metric.

   The train and test parameters are used to control if this metric should
   compute and log values referred to the train phase, test phase or both.
   At least one of them must be True!

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to True.

   .. method:: before_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: before_test_step(self, strategy: PluggableStrategy) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: RunningEpochLoss(*, train=True, test=True)

   Bases: :class:`avalanche.evaluation.metrics.loss.EpochLoss`

   The running average loss metric.

   This metric behaves like :class:`EpochLoss` but, differently from it,
   this metric will log the running loss value after each iteration.

   Creates an instance of the RunningEpochLoss metric.

   The train and test parameters are used to control if this metric should
   compute and log values referred to the train phase, test phase or both.
   At least one of them must be True!

   Beware that the test parameter defaults to False because logging
   the running test accuracy it's and uncommon practice.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: TaskLoss

   Bases: :class:`PluginMetric[Dict[int, float]]`

   The task loss metric.

   The logged loss value is the per-pattern loss obtained by averaging the loss
   of all test patterns of a task. This is a common metric used in the
   evaluation of a Continual Learning algorithm.

   Can be safely used when evaluation task-free scenarios, in which case the
   default task label "0" will be used.

   The task losses will be logged at the end of the test phase. This metric
   doesn't apply to the training phase.

   Creates an instance of the TaskLoss metric.

   .. attribute:: _task_loss
      :annotation: :Dict[int, Loss]

      A dictionary used to store the loss for each task.


   .. method:: reset(self) -> None


   .. method:: result(self) -> Dict[int, float]


   .. method:: update(self, loss: Tensor, patterns: int, task_label: int) -> None


   .. method:: before_test(self, strategy) -> None


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_test(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self) -> MetricResult



.. py:class:: Mean

   Bases: :class:`Metric[float]`

   The mean metric.

   This utility metric is a general purpose metric that can be used to keep
   track of the mean of a sequence of values.

   Creates an instance of the mean metric.

   This metric in its initial state will return a mean value of 0.
   The metric can be updated by using the `update` method while the mean
   can be retrieved using the `result` method.

   .. method:: update(self, value: SupportsFloat, weight: SupportsFloat = 1.0) -> None

      Update the running mean given the value.

      The value can be weighted with a custom value, defined by the `weight`
      parameter.

      :param value: The value to be used to update the mean.
      :param weight: The weight of the value. Defaults to 1.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the mean.

      Calling this method will not change the internal state of the metric.

      :return: The mean, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: Sum

   Bases: :class:`Metric[float]`

   The sum metric.

   This utility metric is a general purpose metric that can be used to keep
   track of the sum of a sequence of values.

   Beware that this metric only supports summing numbers and the result is
   always a float value, even when `update` is called by passing `int`s only.

   Creates an instance of the sum metric.

   This metric in its initial state will return a sum value of 0.
   The metric can be updated by using the `update` method while the sum
   can be retrieved using the `result` method.

   .. method:: update(self, value: SupportsFloat) -> None

      Update the running sum given the value.

      :param value: The value to be used to update the sum.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the sum.

      Calling this method will not change the internal state of the metric.

      :return: The sum, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: ElapsedTime

   Bases: :class:`Metric[float]`

   The elapsed time metric.

   Instances of this metric keep track of the time elapsed between calls to the
   `update` method. The starting time is set when the `update` method is called
   for the first time. That is, the starting time is *not* taken at the time
   the constructor is invoked.

   Calling the `update` method more than twice will update the metric to the
   elapsed time between the first and the last call to `update`.

   The result, obtained using the `result` method, is the time, in seconds,
   computed as stated above.

   The `reset` method will set the metric to its initial state, thus resetting
   the initial time. This metric in its initial state (or if the `update`
   method was invoked only once) will return an elapsed time of 0.

   Creates an instance of the accuracy metric.

   This metric in its initial state (or if the `update` method was invoked
   only once) will return an elapsed time of 0. The metric can be updated
   by using the `update` method while the running accuracy can be retrieved
   using the `result` method.

   .. method:: update(self) -> None

      Update the elapsed time.

      For more info on how to set the initial time see the class description.

      :return: None.


   .. method:: result(self) -> float

      Retrieves the elapsed time.

      Calling this method will not change the internal state of the metric.

      :return: The elapsed time, in seconds, as a float value.


   .. method:: reset(self) -> None

      Resets the metric, including the initial time.

      :return: None.



.. py:class:: MinibatchTime(*, train=True, test=True)

   Bases: :class:`PluginMetric[float]`

   The minibatch time metric.

   This metric "logs" the elapsed time for each iteration. Beware that this
   metric will not average the time across minibatches!

   If a more coarse-grained logging is needed, consider using
   :class:`EpochTime`, :class:`AverageEpochTime` or
   :class:`StepTime` instead.

   Creates an instance of the minibatch time metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to True.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: before_training_iteration(self, strategy) -> MetricResult


   .. method:: before_test_iteration(self, strategy) -> MetricResult


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _on_iteration(self, strategy: PluggableStrategy)


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: EpochTime(*, train=True, test=True)

   Bases: :class:`PluginMetric[float]`

   The epoch elapsed time metric.

   The elapsed time will be logged after each epoch. Beware that this
   metric will not average the time across epochs!

   If logging the average average across epochs is needed, consider using
   :class:`AverageEpochTime` instead.

   Creates an instance of the epoch time metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to True.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: before_test_step(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: AverageEpochTime(*, train=True, test=True)

   Bases: :class:`PluginMetric[float]`

   The average epoch time metric.

   The average elapsed time will be logged at the end of the step.

   Beware that this metric will average the time across epochs! If logging the
   epoch-specific time is needed, consider using :class:`EpochTime` instead.

   Creates an instance of the average epoch time metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to True.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: before_test_step(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: StepTime(*, train=True, test=True)

   Bases: :class:`PluginMetric[float]`

   The step time metric.

   This metric may seed very similar to :class:`AverageEpochTime`. However,
   differently from that: 1) obviously, the time is not averaged by dividing
   by the number of epochs; 2) most importantly, the time consumed outside the
   epoch loop is accounted too (a thing that :class:`AverageEpochTime` doesn't
   support). For instance, this metric is more suitable when measuring times
   of algorithms involving after-training consolidation, replay pattern
   selection and other time consuming mechanisms.

   Creates an instance of the EpochAccuracy metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to True.

   .. method:: before_training_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: before_test_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_training_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



