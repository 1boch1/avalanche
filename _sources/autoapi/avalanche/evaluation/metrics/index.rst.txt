:mod:`avalanche.evaluation.metrics`
===================================

.. py:module:: avalanche.evaluation.metrics


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   _any_event_metric/index.rst
   accuracy/index.rst
   confusion_matrix/index.rst
   cpu_usage/index.rst
   disk_usage/index.rst
   forgetting/index.rst
   gpu_usage/index.rst
   loss/index.rst
   mac/index.rst
   mean/index.rst
   ram_usage/index.rst
   sum/index.rst
   timing/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.Mean
   avalanche.evaluation.metrics.Sum
   avalanche.evaluation.metrics.Accuracy
   avalanche.evaluation.metrics.MinibatchAccuracy
   avalanche.evaluation.metrics.EpochAccuracy
   avalanche.evaluation.metrics.RunningEpochAccuracy
   avalanche.evaluation.metrics.TaskAccuracy
   avalanche.evaluation.metrics.ConfusionMatrix
   avalanche.evaluation.metrics.TaskConfusionMatrix
   avalanche.evaluation.metrics.CpuUsage
   avalanche.evaluation.metrics.MinibatchCpuUsage
   avalanche.evaluation.metrics.EpochCpuUsage
   avalanche.evaluation.metrics.AverageEpochCpuUsage
   avalanche.evaluation.metrics.StepCpuUsage
   avalanche.evaluation.metrics.DiskUsage
   avalanche.evaluation.metrics.DiskUsageMonitor
   avalanche.evaluation.metrics.Forgetting
   avalanche.evaluation.metrics.GpuUsage
   avalanche.evaluation.metrics.GpuUsageMonitor
   avalanche.evaluation.metrics.Loss
   avalanche.evaluation.metrics.MinibatchLoss
   avalanche.evaluation.metrics.EpochLoss
   avalanche.evaluation.metrics.RunningEpochLoss
   avalanche.evaluation.metrics.TaskLoss
   avalanche.evaluation.metrics.MAC
   avalanche.evaluation.metrics.RamUsage
   avalanche.evaluation.metrics.RamUsageMonitor
   avalanche.evaluation.metrics.ElapsedTime
   avalanche.evaluation.metrics.MinibatchTime
   avalanche.evaluation.metrics.EpochTime
   avalanche.evaluation.metrics.AverageEpochTime
   avalanche.evaluation.metrics.StepTime



Functions
~~~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.accuracy_metrics
   avalanche.evaluation.metrics.cpu_usage_metrics
   avalanche.evaluation.metrics.loss_metrics
   avalanche.evaluation.metrics.timing_metrics


.. py:class:: Mean

   Bases: :class:`Metric[float]`

   The mean metric.

   This utility metric is a general purpose metric that can be used to keep
   track of the mean of a sequence of values.

   Creates an instance of the mean metric.

   This metric in its initial state will return a mean value of 0.
   The metric can be updated by using the `update` method while the mean
   can be retrieved using the `result` method.

   .. method:: update(self, value: SupportsFloat, weight: SupportsFloat = 1.0) -> None

      Update the running mean given the value.

      The value can be weighted with a custom value, defined by the `weight`
      parameter.

      :param value: The value to be used to update the mean.
      :param weight: The weight of the value. Defaults to 1.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the mean.

      Calling this method will not change the internal state of the metric.

      :return: The mean, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: Sum

   Bases: :class:`Metric[float]`

   The sum metric.

   This utility metric is a general purpose metric that can be used to keep
   track of the sum of a sequence of values.

   Beware that this metric only supports summing numbers and the result is
   always a float value, even when `update` is called by passing `int`s only.

   Creates an instance of the sum metric.

   This metric in its initial state will return a sum value of 0.
   The metric can be updated by using the `update` method while the sum
   can be retrieved using the `result` method.

   .. method:: update(self, value: SupportsFloat) -> None

      Update the running sum given the value.

      :param value: The value to be used to update the sum.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the sum.

      Calling this method will not change the internal state of the metric.

      :return: The sum, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: Accuracy

   Bases: :class:`Metric[float]`

   The accuracy metric.

   Instances of this metric compute the average accuracy by receiving a pair
   of "ground truth" and "prediction" Tensors describing the labels of a
   minibatch. Those two tensors can both contain plain labels or
   one-hot/logit vectors.

   The result is the running accuracy computed as the number of correct
   patterns divided by the overall amount of patterns.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an accuracy value of 0.

   Creates an instance of the accuracy metric.

   By default this metric in its initial state will return an accuracy
   value of 0. The metric can be updated by using the `update` method
   while the running accuracy can be retrieved using the `result` method.

   .. attribute:: _mean_accuracy
      

      The mean utility that will be used to store the running accuracy.


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor) -> None

      Update the running accuracy given the true and predicted labels.

      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :param predicted_y: The ground truth. Both labels and logit vectors
          are supported.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the running accuracy.

      Calling this method will not change the internal state of the metric.

      :return: The running accuracy, as a float value between 0 and 1.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchAccuracy(*, train=True, test=True)

   Bases: :class:`PluginMetric[float]`

   The minibatch accuracy metric.

   This metric "logs" the accuracy value after each iteration. Beware that this
   metric will not average the accuracy across minibatches!

   If a more coarse-grained logging is needed, consider using
   :class:`EpochAccuracy` and/or :class:`TaskAccuracy` instead.

   Creates an instance of the MinibatchAccuracy metric.

   The train and test parameters are used to control if this metric should
   compute and log values referred to the train phase, test phase or both.
   At least one of them must be True!

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _on_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: EpochAccuracy(*, train=True, test=True)

   Bases: :class:`PluginMetric[float]`

   The average epoch accuracy metric.

   The accuracy will be logged after each epoch by computing the accuracy
   as the number of correctly predicted patterns divided by the overall
   number of patterns encountered in that epoch, which means that having
   unbalanced minibatch sizes will not affect the metric.

   Creates an instance of the EpochAccuracy metric.

   The train and test parameters are used to control if this metric should
   compute and log values referred to the train phase, test phase or both.
   At least one of them must be True!

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: before_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: before_test_step(self, strategy: PluggableStrategy) -> None


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: RunningEpochAccuracy(*, train=True, test=True)

   Bases: :class:`avalanche.evaluation.metrics.accuracy.EpochAccuracy`

   The running average accuracy metric.

   This metric behaves like :class:`EpochAccuracy` but, differently from it,
   this metric will log the running accuracy value after each iteration.

   Creates an instance of the RunningEpochAccuracy metric.

   The train and test parameters are used to control if this metric should
   compute and log values referred to the train phase, test phase or both.
   At least one of them must be True!

   Beware that the test parameter defaults to False because logging
   the running test accuracy it's and uncommon practice.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy)



.. py:class:: TaskAccuracy

   Bases: :class:`PluginMetric[Dict[int, float]]`

   The task accuracy metric.

   This is the most common metric used in the evaluation of a Continual
   Learning algorithm.

   Can be safely used when evaluation task-free scenarios, in which case the
   default task label "0" will be used.

   The task accuracies will be logged at the end of the test phase. This metric
   doesn't apply to the training phase.

   Creates an instance of the TaskAccuracy metric.

   .. attribute:: _task_accuracy
      :annotation: :Dict[int, Accuracy]

      A dictionary used to store the accuracy for each task.


   .. method:: reset(self) -> None


   .. method:: result(self) -> Dict[int, float]


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor, task_label: int) -> None


   .. method:: before_test(self, strategy) -> None


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_test(self, strategy) -> MetricResult


   .. method:: _package_result(self) -> MetricResult



.. function:: accuracy_metrics(*, minibatch=False, epoch=False, epoch_running=False, task=False, train=None, test=None) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of metric.

   :param minibatch: If True, will return a metric able to log the minibatch
       accuracy.
   :param epoch: If True, will return a metric able to log the epoch accuracy.
   :param epoch_running: If True, will return a metric able to log the running
       epoch accuracy.
   :param task: If True, will return a metric able to log the task accuracy.
       This metric applies to the test flow only. If the `test` parameter is
       False, an error will be raised.
   :param train: If True, metrics will log values for the train flow. Defaults
       to None, which means that the per-metric default value will be used.
   :param test: If True, metrics will log values for the test flow. Defaults
       to None, which means that the per-metric default value will be used.

   :return: A list of plugin metrics.


.. py:class:: ConfusionMatrix(num_classes: int = None)

   Bases: :class:`Metric[Tensor]`

   The confusion matrix metric.

   Instances of this metric keep track of the confusion matrix by receiving a
   pair of "ground truth" and "prediction" Tensors describing the labels of a
   minibatch. Those two tensors can both contain plain labels or
   one-hot/logit vectors.

   The result is the unnormalized running confusion matrix.

   Beware that by default the confusion matrix size will depend on the value of
   the maximum label as detected by looking at both the ground truth and
   predictions Tensors. When passing one-hot/logit vectors, this
   metric will try to infer the number of classes from the vector sizes.
   Otherwise, the maximum label value encountered in the truth/prediction
   Tensors will be used. It is recommended to set the (initial) number of
   classes in the constructor.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an empty Tensor.

   Creates an instance of the confusion matrix metric.

   By default this metric in its initial state will return an empty Tensor.
   The metric can be updated by using the `update` method while the running
   confusion matrix can be retrieved using the `result` method.

   :param num_classes: The initial number of classes. Defaults to None,
       which means that the number of classes will be inferred from
       ground truth and prediction Tensors (see class description for more
       details).

   .. attribute:: _cm_tensor
      :annotation: :Optional[Tensor]

      The Tensor where the running confusion matrix is stored.


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor) -> None

      Update the running confusion matrix given the true and predicted labels.

      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :param predicted_y: The ground truth. Both labels and logit vectors
          are supported.
      :return: None.


   .. method:: result(self) -> Tensor

      Retrieves the unnormalized confusion matrix.

      Calling this method will not change the internal state of the metric.

      :return: The running confusion matrix, as a Tensor.


   .. method:: reset(self) -> None

      Resets the metric.

      Calling this method will *not* reset the default number of classes
      optionally defined in the constructor optional parameter.

      :return: None.



.. py:class:: TaskConfusionMatrix(*, train: bool = False, test: bool = True, num_classes: Union[int, Mapping[int, int]] = None, normalize: Literal['true', 'pred', 'all'] = None, save_image: bool = True, image_creator: Callable[[Tensor], Image] = default_cm_image_creator)

   Bases: :class:`PluginMetric[Tensor]`

   The Confusion Matrix metric.

   This metric logs the confusion matrix for each task at the end of
   each phase. By default this metric computes the matrix on the test phase
   (on the test set) only but this behaviour can be changed by passing
   `train=True` in the constructor.

   The metric will log both a Tensor and PIL Image both representing the
   confusion matrices. The Logger will decide which one to use depending on its
   internal implementation.

   Creates an instance of the Confusion Matrix metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to False.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to True.
   :param num_classes: When not None, is used to properly define the
       amount of rows/columns in the confusion matrix. When None, the
       matrix will have many rows/columns as the maximum value of the
       predicted and true pattern labels. Can be either an int, in which
       case the same value will be used across all tasks, or a dictionary
       defining the amount of classes for each task (key = task label,
       value = amount of classes). Defaults to None.
   :param normalize: Normalizes confusion matrix over the true (rows),
       predicted (columns) conditions or all the population. If None,
       confusion matrix will not be normalized. Valid values are: 'true',
       'pred' and 'all'.
   :param save_image: If True, a graphical representation of the confusion
       matrix will be logged, too. If False, only the Tensor representation
       will be logged. Defaults to True.
   :param image_creator: A callable that, given the tensor representation
       of the confusion matrix, returns a graphical representation of the
       matrix as a PIL Image. Defaults to `default_cm_image_creator`.

   .. method:: reset(self) -> None


   .. method:: result(self) -> Dict[int, Tensor]


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor, task_label: int) -> None


   .. method:: before_training(self, strategy) -> None


   .. method:: before_test(self, strategy) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_training(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: after_test(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _class_num_for_task(self, task_label: int) -> Optional[int]


   .. method:: _normalize_cm(cm: Tensor, normalization: Literal['true', 'pred', 'all'])
      :staticmethod:


   .. method:: nan_to_num(matrix: Tensor) -> Tensor
      :staticmethod:



.. py:class:: CpuUsage

   Bases: :class:`Metric[float]`

   The CPU usage metric.

   Instances of this metric compute the average CPU usage as a float value.
   The metric starts tracking the CPU usage when the `update` method is called
   for the first time. That is, the tracking doesn't start at the time the
   constructor is invoked.

   Calling the `update` method more than twice will update the metric to the
   average usage between the first and the last call to `update`.

   The result, obtained using the `result` method, is the usage computed
   as stated above.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an usage value of 0.

   Creates an instance of the CPU usage metric.

   By default this metric in its initial state will return a CPU usage
   value of 0. The metric can be updated by using the `update` method
   while the average CPU usage can be retrieved using the `result` method.

   .. attribute:: _mean_usage
      

      The mean utility that will be used to store the average usage.


   .. attribute:: _process_handle
      :annotation: :Optional[Process]

      The process handle, lazily initialized.


   .. attribute:: _first_update
      :annotation: = True

      An internal flag to keep track of the first call to the `update` method.


   .. attribute:: _timer
      :annotation: :Callable[[], float]

      The timer implementation (aligned with the one used by psutil).


   .. method:: update(self) -> None

      Update the running CPU usage.

      For more info on how to set the starting moment see the class
      description.

      :return: None.


   .. method:: result(self) -> float

      Retrieves the average CPU usage.

      Calling this method will not change the internal state of the metric.

      :return: The average CPU usage, as a float value.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchCpuUsage(*, train=True, test=False)

   Bases: :class:`PluginMetric[float]`

   The minibatch CPU usage metric.

   This metric "logs" the CPU usage for each iteration. Beware that this
   metric will not average the usage across minibatches!

   If a more coarse-grained logging is needed, consider using
   :class:`EpochCpuUsage`, :class:`AverageEpochCpuUsage` or
   :class:`StepCpuUsage` instead.

   Creates an instance of the minibatch CPU usage metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: before_training_iteration(self, strategy) -> MetricResult


   .. method:: before_test_iteration(self, strategy) -> MetricResult


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: EpochCpuUsage(*, train=True, test=False)

   Bases: :class:`PluginMetric[float]`

   The epoch average CPU usage metric.

   The average usage will be logged after each epoch. Beware that this
   metric will not average the CPU usage across epochs!

   If logging the average usage across epochs is needed, consider using
   :class:`AverageEpochCpuUsage` instead.

   Creates an instance of the epoch CPU usage metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: before_test_step(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: AverageEpochCpuUsage(*, train=True, test=False)

   Bases: :class:`PluginMetric[float]`

   The average epoch CPU usage metric.

   The average usage will be logged at the end of the step.

   Beware that this metric will average the usage across epochs! If logging the
   epoch-specific usage is needed, consider using :class:`EpochCpuUsage`
   instead.

   Creates an instance of the average epoch cpu usage metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: before_test_step(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: StepCpuUsage(*, train=True, test=False)

   Bases: :class:`PluginMetric[float]`

   The average step CPU usage metric.

   This metric may seem very similar to :class:`AverageEpochCpuUsage`. However,
   differently from that: 1) obviously, the usage is not averaged by dividing
   by the number of epochs; 2) most importantly, the usage of code running
   outside the epoch loop is accounted too (a thing that
   :class:`AverageEpochCpuUsage` doesn't support). For instance, this metric is
   more suitable when measuring the CPU usage of algorithms involving
   after-training consolidation, replay pattern selection and other CPU bound
   mechanisms.

   Creates an instance of the step CPU usage metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: before_training_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: before_test_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_training_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. function:: cpu_usage_metrics(*, minibatch=False, epoch=False, epoch_average=False, step=False, train=None, test=None) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of metric.

   :param minibatch: If True, will return a metric able to log the minibatch
       elapsed time.
   :param epoch: If True, will return a metric able to log the epoch elapsed
       time.
   :param epoch_average: If True, will return a metric able to log the average
       epoch elapsed time.
   :param step: If True, will return a metric able to log the step elapsed
       time.
   :param train: If True, metrics will log values for the train flow. Defaults
       to None, which means that the per-metric default value will be used.
   :param test: If True, metrics will log values for the test flow. Defaults
       to None, which means that the per-metric default value will be used.

   :return: A list of plugin metrics.


.. py:class:: DiskUsage(paths_to_monitor: Union[PathAlike, Sequence[PathAlike]] = None, monitor_disk_io: bool = False)

   Bases: :class:`Metric[DiskUsageResult]`

   The disk usage metric.

   This metric can be used to monitor the size of a set of directories. This
   can be useful to monitor the size of a replay buffer,

   This metric can also be used to get info regarding the overall amount of
   other system-wide disk stats (see the constructor for more details).

   Creates an instance of the disk usage metric.

   By default invoking the `result` method will return the sum of the size
   of the directories specified as the first parameter. By passing
   `monitor_disk_io` as true the `result` method will return a 5 elements
   tuple containing 1) the sum of the size of the directories,
   the system-wide 2) read count, 3) write count, 4) read bytes and
   5) written bytes.

   :param paths_to_monitor: a path or a list of paths to monitor. If None,
       the current working directory is used. Defaults to None.
   :param monitor_disk_io: If True enables monitoring of I/O operations on
       disk. WARNING: Reports are system-wide, grouping all disks. Defaults
       to False.

   .. method:: update(self)

      Updates the disk usage statistics.

      :return None.


   .. method:: result(self) -> Optional[DiskUsageResult]

      Retrieves the disk usage as computed during the last call to the
      `update` method.

      Calling this method will not change the internal state of the metric.

      The info returned may vary depending on whether the constructor was
      invoked with `monitor_disk_io` to True. See the constructor for more
      details.

      :return: The disk usage or None if `update` was not invoked yet.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.


   .. method:: get_dir_size(path: str)
      :staticmethod:



.. py:class:: DiskUsageMonitor(*paths: PathAlike, timeout: float = 5.0, train=True, test=False)

   Bases: :class:`AnyEventMetric[float]`

   The disk usage metric.

   This metric logs the disk usage (directory size) of the given list of paths.

   The logged value is in MiB.

   The metric can be either configured to log after a certain timeout or
   at each event.

   Disk usage is logged separately for the train and test phases.

   Creates an instance of the disk usage metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param paths: A list of paths to monitor. If no paths are defined,
       the current working directory is used.
   :param timeout: The timeout between each disk usage check, in seconds.
       If None, the disk usage is checked at every possible event (not
       recommended). Defaults to 5 seconds.
   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: on_event(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: result(self) -> Optional[float]


   .. method:: reset(self) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy) -> 'MetricResult'



.. py:class:: Forgetting(compute_for_step=False)

   Bases: :class:`PluginMetric[Dict[int, float]]`

   The Forgetting metric, describing the accuracy loss detected for a
   certain task or step.

   This metric, computed separately for each task/step
   is the difference between the accuracy result obtained after
   first training on a task/step and the accuracy result obtained
   on the same task/step at the end of successive steps.

   This metric is computed during the test phase only.

   Creates an instance of the Forgetting metric.

   :param compute_for_step: if True, compute the metric at a step level.
       If False, compute the metric at task level. Default to False.

   .. attribute:: _initial_accuracy
      :annotation: :Dict[int, float]

      The initial accuracy of each task/step.


   .. attribute:: _current_accuracy
      :annotation: :Dict[int, Accuracy]

      The current accuracy of each task/step.


   .. method:: reset(self) -> None

      Resets the metric.

      Beware that this will also reset the initial accuracy of each task/step!

      :return: None.


   .. method:: reset_current_accuracy(self) -> None

      Resets the current accuracy.

      This will preserve the initial accuracy value of each task/step.
      To be used at the beginning of each test step.

      :return: None.


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor, label: int) -> None

      Updates the running accuracy of a task/step given the ground truth and
      predicted labels of a minibatch.

      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :param predicted_y: The ground truth. Both labels and logit vectors
          are supported.
      :param label: The task or step label.
      :return: None.


   .. method:: before_test(self, strategy) -> None


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_test(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: result(self) -> Dict[int, float]

      Return the amount of forgetting for each task/step.

      The forgetting is computed as the accuracy difference between the
      initial task/step accuracy (when first encountered
      in the training stream) and the current accuracy.
      A positive value means that forgetting occurred. A negative value
      means that the accuracy on that task/step increased.

      :return: A dictionary in which keys are task/step labels and the
               values are the forgetting measures
               (as floats in range [-1, 1]).


   .. method:: _package_result(self, label: int) -> MetricResult



.. py:class:: GpuUsage(gpu_id, every=2.0)

   GPU usage metric measured as average usage percentage over time.

   This metric will actively poll the system to get the GPU usage over time
   starting from the first call to `update`. Subsequent calls to the `update`
   method will consolidate the values gathered since the last call.

   The `result` method will return `None` until the `update` method is invoked
   at least two times.

   Invoking the `reset` method will stop the measurement and reset the metric
   to its initial state.

   Creates an instance of the GPU usage metric.

   For more info about the usage see the class description.

   :param gpu_id: GPU device ID.
   :param every: time delay (in seconds) between measurements.

   .. attribute:: MAX_BUFFER
      :annotation: = 10000

      

   .. attribute:: SMI_NOT_FOUND_MSG
      :annotation: = No GPU available: nvidia-smi command not found. Gpu Usage logging will be disabled.

      

   .. method:: update(self) -> None

      Consolidates the values got from the GPU sensor.

      This will store the average for retrieval through the `update` method.

      The previously consolidated value will be discarded.

      :return: None


   .. method:: _start_watch(self)


   .. method:: _push_lines(self) -> None


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.


   .. method:: result(self) -> Optional[float]

      Returns the last consolidated GPU usage value.

      For more info about the returned value see the class description.

      :return: The percentage GPU usage as a float value in range [0, 1].
          Returns None if the `update` method was invoked less than twice.


   .. method:: gpu_found(self) -> bool

      Checks if nvidia-smi could me executed.

      This method is experimental. Please use at you own risk.

      :return: True if nvidia-smi could be launched, False otherwise.



.. py:class:: GpuUsageMonitor(gpu_id: int, *, timeout: int = 2, train=True, test=False)

   Bases: :class:`AnyEventMetric[float]`

   The GPU usage metric.

   This metric logs the percentage GPU usage.

   The metric can be either configured to log after a certain timeout or
   at each event.

   GPU usage is logged separately for the train and test phases.

   Creates an instance of the GPU usage metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param gpu_id: The GPU to monitor.
   :param timeout: The timeout between each GPU usage log, in seconds.
        Defaults to 2 seconds. Must be an int.
   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: on_event(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: before_training(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: before_test(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: result(self) -> Optional[float]


   .. method:: reset(self) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy) -> 'MetricResult'



.. py:class:: Loss

   Bases: :class:`Metric[float]`

   The average loss metric.

   Instances of this metric compute the running average loss by receiving a
   Tensor describing the loss of a minibatch. This metric then uses that tensor
   to computes the average loss per pattern.

   The Tensor passed to the `update` method are averaged to obtain a
   minibatch average loss. In order to compute the per-pattern running loss,
   the users should must pass the number of patterns in that minibatch as the
   second parameter of the `update` method. The number of patterns can't be
   usually obtained by analyzing the shape of the loss Tensor, which usually
   consists of a single float value.

   The result is the running loss computed as the accumulated average loss.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return a loss value of 0.

   Creates an instance of the loss metric.

   By default this metric in its initial state will return a loss
   value of 0. The metric can be updated by using the `update` method
   while the running loss can be retrieved using the `result` method.

   .. method:: update(self, loss: Tensor, patterns: int) -> None

      Update the running loss given the loss Tensor and the minibatch size.

      :param loss: The loss Tensor. Different reduction types don't affect
          the result.
      :param patterns: The number of patterns in the minibatch.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the running average loss per pattern.

      Calling this method will not change the internal state of the metric.

      :return: The running loss, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchLoss(*, train=True, test=True)

   Bases: :class:`PluginMetric[float]`

   The minibatch loss metric.

   The logged loss value is the per-pattern loss obtained by averaging the loss
   of patterns contained in the minibatch.

   This metric "logs" the loss value after each iteration. Beware that this
   metric will not average the loss across minibatches!

   If a more coarse-grained logging is needed, consider using
   :class:`EpochLoss` and/or :class:`TaskLoss` instead.

   Creates an instance of the MinibatchLoss metric.

   The train and test parameters are used to control if this metric should
   compute and log values referred to the train phase, test phase or both.
   At least one of them must be True!

   Beware that the test parameter defaults to False because logging
   the test minibatch loss it's and uncommon practice.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _on_iteration(self, strategy: PluggableStrategy)


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: EpochLoss(*, train=True, test=True)

   Bases: :class:`PluginMetric[float]`

   The average epoch loss metric.

   The logged loss value is the per-pattern loss obtained by averaging the loss
   of all patterns encountered in that epoch, which means that having
   unbalanced minibatch sizes will not affect the metric.

   Creates an instance of the EpochLoss metric.

   The train and test parameters are used to control if this metric should
   compute and log values referred to the train phase, test phase or both.
   At least one of them must be True!

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to True.

   .. method:: before_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: before_test_step(self, strategy: PluggableStrategy) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: RunningEpochLoss(*, train=True, test=True)

   Bases: :class:`avalanche.evaluation.metrics.loss.EpochLoss`

   The running average loss metric.

   This metric behaves like :class:`EpochLoss` but, differently from it,
   this metric will log the running loss value after each iteration.

   Creates an instance of the RunningEpochLoss metric.

   The train and test parameters are used to control if this metric should
   compute and log values referred to the train phase, test phase or both.
   At least one of them must be True!

   Beware that the test parameter defaults to False because logging
   the running test loss it's and uncommon practice.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: TaskLoss

   Bases: :class:`PluginMetric[Dict[int, float]]`

   The task loss metric.

   The logged loss value is the per-pattern loss obtained by averaging the loss
   of all test patterns of a task. This is a common metric used in the
   evaluation of a Continual Learning algorithm.

   Can be safely used when evaluation task-free scenarios, in which case the
   default task label "0" will be used.

   The task losses will be logged at the end of the test phase. This metric
   doesn't apply to the training phase.

   Creates an instance of the TaskLoss metric.

   .. attribute:: _task_loss
      :annotation: :Dict[int, Loss]

      A dictionary used to store the loss for each task.


   .. method:: reset(self) -> None


   .. method:: result(self) -> Dict[int, float]


   .. method:: update(self, loss: Tensor, patterns: int, task_label: int) -> None


   .. method:: before_test(self, strategy) -> None


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_test(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self) -> MetricResult



.. function:: loss_metrics(*, minibatch=False, epoch=False, epoch_running=False, task=False, train=None, test=None) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of metric.

   :param minibatch: If True, will return a metric able to log the minibatch
       loss.
   :param epoch: If True, will return a metric able to log the epoch loss.
   :param epoch_running: If True, will return a metric able to log the running
       epoch loss.
   :param task: If True, will return a metric able to log the task loss. This
       metric applies to the test flow only. If the `test` parameter is False,
       an error will be raised.
   :param train: If True, metrics will log values for the train flow. Defaults
       to None, which means that the per-metric default value will be used.
   :param test: If True, metrics will log values for the test flow. Defaults
       to None, which means that the per-metric default value will be used.

   :return: A list of plugin metrics.


.. py:class:: MAC

   Bases: :class:`Metric[int]`

   Multiply-and-accumulate metric. Provides a lower bound of the
   computational cost of a model in a hardware-independent way by
   computing the number of multiplications. Currently supports only
   Linear or Conv2d modules. Other operations are ignored.

   Creates an instance of the MAC metric.

   .. method:: update(self, model: Module, dummy_input: Tensor)

      Computes the MAC metric.

      :param model: current model.
      :param dummy_input: A tensor of the correct size to feed as input
          to model.
      :return: MAC metric.


   .. method:: result(self) -> Optional[int]

      Return the number of MAC operations as computed in the previous call
      to the `update` method.

      :return: The number of MAC operations or None if `update` has not been
          called yet.


   .. method:: update_compute_cost(self, module, dummy_input, output)


   .. method:: is_recognized_module(mod)
      :staticmethod:



.. py:class:: RamUsage(two_read_average=False)

   Bases: :class:`Metric[float]`

   The RAM usage metric.

   Instances of this metric compute the punctual RAM usage as a float value.
   The metric updates the value each time the `update` method is called.

   The result, obtained using the `result` method, is the usage in bytes.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an usage value of `None`.

   Creates an instance of the RAM usage metric.

   By default this metric in its initial state will return a RAM usage
   value of `None`. The metric can be updated by using the `update` method
   while the average usage value can be retrieved using the `result`
   method.

   :param two_read_average: If True, the value resulting from calling
       `update` more than once will set the result to the average between
       the last read and the current RAM usage value.

   .. attribute:: _process_handle
      :annotation: :Optional[Process]

      The process handle, lazily initialized.


   .. attribute:: _last_values
      

      The last detected RAM usage.


   .. attribute:: _first_update
      :annotation: = True

      An internal flag to keep track of the first call to the `update` method.


   .. attribute:: _two_read_average
      

      If True, the value resulting from calling `update` more than once will 
      set the result to the average between the last read and the current RAM
      usage value.


   .. method:: update(self) -> None

      Update the RAM usage.

      :return: None.


   .. method:: result(self) -> Optional[float]

      Retrieves the RAM usage.

      Calling this method will not change the internal state of the metric.

      :return: The average RAM usage in bytes, as a float value.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: RamUsageMonitor(*, timeout: float = 5.0, train=True, test=False)

   Bases: :class:`AnyEventMetric[float]`

   The RAM usage metric.

   This metric logs the RAM usage.

   The logged value is in MiB.

   The metric can be either configured to log after a certain timeout or
   at each event.

   RAM usage is logged separately for the train and test phases.

   Creates an instance of the RAM usage metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param timeout: The timeout between each RAM usage check, in seconds.
       If None, the RAM usage is checked at every possible event (not
       recommended). Defaults to 5 seconds.
   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to False.

   .. method:: on_event(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: result(self) -> Optional[float]


   .. method:: reset(self) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy) -> 'MetricResult'



.. py:class:: ElapsedTime

   Bases: :class:`Metric[float]`

   The elapsed time metric.

   Instances of this metric keep track of the time elapsed between calls to the
   `update` method. The starting time is set when the `update` method is called
   for the first time. That is, the starting time is *not* taken at the time
   the constructor is invoked.

   Calling the `update` method more than twice will update the metric to the
   elapsed time between the first and the last call to `update`.

   The result, obtained using the `result` method, is the time, in seconds,
   computed as stated above.

   The `reset` method will set the metric to its initial state, thus resetting
   the initial time. This metric in its initial state (or if the `update`
   method was invoked only once) will return an elapsed time of 0.

   Creates an instance of the accuracy metric.

   This metric in its initial state (or if the `update` method was invoked
   only once) will return an elapsed time of 0. The metric can be updated
   by using the `update` method while the running accuracy can be retrieved
   using the `result` method.

   .. method:: update(self) -> None

      Update the elapsed time.

      For more info on how to set the initial time see the class description.

      :return: None.


   .. method:: result(self) -> float

      Retrieves the elapsed time.

      Calling this method will not change the internal state of the metric.

      :return: The elapsed time, in seconds, as a float value.


   .. method:: reset(self) -> None

      Resets the metric, including the initial time.

      :return: None.



.. py:class:: MinibatchTime(*, train=True, test=True)

   Bases: :class:`PluginMetric[float]`

   The minibatch time metric.

   This metric "logs" the elapsed time for each iteration. Beware that this
   metric will not average the time across minibatches!

   If a more coarse-grained logging is needed, consider using
   :class:`EpochTime`, :class:`AverageEpochTime` or
   :class:`StepTime` instead.

   Creates an instance of the minibatch time metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to True.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: before_training_iteration(self, strategy) -> MetricResult


   .. method:: before_test_iteration(self, strategy) -> MetricResult


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: EpochTime(*, train=True, test=True)

   Bases: :class:`PluginMetric[float]`

   The epoch elapsed time metric.

   The elapsed time will be logged after each epoch. Beware that this
   metric will not average the time across epochs!

   If logging the average time across epochs is needed, consider using
   :class:`AverageEpochTime` instead.

   Creates an instance of the epoch time metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to True.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: before_test_step(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: AverageEpochTime(*, train=True, test=True)

   Bases: :class:`PluginMetric[float]`

   The average epoch time metric.

   The average elapsed time will be logged at the end of the step.

   Beware that this metric will average the time across epochs! If logging the
   epoch-specific time is needed, consider using :class:`EpochTime` instead.

   Creates an instance of the average epoch time metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to True.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: before_test_step(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: StepTime(*, train=True, test=True)

   Bases: :class:`PluginMetric[float]`

   The step time metric.

   This metric may seem very similar to :class:`AverageEpochTime`. However,
   differently from that: 1) obviously, the time is not averaged by dividing
   by the number of epochs; 2) most importantly, the time consumed outside the
   epoch loop is accounted too (a thing that :class:`AverageEpochTime` doesn't
   support). For instance, this metric is more suitable when measuring times
   of algorithms involving after-training consolidation, replay pattern
   selection and other time consuming mechanisms.

   Creates an instance of the step time metric.

   The train and test parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param test: When True, the metric will be computed on the test
       phase. Defaults to True.

   .. method:: before_training_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: before_test_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_training_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_test_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. function:: timing_metrics(*, minibatch=False, epoch=False, epoch_average=False, step=False, train=None, test=None) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of metric.

   :param minibatch: If True, will return a metric able to log the minibatch
       elapsed time.
   :param epoch: If True, will return a metric able to log the epoch elapsed
       time.
   :param epoch_average: If True, will return a metric able to log the average
       epoch elapsed time.
   :param step: If True, will return a metric able to log the step elapsed
       time.
   :param train: If True, metrics will log values for the train flow. Defaults
       to None, which means that the per-metric default value will be used.
   :param test: If True, metrics will log values for the test flow. Defaults
       to None, which means that the per-metric default value will be used.

   :return: A list of plugin metrics.


