:mod:`avalanche.evaluation.metrics`
===================================

.. py:module:: avalanche.evaluation.metrics


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   _any_event_metric/index.rst
   accuracy/index.rst
   confusion_matrix/index.rst
   cpu_usage/index.rst
   disk_usage/index.rst
   forgetting/index.rst
   gpu_usage/index.rst
   loss/index.rst
   mac/index.rst
   mean/index.rst
   ram_usage/index.rst
   sum/index.rst
   timing/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.Mean
   avalanche.evaluation.metrics.Sum
   avalanche.evaluation.metrics.Accuracy
   avalanche.evaluation.metrics.MinibatchAccuracy
   avalanche.evaluation.metrics.EpochAccuracy
   avalanche.evaluation.metrics.RunningEpochAccuracy
   avalanche.evaluation.metrics.StepAccuracy
   avalanche.evaluation.metrics.StreamAccuracy
   avalanche.evaluation.metrics.ConfusionMatrix
   avalanche.evaluation.metrics.StreamConfusionMatrix
   avalanche.evaluation.metrics.CpuUsage
   avalanche.evaluation.metrics.MinibatchCpuUsage
   avalanche.evaluation.metrics.EpochCpuUsage
   avalanche.evaluation.metrics.AverageEpochCpuUsage
   avalanche.evaluation.metrics.StepCpuUsage
   avalanche.evaluation.metrics.DiskUsage
   avalanche.evaluation.metrics.DiskUsageMonitor
   avalanche.evaluation.metrics.StepForgetting
   avalanche.evaluation.metrics.GpuUsage
   avalanche.evaluation.metrics.GpuUsageMonitor
   avalanche.evaluation.metrics.Loss
   avalanche.evaluation.metrics.MinibatchLoss
   avalanche.evaluation.metrics.EpochLoss
   avalanche.evaluation.metrics.RunningEpochLoss
   avalanche.evaluation.metrics.StepLoss
   avalanche.evaluation.metrics.StreamLoss
   avalanche.evaluation.metrics.MAC
   avalanche.evaluation.metrics.RamUsage
   avalanche.evaluation.metrics.RamUsageMonitor
   avalanche.evaluation.metrics.ElapsedTime
   avalanche.evaluation.metrics.MinibatchTime
   avalanche.evaluation.metrics.EpochTime
   avalanche.evaluation.metrics.AverageEpochTime
   avalanche.evaluation.metrics.StepTime



Functions
~~~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.accuracy_metrics
   avalanche.evaluation.metrics.cpu_usage_metrics
   avalanche.evaluation.metrics.loss_metrics
   avalanche.evaluation.metrics.timing_metrics


.. py:class:: Mean

   Bases: :class:`Metric[float]`

   The mean metric.

   This utility metric is a general purpose metric that can be used to keep
   track of the mean of a sequence of values.

   Creates an instance of the mean metric.

   This metric in its initial state will return a mean value of 0.
   The metric can be updated by using the `update` method while the mean
   can be retrieved using the `result` method.

   .. method:: update(self, value: SupportsFloat, weight: SupportsFloat = 1.0) -> None

      Update the running mean given the value.

      The value can be weighted with a custom value, defined by the `weight`
      parameter.

      :param value: The value to be used to update the mean.
      :param weight: The weight of the value. Defaults to 1.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the mean.

      Calling this method will not change the internal state of the metric.

      :return: The mean, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: Sum

   Bases: :class:`Metric[float]`

   The sum metric.

   This utility metric is a general purpose metric that can be used to keep
   track of the sum of a sequence of values.

   Beware that this metric only supports summing numbers and the result is
   always a float value, even when `update` is called by passing `int`s only.

   Creates an instance of the sum metric.

   This metric in its initial state will return a sum value of 0.
   The metric can be updated by using the `update` method while the sum
   can be retrieved using the `result` method.

   .. method:: update(self, value: SupportsFloat) -> None

      Update the running sum given the value.

      :param value: The value to be used to update the sum.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the sum.

      Calling this method will not change the internal state of the metric.

      :return: The sum, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: Accuracy

   Bases: :class:`Metric[float]`

   The Accuracy metric. This is a general metric
   used to compute more specific ones.

   Instances of this metric keeps the running average accuracy
   over multiple <prediction, target> pairs of Tensors,
   provided incrementally.
   The "prediction" and "target" tensors may contain plain labels or
   one-hot/logit vectors.

   Each time `result` is called, this metric emits the average accuracy
   across all predictions made since the last `reset`.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an accuracy value of 0.

   Creates an instance of the Accuracy metric.

   By default this metric in its initial state will return an accuracy
   value of 0. The metric can be updated by using the `update` method
   while the running accuracy can be retrieved using the `result` method.

   .. attribute:: _mean_accuracy
      

      The mean utility that will be used to store the running accuracy.


   .. method:: update(self, predicted_y: Tensor, true_y: Tensor) -> None

      Update the running accuracy given the true and predicted labels.

      :param predicted_y: The model prediction. Both labels and logit vectors
          are supported.
      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the running accuracy.

      Calling this method will not change the internal state of the metric.

      :return: The running accuracy, as a float value between 0 and 1.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchAccuracy

   Bases: :class:`PluginMetric[float]`

   The minibatch accuracy metric.
   This metric only works at training time.

   This metric computes the average accuracy over patterns
   from a single minibatch.
   It reports the result after each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochAccuracy` instead.

   Creates an instance of the MinibatchAccuracy metric.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: EpochAccuracy

   Bases: :class:`PluginMetric[float]`

   The average accuracy over a single training epoch.
   This metric only works at training time.

   The accuracy will be logged after each training epoch by computing
   the number of correctly predicted patterns during the epoch divided by
   the overall number of patterns encountered in that epoch.

   Creates an instance of the EpochAccuracy metric.

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: before_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: RunningEpochAccuracy

   Bases: :class:`avalanche.evaluation.metrics.accuracy.EpochAccuracy`

   The average accuracy across all minibatches up to the current
   epoch iteration.
   This metric only works at training time.

   At each iteration, this metric logs the accuracy averaged over all patterns
   seen so far in the current epoch.
   The metric resets its state after each training epoch.

   Creates an instance of the RunningEpochAccuracy metric.

   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy)


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StepAccuracy

   Bases: :class:`PluginMetric[float]`

   At the end of each step, this metric reports
   the average accuracy over all patterns seen in that step.
   This metric only works at eval time.

   Creates an instance of StepAccuracy metric

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: before_eval_step(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_step(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StreamAccuracy

   Bases: :class:`PluginMetric[float]`

   At the end of the entire stream of steps, this metric reports the average
   accuracy over all patterns seen in all steps.
   This metric only works at eval time.

   Creates an instance of StreamAccuracy metric

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: before_eval(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. function:: accuracy_metrics(*, minibatch=False, epoch=False, epoch_running=False, step=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of metric.

   :param minibatch: If True, will return a metric able to log
       the minibatch accuracy at training time.
   :param epoch: If True, will return a metric able to log
       the epoch accuracy at training time.
   :param epoch_running: If True, will return a metric able to log
       the running epoch accuracy at training time.
   :param step: If True, will return a metric able to log
       the accuracy on each evaluation step.
   :param stream: If True, will return a metric able to log
       the accuracy averaged over the entire evaluation stream of steps.

   :return: A list of plugin metrics.


.. py:class:: ConfusionMatrix(num_classes: int = None)

   Bases: :class:`Metric[Tensor]`

   The confusion matrix metric.

   Instances of this metric keep track of the confusion matrix by receiving a
   pair of "ground truth" and "prediction" Tensors describing the labels of a
   minibatch. Those two tensors can both contain plain labels or
   one-hot/logit vectors.

   The result is the unnormalized running confusion matrix.

   Beware that by default the confusion matrix size will depend on the value of
   the maximum label as detected by looking at both the ground truth and
   predictions Tensors. When passing one-hot/logit vectors, this
   metric will try to infer the number of classes from the vector sizes.
   Otherwise, the maximum label value encountered in the truth/prediction
   Tensors will be used. It is recommended to set the (initial) number of
   classes in the constructor.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an empty Tensor.

   Creates an instance of the confusion matrix metric.

   By default this metric in its initial state will return an empty Tensor.
   The metric can be updated by using the `update` method while the running
   confusion matrix can be retrieved using the `result` method.

   :param num_classes: The initial number of classes. Defaults to None,
       which means that the number of classes will be inferred from
       ground truth and prediction Tensors (see class description for more
       details).

   .. attribute:: _cm_tensor
      :annotation: :Optional[Tensor]

      The Tensor where the running confusion matrix is stored.


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor) -> None

      Update the running confusion matrix given the true and predicted labels.

      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :param predicted_y: The ground truth. Both labels and logit vectors
          are supported.
      :return: None.


   .. method:: result(self) -> Tensor

      Retrieves the unnormalized confusion matrix.

      Calling this method will not change the internal state of the metric.

      :return: The running confusion matrix, as a Tensor.


   .. method:: reset(self) -> None

      Resets the metric.

      Calling this method will *not* reset the default number of classes
      optionally defined in the constructor optional parameter.

      :return: None.



.. py:class:: StreamConfusionMatrix(*, num_classes: Union[int, Mapping[int, int]] = None, normalize: Literal['true', 'pred', 'all'] = None, save_image: bool = True, image_creator: Callable[[Tensor], Image] = default_cm_image_creator)

   Bases: :class:`PluginMetric[Tensor]`

   The Stream Confusion Matrix metric.
   This metric only works on the eval phase.

   At the end of the eval phase, this metric logs the confusion matrix
   relative to all the patterns seen during eval.

   The metric can log either a Tensor or a PIL Image representing the
   confusion matrix.

   Creates an instance of the Stream Confusion Matrix metric.

   :param num_classes: When not None, is used to properly define the
       amount of rows/columns in the confusion matrix. When None, the
       matrix will have many rows/columns as the maximum value of the
       predicted and true pattern labels. Can be either an int, in which
       case the same value will be used across all steps, or a dictionary
       defining the amount of classes for each step (key = step label,
       value = amount of classes). Defaults to None.
   :param normalize: Normalizes confusion matrix over the true (rows),
       predicted (columns) conditions or all the population. If None,
       confusion matrix will not be normalized. Valid values are: 'true',
       'pred' and 'all' or None.
   :param save_image: If True, a graphical representation of the confusion
       matrix will be logged, too. If False, only the Tensor representation
       will be logged. Defaults to True.
   :param image_creator: A callable that, given the tensor representation
       of the confusion matrix, returns a graphical representation of the
       matrix as a PIL Image. Defaults to `default_cm_image_creator`.

   .. method:: reset(self) -> None


   .. method:: result(self) -> Tensor


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor) -> None


   .. method:: before_eval(self, strategy) -> None


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _class_num_for_step(self, step_label: int) -> Optional[int]


   .. method:: _normalize_cm(cm: Tensor, normalization: Literal['true', 'pred', 'all'])
      :staticmethod:


   .. method:: nan_to_num(matrix: Tensor) -> Tensor
      :staticmethod:


   .. method:: __str__(self)

      Return str(self).



.. py:class:: CpuUsage

   Bases: :class:`Metric[float]`

   The CPU usage metric.

   Instances of this metric compute the average CPU usage as a float value.
   The metric starts tracking the CPU usage when the `update` method is called
   for the first time. That is, the tracking doesn't start at the time the
   constructor is invoked.

   Calling the `update` method more than twice will update the metric to the
   average usage between the first and the last call to `update`.

   The result, obtained using the `result` method, is the usage computed
   as stated above.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an usage value of 0.

   Creates an instance of the CPU usage metric.

   By default this metric in its initial state will return a CPU usage
   value of 0. The metric can be updated by using the `update` method
   while the average CPU usage can be retrieved using the `result` method.

   .. attribute:: _mean_usage
      

      The mean utility that will be used to store the average usage.


   .. attribute:: _process_handle
      :annotation: :Optional[Process]

      The process handle, lazily initialized.


   .. attribute:: _first_update
      :annotation: = True

      An internal flag to keep track of the first call to the `update` method.


   .. attribute:: _timer
      :annotation: :Callable[[], float]

      The timer implementation (aligned with the one used by psutil).


   .. method:: update(self) -> None

      Update the running CPU usage.

      For more info on how to set the starting moment see the class
      description.

      :return: None.


   .. method:: result(self) -> float

      Retrieves the average CPU usage.

      Calling this method will not change the internal state of the metric.

      :return: The average CPU usage, as a float value.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchCpuUsage(*, train=True, eval=False)

   Bases: :class:`PluginMetric[float]`

   The minibatch CPU usage metric.

   This metric "logs" the CPU usage for each iteration. Beware that this
   metric will not average the usage across minibatches!

   If a more coarse-grained logging is needed, consider using
   :class:`EpochCpuUsage`, :class:`AverageEpochCpuUsage` or
   :class:`StepCpuUsage` instead.

   Creates an instance of the minibatch CPU usage metric.

   The train and eval parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to False.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: before_training_iteration(self, strategy) -> MetricResult


   .. method:: before_eval_iteration(self, strategy) -> MetricResult


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: EpochCpuUsage(*, train=True, eval=False)

   Bases: :class:`PluginMetric[float]`

   The epoch average CPU usage metric.

   The average usage will be logged after each epoch. Beware that this
   metric will not average the CPU usage across epochs!

   If logging the average usage across epochs is needed, consider using
   :class:`AverageEpochCpuUsage` instead.

   Creates an instance of the epoch CPU usage metric.

   The train and eval parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to False.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: before_eval_step(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_eval_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: AverageEpochCpuUsage(*, train=True, eval=False)

   Bases: :class:`PluginMetric[float]`

   The average epoch CPU usage metric.

   The average usage will be logged at the end of the step.

   Beware that this metric will average the usage across epochs! If logging the
   epoch-specific usage is needed, consider using :class:`EpochCpuUsage`
   instead.

   Creates an instance of the average epoch cpu usage metric.

   The train and eval parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to False.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: before_eval_step(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_eval_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: StepCpuUsage(*, train=True, eval=False)

   Bases: :class:`PluginMetric[float]`

   The average step CPU usage metric.

   This metric may seem very similar to :class:`AverageEpochCpuUsage`. However,
   differently from that: 1) obviously, the usage is not averaged by dividing
   by the number of epochs; 2) most importantly, the usage of code running
   outside the epoch loop is accounted too (a thing that
   :class:`AverageEpochCpuUsage` doesn't support). For instance, this metric is
   more suitable when measuring the CPU usage of algorithms involving
   after-training consolidation, replay pattern selection and other CPU bound
   mechanisms.

   Creates an instance of the step CPU usage metric.

   The train and eval parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to False.

   .. method:: before_training_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: before_eval_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_training_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_eval_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. function:: cpu_usage_metrics(*, minibatch=False, epoch=False, epoch_average=False, step=False, train=None, eval=None) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of metric.

   :param minibatch: If True, will return a metric able to log the minibatch
       elapsed time.
   :param epoch: If True, will return a metric able to log the epoch elapsed
       time.
   :param epoch_average: If True, will return a metric able to log the average
       epoch elapsed time.
   :param step: If True, will return a metric able to log the step elapsed
       time.
   :param train: If True, metrics will log values for the train flow. Defaults
       to None, which means that the per-metric default value will be used.
   :param eval: If True, metrics will log values for the eval flow. Defaults
       to None, which means that the per-metric default value will be used.

   :return: A list of plugin metrics.


.. py:class:: DiskUsage(paths_to_monitor: Union[PathAlike, Sequence[PathAlike]] = None, monitor_disk_io: bool = False)

   Bases: :class:`Metric[DiskUsageResult]`

   The disk usage metric.

   This metric can be used to monitor the size of a set of directories. This
   can be useful to monitor the size of a replay buffer,

   This metric can also be used to get info regarding the overall amount of
   other system-wide disk stats (see the constructor for more details).

   Creates an instance of the disk usage metric.

   By default invoking the `result` method will return the sum of the size
   of the directories specified as the first parameter. By passing
   `monitor_disk_io` as true the `result` method will return a 5 elements
   tuple containing 1) the sum of the size of the directories,
   the system-wide 2) read count, 3) write count, 4) read bytes and
   5) written bytes.

   :param paths_to_monitor: a path or a list of paths to monitor. If None,
       the current working directory is used. Defaults to None.
   :param monitor_disk_io: If True enables monitoring of I/O operations on
       disk. WARNING: Reports are system-wide, grouping all disks. Defaults
       to False.

   .. method:: update(self)

      Updates the disk usage statistics.

      :return None.


   .. method:: result(self) -> Optional[DiskUsageResult]

      Retrieves the disk usage as computed during the last call to the
      `update` method.

      Calling this method will not change the internal state of the metric.

      The info returned may vary depending on whether the constructor was
      invoked with `monitor_disk_io` to True. See the constructor for more
      details.

      :return: The disk usage or None if `update` was not invoked yet.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.


   .. method:: get_dir_size(path: str)
      :staticmethod:



.. py:class:: DiskUsageMonitor(*paths: PathAlike, timeout: float = 5.0, train=True, eval=False)

   Bases: :class:`AnyEventMetric[float]`

   The disk usage metric.

   This metric logs the disk usage (directory size) of the given list of paths.

   The logged value is in MiB.

   The metric can be either configured to log after a certain timeout or
   at each event.

   Disk usage is logged separately for the train and eval phases.

   Creates an instance of the disk usage metric.

   The train and eval parameters can be True at the same time. However,
   at least one of them must be True.

   :param paths: A list of paths to monitor. If no paths are defined,
       the current working directory is used.
   :param timeout: The timeout between each disk usage check, in seconds.
       If None, the disk usage is checked at every possible event (not
       recommended). Defaults to 5 seconds.
   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to False.

   .. method:: on_event(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: result(self) -> Optional[float]


   .. method:: reset(self) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy) -> 'MetricResult'



.. py:class:: StepForgetting

   Bases: :class:`PluginMetric[Dict[int, float]]`

   The Forgetting metric, describing the accuracy loss detected for a
   certain step.

   This metric, computed separately for each step,
   is the difference between the accuracy result obtained after
   first training on a step and the accuracy result obtained
   on the same step at the end of successive steps.

   This metric is computed during the eval phase only.

   Creates an instance of the StepForgetting metric.

   .. attribute:: _initial_accuracy
      :annotation: :Dict[int, float]

      The initial accuracy of each step.


   .. attribute:: _current_accuracy
      :annotation: :Dict[int, Accuracy]

      The current accuracy of each step.


   .. attribute:: eval_step_id
      

      The current evaluation step id


   .. method:: reset(self) -> None

      Resets the metric.

      Beware that this will also reset the initial accuracy of each step!

      :return: None.


   .. method:: reset_current_accuracy(self) -> None

      Resets the current accuracy.

      This will preserve the initial accuracy value of each step.
      To be used at the beginning of each eval step.

      :return: None.


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor, label: int) -> None

      Updates the running accuracy of a step given the ground truth and
      predicted labels of a minibatch.

      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :param predicted_y: The ground truth. Both labels and logit vectors
          are supported.
      :param label: The step label.
      :return: None.


   .. method:: before_eval(self, strategy) -> None


   .. method:: before_eval_step(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: result(self) -> float

      Return the amount of forgetting for the eval step
      associated to `eval_label`.

      The forgetting is computed as the accuracy difference between the
      initial step accuracy (when first encountered
      in the training stream) and the current accuracy.
      A positive value means that forgetting occurred. A negative value
      means that the accuracy on that step increased.

      :param eval_label: integer label describing the eval step
              of which measuring the forgetting
      :return: The amount of forgetting on `eval_step` step
               (as float in range [-1, 1]).


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: GpuUsage(gpu_id, every=2.0)

   GPU usage metric measured as average usage percentage over time.

   This metric will actively poll the system to get the GPU usage over time
   starting from the first call to `update`. Subsequent calls to the `update`
   method will consolidate the values gathered since the last call.

   The `result` method will return `None` until the `update` method is invoked
   at least two times.

   Invoking the `reset` method will stop the measurement and reset the metric
   to its initial state.

   Creates an instance of the GPU usage metric.

   For more info about the usage see the class description.

   :param gpu_id: GPU device ID.
   :param every: time delay (in seconds) between measurements.

   .. attribute:: MAX_BUFFER
      :annotation: = 10000

      

   .. attribute:: SMI_NOT_FOUND_MSG
      :annotation: = No GPU available: nvidia-smi command not found. Gpu Usage logging will be disabled.

      

   .. method:: update(self) -> None

      Consolidates the values got from the GPU sensor.

      This will store the average for retrieval through the `update` method.

      The previously consolidated value will be discarded.

      :return: None


   .. method:: _start_watch(self)


   .. method:: _push_lines(self) -> None


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.


   .. method:: result(self) -> Optional[float]

      Returns the last consolidated GPU usage value.

      For more info about the returned value see the class description.

      :return: The percentage GPU usage as a float value in range [0, 1].
          Returns None if the `update` method was invoked less than twice.


   .. method:: gpu_found(self) -> bool

      Checks if nvidia-smi could me executed.

      This method is experimental. Please use at you own risk.

      :return: True if nvidia-smi could be launched, False otherwise.



.. py:class:: GpuUsageMonitor(gpu_id: int, *, timeout: int = 2, train=True, eval=False)

   Bases: :class:`AnyEventMetric[float]`

   The GPU usage metric.

   This metric logs the percentage GPU usage.

   The metric can be either configured to log after a certain timeout or
   at each event.

   GPU usage is logged separately for the train and eval phases.

   Creates an instance of the GPU usage metric.

   The train and eval parameters can be True at the same time. However,
   at least one of them must be True.

   :param gpu_id: The GPU to monitor.
   :param timeout: The timeout between each GPU usage log, in seconds.
        Defaults to 2 seconds. Must be an int.
   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to False.

   .. method:: on_event(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: before_training(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: before_eval(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: result(self) -> Optional[float]


   .. method:: reset(self) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy) -> 'MetricResult'



.. py:class:: Loss

   Bases: :class:`Metric[float]`

   The Loss metric. This is a general metric
   used to compute more specific ones.

   Instances of this metric keeps the running average loss
   over multiple <prediction, target> pairs of Tensors,
   provided incrementally.
   The "prediction" and "target" tensors may contain plain labels or
   one-hot/logit vectors.

   Each time `result` is called, this metric emits the average loss
   across all predictions made since the last `reset`.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return a loss value of 0.

   Creates an instance of the loss metric.

   By default this metric in its initial state will return a loss
   value of 0. The metric can be updated by using the `update` method
   while the running loss can be retrieved using the `result` method.

   .. method:: update(self, loss: Tensor, patterns: int) -> None

      Update the running loss given the loss Tensor and the minibatch size.

      :param loss: The loss Tensor. Different reduction types don't affect
          the result.
      :param patterns: The number of patterns in the minibatch.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the running average loss per pattern.

      Calling this method will not change the internal state of the metric.

      :return: The running loss, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchLoss

   Bases: :class:`PluginMetric[float]`

   The minibatch loss metric.
   This metric only works at training time.

   This metric computes the average loss over patterns
   from a single minibatch.
   It reports the result after each iteration.

   If a more coarse-grained logging is needed, consider using
   :class:`EpochLoss` instead.

   Creates an instance of the MinibatchLoss metric.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: EpochLoss

   Bases: :class:`PluginMetric[float]`

   The average loss over a single training epoch.
   This metric only works at training time.

   The loss will be logged after each training epoch by computing
   the loss on the predicted patterns during the epoch divided by
   the overall number of patterns encountered in that epoch.

   Creates an instance of the EpochLoss metric.

   .. method:: before_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: RunningEpochLoss

   Bases: :class:`avalanche.evaluation.metrics.loss.EpochLoss`

   The average loss across all minibatches up to the current
   epoch iteration.
   This metric only works at training time.

   At each iteration, this metric logs the loss averaged over all patterns
   seen so far in the current epoch.
   The metric resets its state after each training epoch.

   Creates an instance of the RunningEpochLoss metric.

   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StepLoss

   Bases: :class:`PluginMetric[float]`

   At the end of each step, this metric reports
   the average loss over all patterns seen in that step.
   This metric only works at eval time.

   Creates an instance of StepLoss metric

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: before_eval_step(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_step(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. py:class:: StreamLoss

   Bases: :class:`PluginMetric[float]`

   At the end of the entire stream of steps, this metric reports the average
   loss over all patterns seen in all steps.
   This metric only works at eval time.

   Creates an instance of StreamLoss metric

   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: before_eval(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



.. function:: loss_metrics(*, minibatch=False, epoch=False, epoch_running=False, step=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of metric.

   :param minibatch: If True, will return a metric able to log
       the minibatch loss at training time.
   :param epoch: If True, will return a metric able to log
       the epoch loss at training time.
   :param epoch_running: If True, will return a metric able to log
       the running epoch loss at training time.
   :param step: If True, will return a metric able to log
       the loss on each evaluation step.
   :param stream: If True, will return a metric able to log
       the loss averaged over the entire evaluation stream of steps.

   :return: A list of plugin metrics.


.. py:class:: MAC

   Bases: :class:`Metric[int]`

   Multiply-and-accumulate metric. Provides a lower bound of the
   computational cost of a model in a hardware-independent way by
   computing the number of multiplications. Currently supports only
   Linear or Conv2d modules. Other operations are ignored.

   Creates an instance of the MAC metric.

   .. method:: update(self, model: Module, dummy_input: Tensor)

      Computes the MAC metric.

      :param model: current model.
      :param dummy_input: A tensor of the correct size to feed as input
          to model.
      :return: MAC metric.


   .. method:: result(self) -> Optional[int]

      Return the number of MAC operations as computed in the previous call
      to the `update` method.

      :return: The number of MAC operations or None if `update` has not been
          called yet.


   .. method:: update_compute_cost(self, module, dummy_input, output)


   .. method:: is_recognized_module(mod)
      :staticmethod:



.. py:class:: RamUsage(two_read_average=False)

   Bases: :class:`Metric[float]`

   The RAM usage metric.

   Instances of this metric compute the punctual RAM usage as a float value.
   The metric updates the value each time the `update` method is called.

   The result, obtained using the `result` method, is the usage in bytes.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return an usage value of `None`.

   Creates an instance of the RAM usage metric.

   By default this metric in its initial state will return a RAM usage
   value of `None`. The metric can be updated by using the `update` method
   while the average usage value can be retrieved using the `result`
   method.

   :param two_read_average: If True, the value resulting from calling
       `update` more than once will set the result to the average between
       the last read and the current RAM usage value.

   .. attribute:: _process_handle
      :annotation: :Optional[Process]

      The process handle, lazily initialized.


   .. attribute:: _last_values
      

      The last detected RAM usage.


   .. attribute:: _first_update
      :annotation: = True

      An internal flag to keep track of the first call to the `update` method.


   .. attribute:: _two_read_average
      

      If True, the value resulting from calling `update` more than once will
      set the result to the average between the last read and the current RAM
      usage value.


   .. method:: update(self) -> None

      Update the RAM usage.

      :return: None.


   .. method:: result(self) -> Optional[float]

      Retrieves the RAM usage.

      Calling this method will not change the internal state of the metric.

      :return: The average RAM usage in bytes, as a float value.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: RamUsageMonitor(*, timeout: float = 5.0, train=True, eval=False)

   Bases: :class:`AnyEventMetric[float]`

   The RAM usage metric.

   This metric logs the RAM usage.

   The logged value is in MiB.

   The metric can be either configured to log after a certain timeout or
   at each event.

   RAM usage is logged separately for the train and eval phases.

   Creates an instance of the RAM usage metric.

   The train and eval parameters can be True at the same time. However,
   at least one of them must be True.

   :param timeout: The timeout between each RAM usage check, in seconds.
       If None, the RAM usage is checked at every possible event (not
       recommended). Defaults to 5 seconds.
   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to False.

   .. method:: on_event(self, strategy: PluggableStrategy) -> 'MetricResult'


   .. method:: result(self) -> Optional[float]


   .. method:: reset(self) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy) -> 'MetricResult'



.. py:class:: ElapsedTime

   Bases: :class:`Metric[float]`

   The elapsed time metric.

   Instances of this metric keep track of the time elapsed between calls to the
   `update` method. The starting time is set when the `update` method is called
   for the first time. That is, the starting time is *not* taken at the time
   the constructor is invoked.

   Calling the `update` method more than twice will update the metric to the
   elapsed time between the first and the last call to `update`.

   The result, obtained using the `result` method, is the time, in seconds,
   computed as stated above.

   The `reset` method will set the metric to its initial state, thus resetting
   the initial time. This metric in its initial state (or if the `update`
   method was invoked only once) will return an elapsed time of 0.

   Creates an instance of the accuracy metric.

   This metric in its initial state (or if the `update` method was invoked
   only once) will return an elapsed time of 0. The metric can be updated
   by using the `update` method while the running accuracy can be retrieved
   using the `result` method.

   .. method:: update(self) -> None

      Update the elapsed time.

      For more info on how to set the initial time see the class description.

      :return: None.


   .. method:: result(self) -> float

      Retrieves the elapsed time.

      Calling this method will not change the internal state of the metric.

      :return: The elapsed time, in seconds, as a float value.


   .. method:: reset(self) -> None

      Resets the metric, including the initial time.

      :return: None.



.. py:class:: MinibatchTime(*, train=True, eval=True)

   Bases: :class:`PluginMetric[float]`

   The minibatch time metric.

   This metric "logs" the elapsed time for each iteration. Beware that this
   metric will not average the time across minibatches!

   If a more coarse-grained logging is needed, consider using
   :class:`EpochTime`, :class:`AverageEpochTime` or
   :class:`StepTime` instead.

   Creates an instance of the minibatch time metric.

   The train and eval parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to True.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: before_training_iteration(self, strategy) -> MetricResult


   .. method:: before_eval_iteration(self, strategy) -> MetricResult


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: EpochTime(*, train=True, eval=True)

   Bases: :class:`PluginMetric[float]`

   The epoch elapsed time metric.

   The elapsed time will be logged after each epoch. Beware that this
   metric will not average the time across epochs!

   If logging the average time across epochs is needed, consider using
   :class:`AverageEpochTime` instead.

   Creates an instance of the epoch time metric.

   The train and eval parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to True.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: before_eval_step(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_eval_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: AverageEpochTime(*, train=True, eval=True)

   Bases: :class:`PluginMetric[float]`

   The average epoch time metric.

   The average elapsed time will be logged at the end of the step.

   Beware that this metric will average the time across epochs! If logging the
   epoch-specific time is needed, consider using :class:`EpochTime` instead.

   Creates an instance of the average epoch time metric.

   The train and eval parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to True.

   .. method:: before_training_epoch(self, strategy) -> MetricResult


   .. method:: before_eval_step(self, strategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_eval_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: StepTime(*, train=True, eval=True)

   Bases: :class:`PluginMetric[float]`

   The step time metric.

   This metric may seem very similar to :class:`AverageEpochTime`. However,
   differently from that: 1) obviously, the time is not averaged by dividing
   by the number of epochs; 2) most importantly, the time consumed outside the
   epoch loop is accounted too (a thing that :class:`AverageEpochTime` doesn't
   support). For instance, this metric is more suitable when measuring times
   of algorithms involving after-training consolidation, replay pattern
   selection and other time consuming mechanisms.

   Creates an instance of the step time metric.

   The train and eval parameters can be True at the same time. However,
   at least one of them must be True.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to True.

   .. method:: before_training_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: before_eval_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_training_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_eval_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. function:: timing_metrics(*, minibatch=False, epoch=False, epoch_average=False, step=False, train=None, eval=None) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of metric.

   :param minibatch: If True, will return a metric able to log the minibatch
       elapsed time.
   :param epoch: If True, will return a metric able to log the epoch elapsed
       time.
   :param epoch_average: If True, will return a metric able to log the average
       epoch elapsed time.
   :param step: If True, will return a metric able to log the step elapsed
       time.
   :param train: If True, metrics will log values for the train flow. Defaults
       to None, which means that the per-metric default value will be used.
   :param eval: If True, metrics will log values for the eval flow. Defaults
       to None, which means that the per-metric default value will be used.

   :return: A list of plugin metrics.


