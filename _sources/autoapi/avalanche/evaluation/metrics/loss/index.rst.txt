:mod:`avalanche.evaluation.metrics.loss`
========================================

.. py:module:: avalanche.evaluation.metrics.loss


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.loss.Loss
   avalanche.evaluation.metrics.loss.MinibatchLoss
   avalanche.evaluation.metrics.loss.EpochLoss
   avalanche.evaluation.metrics.loss.RunningEpochLoss
   avalanche.evaluation.metrics.loss.TaskLoss



Functions
~~~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.loss.loss_metrics


.. py:class:: Loss

   Bases: :class:`Metric[float]`

   The average loss metric.

   Instances of this metric compute the running average loss by receiving a
   Tensor describing the loss of a minibatch. This metric then uses that tensor
   to computes the average loss per pattern.

   The Tensor passed to the `update` method are averaged to obtain a
   minibatch average loss. In order to compute the per-pattern running loss,
   the users should must pass the number of patterns in that minibatch as the
   second parameter of the `update` method. The number of patterns can't be
   usually obtained by analyzing the shape of the loss Tensor, which usually
   consists of a single float value.

   The result is the running loss computed as the accumulated average loss.

   The reset method will bring the metric to its initial state. By default
   this metric in its initial state will return a loss value of 0.

   Creates an instance of the loss metric.

   By default this metric in its initial state will return a loss
   value of 0. The metric can be updated by using the `update` method
   while the running loss can be retrieved using the `result` method.

   .. method:: update(self, loss: Tensor, patterns: int) -> None

      Update the running loss given the loss Tensor and the minibatch size.

      :param loss: The loss Tensor. Different reduction types don't affect
          the result.
      :param patterns: The number of patterns in the minibatch.
      :return: None.


   .. method:: result(self) -> float

      Retrieves the running average loss per pattern.

      Calling this method will not change the internal state of the metric.

      :return: The running loss, as a float.


   .. method:: reset(self) -> None

      Resets the metric.

      :return: None.



.. py:class:: MinibatchLoss(*, train=True, eval=True)

   Bases: :class:`PluginMetric[float]`

   The minibatch loss metric.

   The logged loss value is the per-pattern loss obtained by averaging the loss
   of patterns contained in the minibatch.

   This metric "logs" the loss value after each iteration. Beware that this
   metric will not average the loss across minibatches!

   If a more coarse-grained logging is needed, consider using
   :class:`EpochLoss` and/or :class:`TaskLoss` instead.

   Creates an instance of the MinibatchLoss metric.

   The train and eval parameters are used to control if this metric should
   compute and log values referred to the train phase, eval phase or both.
   At least one of them must be True!

   Beware that the eval parameter defaults to False because logging
   the eval minibatch loss it's and uncommon practice.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to False.

   .. method:: result(self) -> float


   .. method:: reset(self) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _on_iteration(self, strategy: PluggableStrategy)


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: EpochLoss(*, train=True, eval=True)

   Bases: :class:`PluginMetric[float]`

   The average epoch loss metric.

   The logged loss value is the per-pattern loss obtained by averaging the loss
   of all patterns encountered in that epoch, which means that having
   unbalanced minibatch sizes will not affect the metric.

   Creates an instance of the EpochLoss metric.

   The train and eval parameters are used to control if this metric should
   compute and log values referred to the train phase, eval phase or both.
   At least one of them must be True!

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to True.

   .. method:: before_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: before_eval_step(self, strategy: PluggableStrategy) -> None


   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_eval_step(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: reset(self) -> None


   .. method:: result(self) -> float


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: RunningEpochLoss(*, train=True, eval=True)

   Bases: :class:`avalanche.evaluation.metrics.loss.EpochLoss`

   The running average loss metric.

   This metric behaves like :class:`EpochLoss` but, differently from it,
   this metric will log the running loss value after each iteration.

   Creates an instance of the RunningEpochLoss metric.

   The train and eval parameters are used to control if this metric should
   compute and log values referred to the train phase, eval phase or both.
   At least one of them must be True!

   Beware that the eval parameter defaults to False because logging
   the running eval loss it's and uncommon practice.

   :param train: When True, the metric will be computed on the training
       phase. Defaults to True.
   :param eval: When True, the metric will be computed on the eval
       phase. Defaults to False.

   .. method:: after_training_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: after_training_epoch(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_step(self, strategy: PluggableStrategy) -> None


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult



.. py:class:: TaskLoss

   Bases: :class:`PluginMetric[Dict[int, float]]`

   The task loss metric.

   The logged loss value is the per-pattern loss obtained by averaging the loss
   of all eval patterns of a task. This is a common metric used in the
   evaluation of a Continual Learning algorithm.

   Can be safely used when evaluation task-free scenarios, in which case the
   default task label "0" will be used.

   The task losses will be logged at the end of the eval phase. This metric
   doesn't apply to the training phase.

   Creates an instance of the TaskLoss metric.

   .. attribute:: _task_loss
      :annotation: :Dict[int, Loss]

      A dictionary used to store the loss for each task.


   .. method:: reset(self) -> None


   .. method:: result(self) -> Dict[int, float]


   .. method:: update(self, loss: Tensor, patterns: int, task_label: int) -> None


   .. method:: before_eval(self, strategy) -> None


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: _package_result(self) -> MetricResult



.. function:: loss_metrics(*, minibatch=False, epoch=False, epoch_running=False, task=False, train=None, eval=None) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of metric.

   :param minibatch: If True, will return a metric able to log the minibatch
       loss.
   :param epoch: If True, will return a metric able to log the epoch loss.
   :param epoch_running: If True, will return a metric able to log the running
       epoch loss.
   :param task: If True, will return a metric able to log the task loss. This
       metric applies to the eval flow only. If the `eval` parameter is False,
       an error will be raised.
   :param train: If True, metrics will log values for the train flow. Defaults
       to None, which means that the per-metric default value will be used.
   :param eval: If True, metrics will log values for the eval flow. Defaults
       to None, which means that the per-metric default value will be used.

   :return: A list of plugin metrics.


