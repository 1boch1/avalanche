:mod:`avalanche.evaluation.metrics.forgetting`
==============================================

.. py:module:: avalanche.evaluation.metrics.forgetting


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.forgetting.Forgetting



.. py:class:: Forgetting(compute_for_step=False)

   Bases: :class:`PluginMetric[Dict[int, float]]`

   The Forgetting metric, describing the accuracy loss detected for a
   certain task or step.

   This metric, computed separately for each task/step
   is the difference between the accuracy result obtained after
   first training on a task/step and the accuracy result obtained
   on the same task/step at the end of successive steps.

   This metric is computed during the test phase only.

   Creates an instance of the Forgetting metric.

   :param compute_for_step: if True, compute the metric at a step level.
       If False, compute the metric at task level. Default to False.

   .. attribute:: _initial_accuracy
      :annotation: :Dict[int, float]

      The initial accuracy of each task/step.


   .. attribute:: _current_accuracy
      :annotation: :Dict[int, Accuracy]

      The current accuracy of each task/step.


   .. method:: reset(self) -> None

      Resets the metric.

      Beware that this will also reset the initial accuracy of each task/step!

      :return: None.


   .. method:: reset_current_accuracy(self) -> None

      Resets the current accuracy.

      This will preserve the initial accuracy value of each task/step.
      To be used at the beginning of each test step.

      :return: None.


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor, label: int) -> None

      Updates the running accuracy of a task/step given the ground truth and
      predicted labels of a minibatch.

      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :param predicted_y: The ground truth. Both labels and logit vectors
          are supported.
      :param label: The task or step label.
      :return: None.


   .. method:: before_test(self, strategy) -> None


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_test(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: result(self) -> Dict[int, float]

      Return the amount of forgetting for each task/step.

      The forgetting is computed as the accuracy difference between the
      initial task/step accuracy (when first encountered
      in the training stream) and the current accuracy.
      A positive value means that forgetting occurred. A negative value
      means that the accuracy on that task/step increased.

      :return: A dictionary in which keys are task/step labels and the
               values are the forgetting measures
               (as floats in range [-1, 1]).


   .. method:: _package_result(self, label: int) -> MetricResult



