:mod:`avalanche.evaluation.metrics.forgetting`
==============================================

.. py:module:: avalanche.evaluation.metrics.forgetting


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.forgetting.TaskForgetting



.. py:class:: TaskForgetting

   Bases: :class:`PluginMetric[Dict[int, float]]`

   The TaskForgetting metric, describing the accuracy loss detected for a
   certain task.

   This metric, computed separately for each task, is the difference between
   the accuracy result obtained after first training on a task and the accuracy
   result obtained on the same task at the end of successive steps.

   This metric is computed during the test phase only.

   Creates an instance of the Catastrophic TaskForgetting metric.

   .. attribute:: _initial_task_accuracy
      :annotation: :Dict[int, float]

      The initial accuracy of each task.


   .. attribute:: _current_task_accuracy
      :annotation: :Dict[int, Accuracy]

      The current accuracy of each task.


   .. method:: reset(self) -> None

      Resets the metric.

      Beware that this will also reset the initial accuracy of each task!

      :return: None.


   .. method:: reset_current_accuracy(self) -> None

      Resets the current accuracy.

      This will preserve the initial accuracy value of each task. To be used
      at the beginning of each test step.

      :return: None.


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor, task_label: int) -> None

      Updates the running accuracy of a task given the ground truth and
      predicted labels of a minibatch.

      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :param predicted_y: The ground truth. Both labels and logit vectors
          are supported.
      :param task_label: The task label.
      :return: None.


   .. method:: before_test(self, strategy) -> None


   .. method:: after_test_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_test(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: result(self) -> Dict[int, float]

      Return the amount of forgetting for each task.

      The forgetting is computed as the accuracy difference between the
      initial task accuracy (when first encountered in the training stream)
      and the current accuracy. A positive value means that forgetting
      occurred. A negative value means that the accuracy on that task
      increased.

      :return: A dictionary in which keys are task labels and the values are
          the forgetting measures (as floats in range [-1, 1]).


   .. method:: _package_result(self, train_task: int) -> MetricResult



