:mod:`avalanche.evaluation.metrics.forgetting`
==============================================

.. py:module:: avalanche.evaluation.metrics.forgetting


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.evaluation.metrics.forgetting.ExperienceForgetting



.. py:class:: ExperienceForgetting

   Bases: :class:`PluginMetric[Dict[int, float]]`

   The Forgetting metric, describing the accuracy loss detected for a
   certain experience.

   This metric, computed separately for each experience,
   is the difference between the accuracy result obtained after
   first training on a experience and the accuracy result obtained
   on the same experience at the end of successive experiences.

   This metric is computed during the eval phase only.

   Creates an instance of the ExperienceForgetting metric.

   .. attribute:: _initial_accuracy
      :annotation: :Dict[int, float]

      The initial accuracy of each experience.


   .. attribute:: _current_accuracy
      :annotation: :Dict[int, Accuracy]

      The current accuracy of each experience.


   .. attribute:: eval_exp_id
      

      The current evaluation experience id


   .. attribute:: train_exp_id
      

      The last encountered training experience id


   .. method:: reset(self) -> None

      Resets the metric.

      Beware that this will also reset the initial accuracy of each
      experience!

      :return: None.


   .. method:: reset_current_accuracy(self) -> None

      Resets the current accuracy.

      This will preserve the initial accuracy value of each experience.
      To be used at the beginning of each eval experience.

      :return: None.


   .. method:: update(self, true_y: Tensor, predicted_y: Tensor, label: int) -> None

      Updates the running accuracy of a experience given the ground truth and
      predicted labels of a minibatch.

      :param true_y: The ground truth. Both labels and one-hot vectors
          are supported.
      :param predicted_y: The ground truth. Both labels and logit vectors
          are supported.
      :param label: The experience label.
      :return: None.


   .. method:: before_training_exp(self, strategy: PluggableStrategy) -> None


   .. method:: before_eval(self, strategy) -> None


   .. method:: after_eval_iteration(self, strategy: PluggableStrategy) -> None


   .. method:: after_eval_exp(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: result(self) -> float

      Return the amount of forgetting for the eval experience
      associated to `eval_label`.

      The forgetting is computed as the accuracy difference between the
      initial experience accuracy (when first encountered
      in the training stream) and the current accuracy.
      A positive value means that forgetting occurred. A negative value
      means that the accuracy on that experience increased.

      :param eval_label: integer label describing the eval experience
              of which measuring the forgetting
      :return: The amount of forgetting on `eval_exp` experience
               (as float in range [-1, 1]).


   .. method:: _package_result(self, strategy: PluggableStrategy) -> MetricResult


   .. method:: __str__(self)

      Return str(self).



