:mod:`avalanche.training.utils`
===============================

.. py:module:: avalanche.training.utils


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   dataset_utils/index.rst
   transform_dataset/index.rst
   utils/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.training.utils.IDataset
   avalanche.training.utils.IDatasetWithTargets
   avalanche.training.utils.IDatasetWithIntTargets
   avalanche.training.utils.DatasetWithTargets
   avalanche.training.utils.LazyClassMapping
   avalanche.training.utils.LazyConcatTargets
   avalanche.training.utils.LazyTargetsConversion
   avalanche.training.utils.SubsetWithTargets
   avalanche.training.utils.ConcatDatasetWithTargets
   avalanche.training.utils.SequenceDataset
   avalanche.training.utils.TransformationDataset
   avalanche.training.utils.TransformationSubset
   avalanche.training.utils.TransformationTensorDataset



Functions
~~~~~~~~~

.. autoapisummary::

   avalanche.training.utils.get_accuracy
   avalanche.training.utils.train_net
   avalanche.training.utils.preprocess_imgs
   avalanche.training.utils.maybe_cuda
   avalanche.training.utils.change_lr
   avalanche.training.utils.set_classifier
   avalanche.training.utils.reset_classifier
   avalanche.training.utils.shuffle_in_unison
   avalanche.training.utils.softmax
   avalanche.training.utils.count_lines
   avalanche.training.utils.pad_data
   avalanche.training.utils.compute_one_hot
   avalanche.training.utils.imagenet_batch_preproc
   avalanche.training.utils.load_all_dataset
   avalanche.training.utils.concat_datasets_sequentially
   avalanche.training.utils.find_list_from_index
   avalanche.training.utils.manage_advanced_indexing


.. function:: get_accuracy(model, criterion, batch_size, test_x, test_y, test_it, device=None, mask=None)

   Test accuracy given net and data. 


.. function:: train_net(optimizer, model, criterion, batch_size, train_x, train_y, train_it, device=None, mask=None)

   Train net from memory using pytorch 


.. function:: preprocess_imgs(img_batch, scale=True, norm=True, channel_first=True)

   Here we get a batch of PIL imgs and we return them normalized as for
   the pytorch pre-trained models. 


.. function:: maybe_cuda(what, use_cuda=True, **kw)

   Moves `what` to CUDA and returns it, if `use_cuda` and it's available.
       


.. function:: change_lr(optimizer, lr)

   Change the learning rate of the optimizer


.. function:: set_classifier(model, weigth, bias, clas=None)

   Change weights and biases of the last layer in the network. 


.. function:: reset_classifier(model, val=0, std=None)

   Set weights and biases of the last layer in the network to zero. 


.. function:: shuffle_in_unison(dataset, seed=None, in_place=False)

   Shuffle two (or more) list in unison. It's important to shuffle the images
   and the labels maintaining their correspondence.

   :args dataset: list of shuffle with the same order.
   :args seed: set of fixed Cifar parameters.
   :args in_place: if we want to shuffle the same data or we want
                    to return a new shuffled dataset.

   :return: train and test sets composed of images and labels, if in_place
       is set to False.


.. function:: softmax(x)

   Compute softmax values for each sets of scores in x. 


.. function:: count_lines(fpath)

   Count line in file. 


.. function:: pad_data(dataset, mb_size)

   Padding all the matrices contained in dataset to suit the mini-batch
   size. We assume they have the same shape. 


.. function:: compute_one_hot(train_y, class_count)

   Compute one-hot from labels. 


.. function:: imagenet_batch_preproc(img_batch, rgb_swap=True, channel_first=True, avg_sub=True)

   Pre-process batch of PIL img for Imagenet pre-trained models with caffe.
   It may be need adjustements depending on the pre-trained model
   since it is training dependent. 


.. function:: load_all_dataset(dataset: Dataset, num_workers: int = 0)

   Retrieves the contents of a whole dataset by using a DataLoader

   :param dataset: The dataset
   :param num_workers: The number of workers the DataLoader should use.
       Defaults to 0.
   :return: The content of the whole Dataset


.. py:class:: IDataset

   Bases: :class:`Protocol[T_co]`

   Protocol definition of a Dataset.

   Note: no __add__ method is defined.

   .. method:: __getitem__(self, index: int) -> Tuple[T_co, int]


   .. method:: __len__(self) -> int



.. py:class:: IDatasetWithTargets

   Bases: :class:`IDataset[T_co]`, :class:`typing.Protocol`

   Protocol definition of a Dataset that has a valid targets field (like the
   Datasets in the torchvision package).

   Note: no __add__ method is defined.

   Create and return a new object.  See help(type) for accurate signature.

   .. attribute:: targets
      :annotation: :Sequence[SupportsInt]

      A sequence of ints or a PyTorch Tensor or a NumPy ndarray describing the
      label of each pattern contained in the dataset.


   .. method:: __getitem__(self, index: int) -> Tuple[T_co, int]


   .. method:: __len__(self) -> int



.. py:class:: IDatasetWithIntTargets

   Bases: :class:`IDatasetWithTargets[T_co]`, :class:`typing.Protocol`

   Protocol definition of a Dataset that has a valid targets field (like the
   Datasets in the torchvision package) where the targets field is a sequence
   of native ints.

   Create and return a new object.  See help(type) for accurate signature.

   .. attribute:: targets
      :annotation: :Sequence[int]

      A sequence of ints describing the label of each pattern contained in the
      dataset.


   .. method:: __getitem__(self, index: int) -> Tuple[T_co, int]


   .. method:: __len__(self) -> int



.. py:class:: DatasetWithTargets

   Bases: :class:`IDatasetWithIntTargets[T_co]`, :class:`torch.utils.data.dataset.Dataset`

   Dataset that has a valid targets field (like the Datasets in the
   torchvision package) where the targets field is a sequence of native ints.

   The actual value of the targets field should be set by the child class.

   .. attribute:: targets
      :annotation: = []

      A sequence of ints describing the label of each pattern contained in the
      dataset.



.. py:class:: LazyClassMapping(targets: Sequence[SupportsInt], indices: Union[Sequence[int], None], mapping: Optional[Sequence[int]] = None)

   Bases: :class:`Sequence[int]`

   This class is used when in need of lazy populating a targets field whose
   elements need to be filtered out (when subsetting, see
   :class:`torch.utils.data.Subset`) and/or transformed (remapped). This will
   allow for a more efficient memory usage as the conversion is done on the fly
   instead of actually allocating a new targets list.

   Initialize self.  See help(type(self)) for accurate signature.

   .. method:: __len__(self)


   .. method:: __getitem__(self, item_idx) -> int


   .. method:: __str__(self)

      Return str(self).



.. py:class:: LazyConcatTargets(targets_list: Sequence[Sequence[SupportsInt]])

   Bases: :class:`Sequence[int]`

   Defines a lazy targets concatenation.

   This class is used when in need of lazy populating a targets created
   as the concatenation of the targets field of multiple datasets.
   This will allow for a more efficient memory usage as the concatenation is
   done on the fly instead of actually allocating a new targets list.

   Initialize self.  See help(type(self)) for accurate signature.

   .. method:: __len__(self)


   .. method:: __getitem__(self, item_idx) -> int


   .. method:: __str__(self)

      Return str(self).



.. py:class:: LazyTargetsConversion(targets: Sequence[SupportsInt])

   Bases: :class:`Sequence[int]`

   Defines a lazy conversion of targets defined in some other format.

   Initialize self.  See help(type(self)) for accurate signature.

   .. method:: __len__(self)


   .. method:: __getitem__(self, item_idx) -> int


   .. method:: __str__(self)

      Return str(self).



.. py:class:: SubsetWithTargets(dataset: IDatasetWithTargets[T_co], indices: Union[Sequence[int], None], class_mapping: Optional[Sequence[int]] = None)

   Bases: :class:`DatasetWithTargets[T_co]`

   A Dataset that behaves like a PyTorch :class:`torch.utils.data.Subset`.
   However, this dataset also supports the targets field and class mapping.

   .. method:: __getitem__(self, idx)


   .. method:: __len__(self) -> int



.. py:class:: ConcatDatasetWithTargets(datasets: Sequence[IDatasetWithTargets[T_co]])

   Bases: :class:`DatasetWithTargets[T_co]`

   A Dataset that behaves like a PyTorch
   :class:`torch.utils.data.ConcatDataset`. However, this dataset also
   supports the targets field.

   .. method:: __getitem__(self, idx)


   .. method:: __len__(self) -> int



.. function:: concat_datasets_sequentially(train_dataset_list: Sequence[IDatasetWithTargets[T_co]], test_dataset_list: Sequence[IDatasetWithTargets[T_co]]) -> Tuple[IDatasetWithIntTargets[T_co], IDatasetWithIntTargets[T_co], List[list]]

   Concatenates a list of datasets. This is completely different from
   :class:`ConcatDataset`, in which datasets are merged together without
   other processing. Instead, this function re-maps the datasets class IDs.
   For instance:
   let the dataset[0] contain patterns of 3 different classes,
   let the dataset[1] contain patterns of 2 different classes, then class IDs
   will be mapped as follows:

   dataset[0] class "0" -> new class ID is "0"

   dataset[0] class "1" -> new class ID is "1"

   dataset[0] class "2" -> new class ID is "2"

   dataset[1] class "0" -> new class ID is "3"

   dataset[1] class "1" -> new class ID is "4"

   ... -> ...

   dataset[-1] class "C-1" -> new class ID is "overall_n_classes-1"

   In contrast, using PyTorch ConcatDataset:

   dataset[0] class "0" -> ID is "0"

   dataset[0] class "1" -> ID is "1"

   dataset[0] class "2" -> ID is "2"

   dataset[1] class "0" -> ID is "0"

   dataset[1] class "1" -> ID is "1"

   Note: ``train_dataset_list`` and ``test_dataset_list`` must have the same
   number of datasets.

   :param train_dataset_list: A list of training datasets
   :param test_dataset_list: A list of test datasets

   :returns: A concatenated dataset.


.. py:class:: SequenceDataset(dataset_x: Sequence[T_co], dataset_y: Sequence[SupportsInt])

   Bases: :class:`DatasetWithTargets[T_co]`

   A Dataset that wraps existing ndarrays, Tensors, lists... to provide
   basic Dataset functionalities. Very similar to TensorDataset.

   Creates a ``SequenceDataset`` instance.

   :param dataset_x: An sequence, Tensor or ndarray representing the X
       values of the patterns.
   :param dataset_y: An sequence, Tensor int or ndarray of integers
       representing the Y values of the patterns.

   .. method:: __getitem__(self, idx)


   .. method:: __len__(self) -> int



.. function:: find_list_from_index(pattern_idx: int, list_sizes: Sequence[int], max_size: int)


.. function:: manage_advanced_indexing(idx, single_element_getter, max_length)

   Utility function used to manage the advanced indexing and slicing.

   If more than a pattern is selected, the X and Y values will be merged
   in two separate torch Tensor objects using the stack operation.

   :param idx: Either an in, a slice object or a list (including ndarrays and
       torch Tensors) of indexes.
   :param single_element_getter: A callable used to obtain a single element
       given its int index.
   :param max_length: The maximum sequence length.
   :return: A tuple consisting of two tensors containing the X and Y values
       of the patterns addressed by the idx parameter.


.. py:class:: TransformationDataset(dataset: IDatasetWithTargets[T_co], transform: Callable[[T_co], Any] = None, target_transform: Callable[[int], int] = None)

   Bases: :class:`DatasetWithTargets[T_co]`, :class:`Generic[T_co]`

   A Dataset that applies transformations before returning patterns/targets.
   Also, this Dataset supports slicing and advanced indexing.

   Creates a ``TransformationDataset`` instance.

   :param dataset: The dataset to decorate. Beware that
       TransformationDataset will not overwrite transformations already
       applied by this dataset.
   :param transform: A function/transform that takes in an PIL image and
       returns a transformed version.
   :param target_transform: A function/transform that takes in the target
       and transforms it.

   .. attribute:: transform
      

      A function/transform that takes in an PIL image and returns a 
      transformed version.


   .. attribute:: target_transform
      

      A function/transform that takes in the target and transforms it.


   .. attribute:: dataset
      :annotation: :IDatasetWithTargets[T_co]

      The original dataset.


   .. attribute:: targets
      :annotation: :Sequence[int]

      A sequence of ints describing the label of each pattern contained in the
      dataset.


   .. method:: __getitem__(self, idx)


   .. method:: __len__(self)


   .. method:: __get_single_item(self, idx: int)



.. py:class:: TransformationSubset(dataset: IDatasetWithTargets[T_co], indices: Sequence[int] = None, class_mapping: Sequence[int] = None, transform=None, target_transform=None)

   Bases: :class:`TransformationDataset[T_co]`

   A Dataset that behaves like a pytorch :class:`torch.utils.data.Subset`.
   This Dataset also supports transformations, slicing, advanced indexing,
   the targets field and class mapping.

   Creates a ``TransformationSubset`` instance.

   :param dataset: The whole dataset.
   :param indices: Indices in the whole set selected for subset. Can
       be None, which means that the whole dataset will be returned.
   :param class_mapping: A list that, for each possible target (Y) value,
       contains its corresponding remapped value. Can be None.
   :param transform: A function/transform that takes a pattern obtained
       from the dataset and returns a transformed version.
   :param target_transform: A function/transform that takes in the target
       and transforms it.


.. py:class:: TransformationTensorDataset(dataset_x: Sequence[T_co], dataset_y: Sequence[SupportsInt], transform=None, target_transform=None)

   Bases: :class:`TransformationDataset[T_co]`

   A Dataset that wraps existing ndarrays, Tensors, lists... to provide
   basic Dataset functionalities. Very similar to TensorDataset from PyTorch,
   this Dataset also supports transformations, slicing, advanced indexing and
   the targets field.

   Creates a ``TransformationTensorDataset`` instance.

   :param dataset_x: An sequence, Tensor or ndarray representing the X
       values of the patterns.
   :param dataset_y: An sequence, Tensor int or ndarray of integers
       representing the Y values of the patterns.
   :param transform: A function/transform that takes in a single element
       from the ``dataset_x`` sequence and returns a transformed version.
   :param target_transform: A function/transform that takes in the target
       and transforms it.


