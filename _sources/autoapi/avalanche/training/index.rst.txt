:mod:`avalanche.training`
=========================

.. py:module:: avalanche.training

.. autoapi-nested-parse::

   The :py:mod:`training` module provides a generic continual learning training
   class (:py:class:`BaseStrategy`) and implementations of the most common
   CL strategies. These are provided either as standalone strategies in
   :py:mod:`training.strategies` or as plugins (:py:mod:`training.plugins`) that
   can be easily combined with your own strategy.



Subpackages
-----------
.. toctree::
   :titlesonly:
   :maxdepth: 3

   plugins/index.rst
   strategies/index.rst


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   utils/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.training.EvaluationPlugin



Functions
~~~~~~~~~

.. autoapisummary::

   avalanche.training.accuracy_metrics
   avalanche.training.loss_metrics


.. function:: accuracy_metrics(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param minibatch: If True, will return a metric able to log
       the minibatch accuracy at training time.
   :param epoch: If True, will return a metric able to log
       the epoch accuracy at training time.
   :param epoch_running: If True, will return a metric able to log
       the running epoch accuracy at training time.
   :param experience: If True, will return a metric able to log
       the accuracy on each evaluation experience.
   :param stream: If True, will return a metric able to log
       the accuracy averaged over the entire evaluation stream of experiences.

   :return: A list of plugin metrics.


.. function:: loss_metrics(*, minibatch=False, epoch=False, epoch_running=False, experience=False, stream=False) -> List[PluginMetric]

   Helper method that can be used to obtain the desired set of
   plugin metrics.

   :param minibatch: If True, will return a metric able to log
       the minibatch loss at training time.
   :param epoch: If True, will return a metric able to log
       the epoch loss at training time.
   :param epoch_running: If True, will return a metric able to log
       the running epoch loss at training time.
   :param experience: If True, will return a metric able to log
       the loss on each evaluation experience.
   :param stream: If True, will return a metric able to log
       the loss averaged over the entire evaluation stream of experiences.

   :return: A list of plugin metrics.


.. py:class:: EvaluationPlugin(*metrics: Union['PluginMetric', Sequence['PluginMetric']], loggers: Union['StrategyLogger', Sequence['StrategyLogger']] = None, collect_all=True)

   Bases: :class:`avalanche.training.plugins.strategy_plugin.StrategyPlugin`

   An evaluation plugin that obtains relevant data from the
   training and eval loops of the strategy through callbacks.

   This plugin updates the given metrics and logs them using the provided
   loggers.

   Creates an instance of the evaluation plugin.

   :param metrics: The metrics to compute.
   :param loggers: The loggers to be used to log the metric values.
   :param collect_curves (bool): enables the collection of the metric
       curves. If True `self.metric_curves` stores all the values of
       each curve in a dictionary. Please disable this if you log large
       values (embeddings, parameters) and you want to reduce memory usage.

   .. method:: _update_metrics(self, strategy: BaseStrategy, callback: str)


   .. method:: before_training(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: adapt_train_dataset(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_epoch(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_training_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_backward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_backward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_update(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_update(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_epoch(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_training(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval(self, strategy: BaseStrategy, **kwargs)


   .. method:: adapt_eval_dataset(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_exp(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_iteration(self, strategy: BaseStrategy, **kwargs)


   .. method:: before_eval_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_forward(self, strategy: BaseStrategy, **kwargs)


   .. method:: after_eval_iteration(self, strategy: BaseStrategy, **kwargs)



.. data:: default_logger
   

   

