:mod:`avalanche.training.strategies.base_strategy`
==================================================

.. py:module:: avalanche.training.strategies.base_strategy


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.training.strategies.base_strategy.BaseStrategy



.. py:class:: BaseStrategy(model: Module, optimizer: Optimizer, criterion, train_mb_size: int = 1, train_epochs: int = 1, eval_mb_size: int = 1, device='cpu', plugins: Optional[Sequence['StrategyPlugin']] = None, evaluator=default_logger)

   BaseStrategy is the super class of all task-based continual learning
   strategies. It implements a basic training loop and callback system
   that allows to execute code at each experience of the training loop.
   Plugins can be used to implement callbacks to augment the training
   loop with additional behavior (e.g. a memory buffer for replay).

   **Scenarios**
   This strategy supports several continual learning scenarios:

   * class-incremental scenarios (no task labels)
   * multi-task scenarios, where task labels are provided)
   * multi-incremental scenarios, where the same task may be revisited

   The exact scenario depends on the data stream and whether it provides
   the task labels.

   **Training loop**
   The training loop and its callbacks are organized as follows::
       train
           before_training
           before_training_exp
           adapt_train_dataset
           make_train_dataloader
           before_training_epoch
               before_training_iteration
                   before_forward
                   after_forward
                   before_backward
                   after_backward
               after_training_iteration
               before_update
               after_update
           after_training_epoch
           after_training_exp
           after_training

   **Evaluation loop**
   The evaluation loop and its callbacks are organized as follows::
       eval
           before_eval
           adapt_eval_dataset
           make_eval_dataloader
           before_eval_exp
               eval_epoch
                   before_eval_iteration
                   before_eval_forward
                   after_eval_forward
                   after_eval_iteration
           after_eval_exp
           after_eval

   :param model: PyTorch model.
   :param optimizer: PyTorch optimizer.
   :param criterion: loss function.
   :param train_mb_size: mini-batch size for training.
   :param train_epochs: number of training epochs.
   :param eval_mb_size: mini-batch size for eval.
   :param device: PyTorch device where the model will be allocated.
   :param plugins: (optional) list of StrategyPlugins.
   :param evaluator: (optional) instance of EvaluationPlugin for logging
       and metric computations. None to remove logging.

   .. attribute:: model
      :annotation: :Module

      PyTorch model. 


   .. attribute:: criterion
      

      Loss function. 


   .. attribute:: optimizer
      

      PyTorch optimizer. 


   .. attribute:: train_epochs
      :annotation: :int

      Number of training epochs. 


   .. attribute:: train_mb_size
      :annotation: :int

      Training mini-batch size. 


   .. attribute:: eval_mb_size
      :annotation: :int

      Eval mini-batch size. 


   .. attribute:: device
      

      PyTorch device where the model will be allocated. 


   .. attribute:: plugins
      

      List of `StrategyPlugin`s. 


   .. attribute:: evaluator
      

      EvaluationPlugin used for logging and metric computations. 


   .. attribute:: training_exp_counter
      :annotation: = 0

      Counts the number of training steps. +1 at the end of each 
      experience. 


   .. attribute:: eval_exp_id
      

      Label of the currently evaluated experience. Only at eval time. 


   .. attribute:: epoch
      :annotation: :Optional[int]

      Epoch counter. 


   .. attribute:: experience
      

      Current experience. 


   .. attribute:: adapted_dataset
      

      Data used to train. It may be modified by plugins. Plugins can 
      append data to it (e.g. for replay). 
       
      .. note:: 
          This dataset may contain samples from different experiences. If you 
          want the original data for the current experience  
          use :attr:`.BaseStrategy.experience`.


   .. attribute:: current_dataloader
      

      Dataloader. 


   .. attribute:: mb_it
      

      Iteration counter. Reset at the start of a new epoch. 


   .. attribute:: mb_x
      

      Current mini-batch input. 


   .. attribute:: mb_y
      

      Current mini-batch target. 


   .. attribute:: loss
      

      Loss of the current mini-batch. 


   .. attribute:: logits
      

      Logits computed on the current mini-batch. 


   .. attribute:: train_task_label
      :annotation: :Optional[int]

      Label of the current experience (train time). 


   .. attribute:: eval_task_label
      :annotation: :Optional[int]

      Label of the current experience (eval time). 


   .. attribute:: is_training
      :annotation: :bool = False

      True if the strategy is in training mode. 


   .. method:: is_eval(self)
      :property:

      True if the strategy is in evaluation mode. 


   .. method:: update_optimizer(self, old_params, new_params, reset_state=True)

      Update the optimizer by substituting old_params with new_params.

      :param old_params: List of old trainable parameters.
      :param new_params: List of new trainable parameters.
      :param reset_state: Wheter to reset the optimizer's state.
          Defaults to True.
      :return:


   .. method:: add_new_params_to_optimizer(self, new_params)

      Add new parameters to the trainable parameters.

      :param new_params: list of trainable parameters


   .. method:: train(self, experiences: Union[IExperience, Sequence[IExperience]], **kwargs)

      Training loop. if experiences is a single element trains on it.
      If it is a sequence, trains the model on each experience in order.
      This is different from joint training on the entire stream.

      :param experiences: single IExperience or sequence.


   .. method:: train_exp(self, experience: IExperience, **kwargs)

      Training loop over a single IExperience object.

      :param experience: CL experience information.
      :param kwargs: custom arguments.


   .. method:: eval(self, exp_list: Union[IExperience, Sequence[IExperience]], **kwargs)

      Evaluate the current model on a series of experiences.

      :param exp_list: CL experience information.
      :param kwargs: custom arguments.


   .. method:: before_training_exp(self, **kwargs)

      Called  after the dataset and data loader creation and
      before the training loop.


   .. method:: make_train_dataloader(self, num_workers=0, shuffle=True, **kwargs)

      Called after the dataset instantiation. Initialize the data loader.
      :param num_workers: number of thread workers for the data loading.
      :param shuffle: True if the data should be shuffled, False otherwise.


   .. method:: make_eval_dataloader(self, num_workers=0, **kwargs)

      Initialize the eval data loader.
      :param num_workers:
      :param kwargs:
      :return:


   .. method:: adapt_train_dataset(self, **kwargs)

      Called after the dataset initialization and before the
      dataloader initialization. Allows to customize the dataset.
      :param kwargs:
      :return:


   .. method:: before_training_epoch(self, **kwargs)

      Called at the beginning of a new training epoch.
      :param kwargs:
      :return:


   .. method:: training_epoch(self, **kwargs)

      Training epoch.
      :param kwargs:
      :return:


   .. method:: before_training(self, **kwargs)


   .. method:: after_training(self, **kwargs)


   .. method:: before_training_iteration(self, **kwargs)


   .. method:: before_forward(self, **kwargs)


   .. method:: after_forward(self, **kwargs)


   .. method:: before_backward(self, **kwargs)


   .. method:: after_backward(self, **kwargs)


   .. method:: after_training_iteration(self, **kwargs)


   .. method:: before_update(self, **kwargs)


   .. method:: after_update(self, **kwargs)


   .. method:: after_training_epoch(self, **kwargs)


   .. method:: after_training_exp(self, **kwargs)


   .. method:: before_eval(self, **kwargs)


   .. method:: before_eval_exp(self, **kwargs)


   .. method:: adapt_eval_dataset(self, **kwargs)


   .. method:: eval_epoch(self, **kwargs)


   .. method:: after_eval_exp(self, **kwargs)


   .. method:: after_eval(self, **kwargs)


   .. method:: before_eval_iteration(self, **kwargs)


   .. method:: before_eval_forward(self, **kwargs)


   .. method:: after_eval_forward(self, **kwargs)


   .. method:: after_eval_iteration(self, **kwargs)



