:mod:`avalanche.training.strategies.base_strategy`
==================================================

.. py:module:: avalanche.training.strategies.base_strategy


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.training.strategies.base_strategy.BaseStrategy



.. py:class:: BaseStrategy(model: Module, optimizer: Optimizer, criterion, train_mb_size: int = 1, train_epochs: int = 1, eval_mb_size: int = 1, device='cpu', plugins: Optional[Sequence['StrategyPlugin']] = None, evaluator=default_logger)

   BaseStrategy is the super class of all task-based continual learning
   strategies. It implements a basic training loop and callback system
   that allows to execute code at each experience of the training loop.
   Plugins can be used to implement callbacks to augment the training
   loop with additional behavior (e.g. a memory buffer for replay).

   This strategy supports several continual learning scenarios:
   - class-incremental scenarios (no task labels)
   - multi-task scenarios, where task labels are provided)
   - multi-incremental scenarios, where the same task may be revisited

   The exact scenario depends on the data stream and whether it provides
   the task labels.

   :param model: PyTorch model.
   :param optimizer: PyTorch optimizer.
   :param criterion: loss function.
   :param train_mb_size: mini-batch size for training.
   :param train_epochs: number of training epochs.
   :param eval_mb_size: mini-batch size for eval.
   :param device: PyTorch device to run the model.
   :param plugins: (optional) list of StrategyPlugins.
   :param evaluator: (optional) instance of EvaluationPlugin for logging
       and metric computations. None to remove logging.

   .. method:: is_eval(self)
      :property:


   .. method:: update_optimizer(self, old_params, new_params, reset_state=True)

      Update the optimizer by substituting old_params with new_params.

      :param old_params: List of old trainable parameters.
      :param new_params: List of new trainable parameters.
      :param reset_state: Wheter to reset the optimizer's state.
          Defaults to True.
      :return:


   .. method:: add_new_params_to_optimizer(self, new_params)

      Add new parameters to the trainable parameters.

      :param new_params: list of trainable parameters


   .. method:: train(self, experiences: Union[IExperience, Sequence[IExperience]], **kwargs)

      Training loop. if experiences is a single element trains on it.
      If it is a sequence, trains the model on each experience in order.
      This is different from joint training on the entire stream.

      :param experiences: single IExperience or sequence.


   .. method:: train_exp(self, experience: IExperience, **kwargs)

      Training loop over a single IExperience object.

      :param experience: CL experience information.
      :param kwargs: custom arguments.


   .. method:: eval(self, exp_list: Union[IExperience, Sequence[IExperience]], **kwargs)

      Evaluate the current model on a series of experiences.

      :param exp_list: CL experience information.
      :param kwargs: custom arguments.


   .. method:: before_training_exp(self, **kwargs)

      Called  after the dataset and data loader creation and
      before the training loop.


   .. method:: make_train_dataloader(self, num_workers=0, shuffle=True, **kwargs)

      Called after the dataset instantiation. Initialize the data loader.
      :param num_workers: number of thread workers for the data loading.
      :param shuffle: True if the data should be shuffled, False otherwise.


   .. method:: make_eval_dataloader(self, num_workers=0, **kwargs)

      Initialize the eval data loader.
      :param num_workers:
      :param kwargs:
      :return:


   .. method:: adapt_train_dataset(self, **kwargs)

      Called after the dataset initialization and before the
      dataloader initialization. Allows to customize the dataset.
      :param kwargs:
      :return:


   .. method:: before_training_epoch(self, **kwargs)

      Called at the beginning of a new training epoch.
      :param kwargs:
      :return:


   .. method:: training_epoch(self, **kwargs)

      Training epoch.
      :param kwargs:
      :return:


   .. method:: before_training(self, **kwargs)


   .. method:: after_training(self, **kwargs)


   .. method:: before_training_iteration(self, **kwargs)


   .. method:: before_forward(self, **kwargs)


   .. method:: after_forward(self, **kwargs)


   .. method:: before_backward(self, **kwargs)


   .. method:: after_backward(self, **kwargs)


   .. method:: after_training_iteration(self, **kwargs)


   .. method:: before_update(self, **kwargs)


   .. method:: after_update(self, **kwargs)


   .. method:: after_training_epoch(self, **kwargs)


   .. method:: after_training_exp(self, **kwargs)


   .. method:: before_eval(self, **kwargs)


   .. method:: before_eval_exp(self, **kwargs)


   .. method:: adapt_eval_dataset(self, **kwargs)


   .. method:: eval_epoch(self, **kwargs)


   .. method:: after_eval_exp(self, **kwargs)


   .. method:: after_eval(self, **kwargs)


   .. method:: before_eval_iteration(self, **kwargs)


   .. method:: before_eval_forward(self, **kwargs)


   .. method:: after_eval_forward(self, **kwargs)


   .. method:: after_eval_iteration(self, **kwargs)



