:mod:`avalanche.training.strategies`
====================================

.. py:module:: avalanche.training.strategies


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   base_strategy/index.rst
   joint_training/index.rst
   strategies/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.training.strategies.BaseStrategy
   avalanche.training.strategies.JointTraining
   avalanche.training.strategies.Naive
   avalanche.training.strategies.CWRStar
   avalanche.training.strategies.Replay
   avalanche.training.strategies.GDumb
   avalanche.training.strategies.Cumulative
   avalanche.training.strategies.LwF
   avalanche.training.strategies.AGEM
   avalanche.training.strategies.GEM
   avalanche.training.strategies.EWC
   avalanche.training.strategies.SynapticIntelligence
   avalanche.training.strategies.AR1



.. py:class:: BaseStrategy(model: Module, optimizer: Optimizer, criterion, train_mb_size: int = 1, train_epochs: int = 1, eval_mb_size: int = 1, device='cpu', plugins: Optional[Sequence['StrategyPlugin']] = None, evaluator=default_logger)

   BaseStrategy is the super class of all task-based continual learning
   strategies. It implements a basic training loop and callback system
   that allows to execute code at each step of the training loop. Plugins
   can be used to implement callbacks to augment the training loop with
   additional behavior (e.g. a memory buffer for replay).

   This strategy supports several continual learning scenarios:
   - class-incremental scenarios (no task labels)
   - multi-task scenarios, where task labels are provided)
   - multi-incremental scenarios, where the same task may be revisited

   The exact scenario depends on the data stream and whether it provides
   the task labels.

   :param model: PyTorch model.
   :param optimizer: PyTorch optimizer.
   :param criterion: loss function.
   :param train_mb_size: mini-batch size for training.
   :param train_epochs: number of training epochs.
   :param eval_mb_size: mini-batch size for eval.
   :param device: PyTorch device to run the model.
   :param plugins: (optional) list of StrategyPlugins.
   :param evaluator: (optional) instance of EvaluationPlugin for logging
       and metric computations. None to remove logging.

   .. method:: is_eval(self)
      :property:


   .. method:: update_optimizer(self, old_params, new_params, reset_state=True)

      Update the optimizer by substituting old_params with new_params.

      :param old_params: List of old trainable parameters.
      :param new_params: List of new trainable parameters.
      :param reset_state: Wheter to reset the optimizer's state.
          Defaults to True.
      :return:


   .. method:: add_new_params_to_optimizer(self, new_params)

      Add new parameters to the trainable parameters.

      :param new_params: list of trainable parameters


   .. method:: train(self, step_infos: Union[IStepInfo, Sequence[IStepInfo]], **kwargs)

      Training loop. if step_infos is a single element trains on it.
      If it is a sequence, trains the model on each step in order.
      This is different from joint training on the entire stream.

      :param step_infos: single IStepInfo or sequence.


   .. method:: train_step(self, step_info: IStepInfo, **kwargs)

      Training loop over a single IStepInfo object.

      :param step_info: CL step information.
      :param kwargs: custom arguments.


   .. method:: eval(self, step_list: Union[IStepInfo, Sequence[IStepInfo]], **kwargs)

      Evaluate the current model on a series of steps.

      :param step_info: CL step information.
      :param kwargs: custom arguments.


   .. method:: before_training_step(self, **kwargs)

      Called  after the dataset and data loader creation and
      before the training loop.


   .. method:: make_train_dataloader(self, num_workers=0, shuffle=True, **kwargs)

      Called after the dataset instantiation. Initialize the data loader.
      :param num_workers: number of thread workers for the data loading.
      :param shuffle: True if the data should be shuffled, False otherwise.


   .. method:: make_eval_dataloader(self, num_workers=0, **kwargs)

      Initialize the eval data loader.
      :param num_workers:
      :param kwargs:
      :return:


   .. method:: adapt_train_dataset(self, **kwargs)

      Called after the dataset initialization and before the
      dataloader initialization. Allows to customize the dataset.
      :param kwargs:
      :return:


   .. method:: before_training_epoch(self, **kwargs)

      Called at the beginning of a new training epoch.
      :param kwargs:
      :return:


   .. method:: training_epoch(self, **kwargs)

      Training epoch.
      :param kwargs:
      :return:


   .. method:: before_training(self, **kwargs)


   .. method:: after_training(self, **kwargs)


   .. method:: before_training_iteration(self, **kwargs)


   .. method:: before_forward(self, **kwargs)


   .. method:: after_forward(self, **kwargs)


   .. method:: before_backward(self, **kwargs)


   .. method:: after_backward(self, **kwargs)


   .. method:: after_training_iteration(self, **kwargs)


   .. method:: before_update(self, **kwargs)


   .. method:: after_update(self, **kwargs)


   .. method:: after_training_epoch(self, **kwargs)


   .. method:: after_training_step(self, **kwargs)


   .. method:: before_eval(self, **kwargs)


   .. method:: before_eval_step(self, **kwargs)


   .. method:: adapt_eval_dataset(self, **kwargs)


   .. method:: eval_epoch(self, **kwargs)


   .. method:: after_eval_step(self, **kwargs)


   .. method:: after_eval(self, **kwargs)


   .. method:: before_eval_iteration(self, **kwargs)


   .. method:: before_eval_forward(self, **kwargs)


   .. method:: after_eval_forward(self, **kwargs)


   .. method:: after_eval_iteration(self, **kwargs)



.. py:class:: JointTraining(model: Module, optimizer: Optimizer, criterion, classifier_field: str = 'classifier', train_mb_size: int = 1, train_epochs: int = 1, eval_mb_size: int = 1, device='cpu', plugins: Optional[Sequence['StrategyPlugin']] = None)

   JointStrategy is a super class for all the joint training strategies.
   This means that it is not a continual learning strategy but it can be
   used as an "offline" upper bound for them. This strategy takes in
   input an entire stream and learn from it one shot. It supports unique
   tasks (i.e. streams with a unique task label) or multiple tasks.

   :param model: PyTorch model.
   :param optimizer: PyTorch optimizer.
   :param criterion: loss function.
   :param classifier_field: (optional) to specify the name of output layer.
   :param train_mb_size: mini-batch size for training.
   :param train_epochs: number of training epochs.
   :param eval_mb_size: mini-batch size for eval.
   :param device: PyTorch device to run the model.
   :param plugins: (optional) list of StrategyPlugins.

   .. method:: is_eval(self)
      :property:


   .. method:: set_task_layer(self, task_label)

      Sets the correct task layer. Creates a new head for previously
      unseen tasks.

      :param task_label: the task label integer identifying the task.
      :return: None


   .. method:: create_task_layer(self, n_output_units: int, previous_task_layer=None)

      Creates a new task layer.

      By default, this method will create a new :class:`Linear` layer with
      n_output_units" output units. If  "previous_task_layer" is None,
      the name of the classifier field is used to retrieve the amount of
      input features.

      This method will also be used to create a new layer when expanding
      an existing task head.

      This method can be overridden by the user so that a layer different
      from :class:`Linear` can be created.

      :param n_output_units: The number of output units.
      :param previous_task_layer: If not None, the previously created layer
           for the same task.
      :return: The new layer.


   .. method:: train(self, step_infos: Sequence[IStepInfo], **kwargs)

      Training loop. it trains only on a sequence of steps (a stream).
      WARNING: Please take in mind that it trains on it "in parallel" not
      iteratively as in the BaseStrategy train method. This is the main
      difference from the JointTraining and BaseStrategy classes.

      :param step_infos: sequence of IStepInfo (a stream).
      :return:


   .. method:: make_train_dataloader(self, num_workers=0, shuffle=True, **kwargs)

      Called after the dataset instantiation. Initialize the data loader.
      :param num_workers: number of thread workers for the data laoding.
      :param shuffle: True if the data should be shuffled, False otherwise.


   .. method:: training_epoch(self, **kwargs)

      Training epoch. How does it work:
      1. From each the n data loader (one for task) we load a mini-batch.
      2. net forward with the right head accumlating gradients for all the n
      mini-batches.
      3. Update the gradient.
      1., 2. and 3. are repeated till all the data of each of the data
      loader have been processed at least once. If a data loader finishes
      his mini-batches, it starts again from the first mini-batch. This
      assumes that each task as roughly the same amount of data.

      :param kwargs: named arguments eventually passed to this method.
      :return: None.


   .. method:: add_new_params_to_optimizer(self, new_params)

      Add new parameters to the trainable parameters.

      :param new_params: list of trainable parameters


   .. method:: eval(self, step_list: Union[IStepInfo, Sequence[IStepInfo]], **kwargs)

      Evaluate the current model on a series of steps.

      :param step_info: CL step information.
      :param kwargs: custom arguments.
      :return: evaluation plugin evaluation results.


   .. method:: before_training_step(self, **kwargs)

      Called  after the dataset and data loader creation and
      before the training loop.


   .. method:: make_eval_dataloader(self, num_workers=0, **kwargs)

      Initialize the eval data loader.
      :param num_workers:
      :param kwargs:
      :return:


   .. method:: adapt_train_dataset(self, **kwargs)

      Called after the dataset initialization and before the
      dataloader initialization. Allows to customize the dataset.
      :param kwargs:
      :return:


   .. method:: before_training_epoch(self, **kwargs)

      Called at the beginning of a new training epoch.
      :param kwargs:
      :return:


   .. method:: before_training(self, **kwargs)


   .. method:: after_training(self, **kwargs)


   .. method:: before_training_iteration(self, **kwargs)


   .. method:: before_forward(self, **kwargs)


   .. method:: after_forward(self, **kwargs)


   .. method:: before_backward(self, **kwargs)


   .. method:: after_backward(self, **kwargs)


   .. method:: after_training_iteration(self, **kwargs)


   .. method:: before_update(self, **kwargs)


   .. method:: after_update(self, **kwargs)


   .. method:: after_training_epoch(self, **kwargs)


   .. method:: after_training_step(self, **kwargs)


   .. method:: before_eval(self, **kwargs)


   .. method:: before_eval_step(self, **kwargs)


   .. method:: adapt_eval_dataset(self, **kwargs)


   .. method:: eval_epoch(self, **kwargs)


   .. method:: after_eval_step(self, **kwargs)


   .. method:: after_eval(self, **kwargs)


   .. method:: before_eval_iteration(self, **kwargs)


   .. method:: before_eval_forward(self, **kwargs)


   .. method:: after_eval_forward(self, **kwargs)


   .. method:: after_eval_iteration(self, **kwargs)



.. py:class:: Naive(model: Module, optimizer: Optimizer, criterion, train_mb_size: int = 1, train_epochs: int = 1, eval_mb_size: int = None, device=None, plugins: Optional[List[StrategyPlugin]] = None, evaluator: EvaluationPlugin = default_logger)

   Bases: :class:`avalanche.training.strategies.base_strategy.BaseStrategy`

   The simplest (and least effective) Continual Learning strategy. Naive just
   incrementally fine tunes a single model without employing any method
   to contrast the catastrophic forgetting of previous knowledge.
   This strategy does not use task identities.

   Naive is easy to set up and its results are commonly used to show the worst
   performing baseline.

   Creates an instance of the Naive strategy.

   :param model: The model.
   :param optimizer: The optimizer to use.
   :param criterion: The loss criterion to use.
   :param train_mb_size: The train minibatch size. Defaults to 1.
   :param train_epochs: The number of training epochs. Defaults to 1.
   :param eval_mb_size: The eval minibatch size. Defaults to 1.
   :param device: The device to use. Defaults to None (cpu).
   :param plugins: Plugins to be added. Defaults to None.
   :param evaluator: (optional) instance of EvaluationPlugin for logging
       and metric computations.


.. py:class:: CWRStar(model: Module, optimizer: Optimizer, criterion, cwr_layer_name: str, train_mb_size: int = 1, train_epochs: int = 1, eval_mb_size: int = None, device=None, plugins: Optional[List[StrategyPlugin]] = None, evaluator: EvaluationPlugin = default_logger)

   Bases: :class:`avalanche.training.strategies.base_strategy.BaseStrategy`

   CWR* Strategy.
   This strategy does not use task identities.

   :param model: The model.
   :param optimizer: The optimizer to use.
   :param criterion: The loss criterion to use.
   :param cwr_layer_name: name of the CWR layer. Defaults to None, which
       means that the last fully connected layer will be used.
   :param train_mb_size: The train minibatch size. Defaults to 1.
   :param train_epochs: The number of training epochs. Defaults to 1.
   :param eval_mb_size: The eval minibatch size. Defaults to 1.
   :param device: The device to use. Defaults to None (cpu).
   :param plugins: Plugins to be added. Defaults to None.
   :param evaluator: (optional) instance of EvaluationPlugin for logging
       and metric computations.


.. py:class:: Replay(model: Module, optimizer: Optimizer, criterion, mem_size: int = 200, train_mb_size: int = 1, train_epochs: int = 1, eval_mb_size: int = None, device=None, plugins: Optional[List[StrategyPlugin]] = None, evaluator: EvaluationPlugin = default_logger)

   Bases: :class:`avalanche.training.strategies.base_strategy.BaseStrategy`

   Experience replay strategy. See ReplayPlugin for more details.
   This strategy does not use task identities.

   :param model: The model.
   :param optimizer: The optimizer to use.
   :param criterion: The loss criterion to use.
   :param mem_size: replay buffer size.
   :param train_mb_size: The train minibatch size. Defaults to 1.
   :param train_epochs: The number of training epochs. Defaults to 1.
   :param eval_mb_size: The eval minibatch size. Defaults to 1.
   :param device: The device to use. Defaults to None (cpu).
   :param plugins: Plugins to be added. Defaults to None.
   :param evaluator: (optional) instance of EvaluationPlugin for logging
       and metric computations.


.. py:class:: GDumb(model: Module, optimizer: Optimizer, criterion, mem_size: int = 200, train_mb_size: int = 1, train_epochs: int = 1, eval_mb_size: int = None, device=None, plugins: Optional[List[StrategyPlugin]] = None, evaluator: EvaluationPlugin = default_logger)

   Bases: :class:`avalanche.training.strategies.base_strategy.BaseStrategy`

   GDumb strategy. See GDumbPlugin for more details.
   This strategy does not use task identities.

   :param model: The model.
   :param optimizer: The optimizer to use.
   :param criterion: The loss criterion to use.
   :param mem_size: replay buffer size.
   :param train_mb_size: The train minibatch size. Defaults to 1.
   :param train_epochs: The number of training epochs. Defaults to 1.
   :param eval_mb_size: The eval minibatch size. Defaults to 1.
   :param device: The device to use. Defaults to None (cpu).
   :param plugins: Plugins to be added. Defaults to None.
   :param evaluator: (optional) instance of EvaluationPlugin for logging
       and metric computations.


.. py:class:: Cumulative(model: Module, optimizer: Optimizer, criterion, train_mb_size: int = 1, train_epochs: int = 1, eval_mb_size: int = None, device=None, plugins: Optional[List[StrategyPlugin]] = None, evaluator: EvaluationPlugin = default_logger)

   Bases: :class:`avalanche.training.strategies.base_strategy.BaseStrategy`

   Cumulative strategy. At each step,
       train model with data from all previous steps and current step.

   :param model: The model.
   :param optimizer: The optimizer to use.
   :param criterion: The loss criterion to use.
   :param train_mb_size: The train minibatch size. Defaults to 1.
   :param train_epochs: The number of training epochs. Defaults to 1.
   :param eval_mb_size: The eval minibatch size. Defaults to 1.
   :param device: The device to use. Defaults to None (cpu).
   :param plugins: Plugins to be added. Defaults to None.
   :param evaluator: (optional) instance of EvaluationPlugin for logging
       and metric computations.

   .. method:: adapt_train_dataset(self, **kwargs)

      Called after the dataset initialization and before the
      dataloader initialization. Allows to customize the dataset.
      :param kwargs:
      :return:



.. py:class:: LwF(model: Module, optimizer: Optimizer, criterion, alpha: Union[float, Sequence[float]], temperature: float, train_mb_size: int = 1, train_epochs: int = 1, eval_mb_size: int = None, device=None, plugins: Optional[List[StrategyPlugin]] = None, evaluator: EvaluationPlugin = default_logger)

   Bases: :class:`avalanche.training.strategies.base_strategy.BaseStrategy`

   Learning without Forgetting strategy.
       See LwF plugin for details.
       This strategy does not use task identities.

   :param model: The model.
   :param optimizer: The optimizer to use.
   :param criterion: The loss criterion to use.
   :param alpha: distillation hyperparameter. It can be either a float
           number or a list containing alpha for each step.
   :param temperature: softmax temperature for distillation
   :param train_mb_size: The train minibatch size. Defaults to 1.
   :param train_epochs: The number of training epochs. Defaults to 1.
   :param eval_mb_size: The eval minibatch size. Defaults to 1.
   :param device: The device to use. Defaults to None (cpu).
   :param plugins: Plugins to be added. Defaults to None.
   :param evaluator: (optional) instance of EvaluationPlugin for logging
       and metric computations.


.. py:class:: AGEM(model: Module, optimizer: Optimizer, criterion, patterns_per_step: int, sample_size: int = 64, train_mb_size: int = 1, train_epochs: int = 1, eval_mb_size: int = None, device=None, plugins: Optional[List[StrategyPlugin]] = None, evaluator: EvaluationPlugin = default_logger)

   Bases: :class:`avalanche.training.strategies.base_strategy.BaseStrategy`

   Average Gradient Episodic Memory (A-GEM) strategy.
       See AGEM plugin for details.
       This strategy does not use task identities.

   :param model: The model.
   :param optimizer: The optimizer to use.
   :param criterion: The loss criterion to use.
   :param patterns_per_step: number of patterns per step in the memory
   :param sample_size: number of patterns in memory sample when computing
       reference gradient.
   :param train_mb_size: The train minibatch size. Defaults to 1.
   :param train_epochs: The number of training epochs. Defaults to 1.
   :param eval_mb_size: The eval minibatch size. Defaults to 1.
   :param device: The device to use. Defaults to None (cpu).
   :param plugins: Plugins to be added. Defaults to None.
   :param evaluator: (optional) instance of EvaluationPlugin for logging
       and metric computations.


.. py:class:: GEM(model: Module, optimizer: Optimizer, criterion, patterns_per_step: int, memory_strength: float = 0.5, train_mb_size: int = 1, train_epochs: int = 1, eval_mb_size: int = None, device=None, plugins: Optional[List[StrategyPlugin]] = None, evaluator: EvaluationPlugin = default_logger)

   Bases: :class:`avalanche.training.strategies.base_strategy.BaseStrategy`

   Gradient Episodic Memory (GEM) strategy.
       See GEM plugin for details.
       This strategy does not use task identities.

   :param model: The model.
   :param optimizer: The optimizer to use.
   :param criterion: The loss criterion to use.
   :param patterns_per_step: number of patterns per step in the memory
   :param memory_strength: offset to add to the projection direction
       in order to favour backward transfer (gamma in original paper).
   :param train_mb_size: The train minibatch size. Defaults to 1.
   :param train_epochs: The number of training epochs. Defaults to 1.
   :param eval_mb_size: The eval minibatch size. Defaults to 1.
   :param device: The device to use. Defaults to None (cpu).
   :param plugins: Plugins to be added. Defaults to None.
   :param evaluator: (optional) instance of EvaluationPlugin for logging
       and metric computations.


.. py:class:: EWC(model: Module, optimizer: Optimizer, criterion, ewc_lambda: float, mode: str = 'separate', decay_factor: Optional[float] = None, keep_importance_data: bool = False, train_mb_size: int = 1, train_epochs: int = 1, eval_mb_size: int = None, device=None, plugins: Optional[List[StrategyPlugin]] = None, evaluator: EvaluationPlugin = default_logger)

   Bases: :class:`avalanche.training.strategies.base_strategy.BaseStrategy`

   Elastic Weight Consolidation (EWC) strategy.
       See EWC plugin for details.
       This strategy does not use task identities.

   :param model: The model.
   :param optimizer: The optimizer to use.
   :param criterion: The loss criterion to use.
   :param ewc_lambda: hyperparameter to weigh the penalty inside the total
          loss. The larger the lambda, the larger the regularization.
   :param mode: `standard` to keep a separate penalty for each previous
          step. `onlinesum` to keep a single penalty summed over all
          previous tasks. `onlineweightedsum` to keep a single penalty
          summed with a decay factor over all previous tasks.
   :param decay_factor: used only if mode is `onlineweightedsum`.
          It specify the decay term of the importance matrix.
   :param keep_importance_data: if True, keep in memory both parameter
           values and importances for all previous task, for all modes.
           If False, keep only last parameter values and importances.
           If mode is `separate`, the value of `keep_importance_data` is
           set to be True.
   :param train_mb_size: The train minibatch size. Defaults to 1.
   :param train_epochs: The number of training epochs. Defaults to 1.
   :param eval_mb_size: The eval minibatch size. Defaults to 1.
   :param device: The device to use. Defaults to None (cpu).
   :param plugins: Plugins to be added. Defaults to None.
   :param evaluator: (optional) instance of EvaluationPlugin for logging
       and metric computations.


.. py:class:: SynapticIntelligence(model: Module, optimizer: Optimizer, criterion, si_lambda: float, train_mb_size: int = 1, train_epochs: int = 1, eval_mb_size: int = 1, device='cpu', plugins: Optional[Sequence['StrategyPlugin']] = None, evaluator=default_logger)

   Bases: :class:`avalanche.training.strategies.base_strategy.BaseStrategy`

   The Synaptic Intelligence strategy.

   This is the Synaptic Intelligence PyTorch implementation of the
   algorithm described in the paper "Continual Learning Through Synaptic
   Intelligence" (https://arxiv.org/abs/1703.04200).

   The Synaptic Intelligence regularization can also be used in a different
   strategy by applying the :class:`SynapticIntelligencePlugin` plugin.

   Creates an instance of the Synaptic Intelligence strategy.

   :param model: PyTorch model.
   :param optimizer: PyTorch optimizer.
   :param criterion: loss function.
   :param si_lambda: Synaptic Intelligence lambda term.
   :param train_mb_size: mini-batch size for training.
   :param train_epochs: number of training epochs.
   :param eval_mb_size: mini-batch size for eval.
   :param device: PyTorch device to run the model.
   :param plugins: (optional) list of StrategyPlugins.
   :param evaluator: (optional) instance of EvaluationPlugin for logging
       and metric computations.


.. py:class:: AR1(criterion=None, lr: float = 0.001, momentum=0.9, l2=0.0005, train_epochs: int = 4, init_update_rate: float = 0.01, inc_update_rate=5e-05, max_r_max=1.25, max_d_max=0.5, inc_step=4.1e-05, rm_sz: int = 1500, freeze_below_layer: str = 'lat_features.19.bn.beta', latent_layer_num: int = 19, ewc_lambda: float = 0, train_mb_size: int = 128, eval_mb_size: int = 128, device=None, plugins: Optional[Sequence[StrategyPlugin]] = None, evaluator: EvaluationPlugin = default_logger)

   Bases: :class:`avalanche.training.strategies.base_strategy.BaseStrategy`

   The AR1 strategy with Latent Replay.

   This implementations allows for the use of both Synaptic Intelligence and
   Latent Replay to protect the lower level of the model from forgetting.

   While the original papers show how to use those two techniques in a mutual
   exclusive way, this implementation allows for the use of both of them
   concurrently. This behaviour is controlled by passing proper constructor
   arguments).

   Creates an instance of the AR1 strategy.

   :param criterion: The loss criterion to use. Defaults to None, in which
       case the cross entropy loss is used.
   :param lr: The learning rate (SGD optimizer).
   :param momentum: The momentum (SGD optimizer).
   :param l2: The L2 penalty used for weight decay.
   :param train_epochs: The number of training epochs. Defaults to 4.
   :param init_update_rate: The initial update rate of BatchReNorm layers.
   :param inc_update_rate: The incremental update rate of BatchReNorm
       layers.
   :param max_r_max: The maximum r value of BatchReNorm layers.
   :param max_d_max: The maximum d value of BatchReNorm layers.
   :param inc_step: The incremental step of r and d values of BatchReNorm
       layers.
   :param rm_sz: The size of the replay buffer. The replay buffer is shared
       across classes. Defaults to 1500.
   :param freeze_below_layer: A string describing the name of the layer
       to use while freezing the lower (nearest to the input) part of the
       model. The given layer is not frozen (exclusive).
   :param latent_layer_num: The number of the layer to use as the Latent
       Replay Layer. Usually this is the same of `freeze_below_layer`.
   :param ewc_lambda: The Synaptic Intelligence lambda term. Defaults to
       0, which means that the Synaptic Intelligence regularization
       will not be applied.
   :param train_mb_size: The train minibatch size. Defaults to 128.
   :param eval_mb_size: The eval minibatch size. Defaults to 128.
   :param device: The device to use. Defaults to None (cpu).
   :param plugins: (optional) list of StrategyPlugins.
   :param evaluator: (optional) instance of EvaluationPlugin for logging
       and metric computations.

   .. method:: before_training_step(self, **kwargs)

      Called  after the dataset and data loader creation and
      before the training loop.


   .. method:: make_train_dataloader(self, num_workers=0, shuffle=True, **kwargs)

      Called after the dataset instantiation. Initialize the data loader.

      For AR1 a "custom" dataloader is used: instead of using
      `self.train_mb_size` as the batch size, the data loader batch size will
      be computed ad `self.train_mb_size - latent_mb_size`. `latent_mb_size`
      is in turn computed as:

      `
      len(train_dataset) // ((len(train_dataset) + len(replay_buffer)
      // self.train_mb_size)
      `

      so that the number of iterations required to run an epoch on the current
      batch is equal to the number of iterations required to run an epoch
      on the replay buffer.

      :param num_workers: number of thread workers for the data loading.
      :param shuffle: True if the data should be shuffled, False otherwise.


   .. method:: training_epoch(self, **kwargs)

      Training epoch.
      :param kwargs:
      :return:


   .. method:: after_training_step(self, **kwargs)


   .. method:: filter_bn_and_brn(param_def: LayerAndParameter)
      :staticmethod:



