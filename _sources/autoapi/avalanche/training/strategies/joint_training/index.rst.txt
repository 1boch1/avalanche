:mod:`avalanche.training.strategies.joint_training`
===================================================

.. py:module:: avalanche.training.strategies.joint_training


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.training.strategies.joint_training.JointTraining



.. py:class:: JointTraining(model: Module, optimizer: Optimizer, criterion, train_mb_size: int = 1, train_epochs: int = 1, eval_mb_size: int = 1, device='cpu', plugins: Optional[Sequence['StrategyPlugin']] = None, evaluator=default_logger)

   Bases: :class:`avalanche.training.strategies.BaseStrategy`

   JointStrategy performs joint training on the entire stream of data.
   This means that it is not a continual learning strategy but it can be
   used as an "offline" upper bound for them.

   WARNING: JointTraining adapts its own dataset.
   Please check that the plugins you are using do not implement
   `adapt_trainin_dataset`. Otherwise, they are incompatible with
   `JointTraining`.

   :param model: PyTorch model.
   :param optimizer: PyTorch optimizer.
   :param criterion: loss function.
   :param train_mb_size: mini-batch size for training.
   :param train_epochs: number of training epochs.
   :param eval_mb_size: mini-batch size for eval.
   :param device: PyTorch device to run the model.
   :param plugins: (optional) list of StrategyPlugins.
   :param evaluator: (optional) instance of EvaluationPlugin for logging
       and metric computations. None to remove logging.

   .. method:: train(self, step_infos: Union[IStepInfo, Sequence[IStepInfo]], **kwargs)

      Training loop. if step_infos is a single element trains on it.
      If it is a sequence, trains the model on each step in order.
      This is different from joint training on the entire stream.

      :param step_infos: single IStepInfo or sequence.



