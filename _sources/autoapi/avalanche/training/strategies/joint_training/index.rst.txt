:mod:`avalanche.training.strategies.joint_training`
===================================================

.. py:module:: avalanche.training.strategies.joint_training


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   avalanche.training.strategies.joint_training.JointTraining



.. py:class:: JointTraining(model: Module, optimizer: Optimizer, criterion, evaluation_protocol: Optional[EvalProtocol] = None, train_mb_size: int = 1, train_epochs: int = 1, test_mb_size: int = 1, device='cpu', plugins: Optional[Sequence[StrategyPlugin]] = None)

   Bases: :class:`avalanche.training.strategies.BaseStrategy`

   JointStrategy is a super class for all the joint training strategies.
   This means that it is not a continual learning strategy but it can be
   used as an "offline" upper bound for them. This strategy takes in
   input an entire stream and learn from it one shot. It supports unique
   tasks (i.e. streams with a unique task label) or multiple tasks.

   :param model: PyTorch model.
   :param optimizer: PyTorch optimizer.
   :param criterion: loss function.
   :param evaluation_protocol: evaluation plugin.
   :param train_mb_size: mini-batch size for training.
   :param train_epochs: number of training epochs.
   :param test_mb_size: mini-batch size for test.
   :param device: PyTorch device to run the model.
   :param plugins: (optional) list of StrategyPlugins.

   .. method:: train(self, step_infos: Sequence[IStepInfo], **kwargs)

      Training loop. if step_infos is a single element trains on it.
      If it is a sequence, trains the model on each step in order.
      This is different from joint training on the entire stream.

      :param step_infos: single IStepInfo or sequence.
      :return:



