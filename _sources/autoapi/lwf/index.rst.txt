:mod:`lwf`
==========

.. py:module:: lwf


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   lwf/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   lwf.LearningWithoutForgetting



.. py:class:: LearningWithoutForgetting(model, classes_per_task, alpha=0.5, distillation_loss_T=2, warmup_epochs=0, optimizer=None, criterion=torch.nn.CrossEntropyLoss(), mb_size=256, train_ep=2, device=None, preproc=None, eval_protocol=EvalProtocol(metrics=[ACC()]))

   Bases: :class:`avalanche.training.deprecated.strategy.Strategy`

   Learning without Forgetting Strategy.

   paper: https://arxiv.org/abs/1606.09282
   original implementation (Matlab):
   https://github.com/lizhitwo/LearningWithoutForgetting
   reference implementation (pytorch):
   https://github.com/arunmallya/packnet/blob/master/src/lwf.py

   :param model: pytorch basic model.
   :param classes_per_task: number of classes for each task.
   :param alpha: distillation loss coefficient.
   :param distillation_loss_T: distillation loss temperature.
   :param warmup_epochs: number of warmup epochs training only
       the new parameters.
   :param optimizer: pytorch optimizer.
   :param criterion: pytorch optimization criterion.
   :param mb_size: mini-batch size for SGD.
   :param train_ep: training epochs for each task/batch.
   :param device: device on which to run the script.
   :param preproc: preprocessing function.
   :param eval_protocol: avalanche evaluation protocol.

   .. method:: warmup_train(self)

      Train only the new parameters for the first epochs. 


   .. method:: compute_loss(self, logits, y_mb)


   .. method:: before_train(self)


   .. method:: after_train(self)



