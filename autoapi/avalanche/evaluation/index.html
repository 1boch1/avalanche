

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>avalanche.evaluation &mdash; Avalanche 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/css/mystyle.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex/" />
    <link rel="search" title="Search" href="../../../search/" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../" class="icon icon-home" alt="Documentation Home"> Avalanche
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Avalanche API:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../">API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche</span></code></a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../benchmarks/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.benchmarks</span></code></a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../evaluation_deprecated/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation_deprecated</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../extras/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.extras</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../training/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.training</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../#package-contents">Package Contents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../#avalanche.__version__">__version__</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../">Avalanche</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../" class="icon icon-home"></a> &raquo;</li>
        
      <li><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation</span></code></li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/autoapi/avalanche/evaluation/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-avalanche.evaluation">
<span id="avalanche-evaluation"></span><h1><a class="reference internal" href="#module-avalanche.evaluation" title="avalanche.evaluation"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation</span></code></a><a class="headerlink" href="#module-avalanche.evaluation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="metrics/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics</span></code></a><ul>
<li class="toctree-l2"><a class="reference internal" href="metrics/accuracy/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.accuracy</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics/confusion_matrix/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.confusion_matrix</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics/forgetting/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.forgetting</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics/loss/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.loss</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="metrics/timing/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metrics.timing</span></code></a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="abstract_metric/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.abstract_metric</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="evaluation_data/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.evaluation_data</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="metric_definitions/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metric_definitions</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="metric_units/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metric_units</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="metric_utils/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.metric_utils</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="raw_accumulators/"><code class="xref py py-mod docutils literal notranslate"><span class="pre">avalanche.evaluation.raw_accumulators</span></code></a></li>
</ul>
</div>
</div>
<div class="section" id="package-contents">
<h2>Package Contents<a class="headerlink" href="#package-contents" title="Permalink to this headline">¶</a></h2>
<div class="section" id="classes">
<h3>Classes<a class="headerlink" href="#classes" title="Permalink to this headline">¶</a></h3>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.EvalData" title="avalanche.evaluation.EvalData"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EvalData</span></code></a></p></td>
<td><p>The base evaluation data class.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.EvalTestData" title="avalanche.evaluation.EvalTestData"><code class="xref py py-obj docutils literal notranslate"><span class="pre">EvalTestData</span></code></a></p></td>
<td><p>The base evaluation data class for test phase related events.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.OnTrainPhaseStart" title="avalanche.evaluation.OnTrainPhaseStart"><code class="xref py py-obj docutils literal notranslate"><span class="pre">OnTrainPhaseStart</span></code></a></p></td>
<td><p>Evaluation data sent to metrics when a training phase is about to start.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.OnTestPhaseStart" title="avalanche.evaluation.OnTestPhaseStart"><code class="xref py py-obj docutils literal notranslate"><span class="pre">OnTestPhaseStart</span></code></a></p></td>
<td><p>Evaluation data sent to metrics when a test phase is about to start.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.OnTrainPhaseEnd" title="avalanche.evaluation.OnTrainPhaseEnd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">OnTrainPhaseEnd</span></code></a></p></td>
<td><p>Evaluation data sent to metrics when a training phase is ended.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.OnTestPhaseEnd" title="avalanche.evaluation.OnTestPhaseEnd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">OnTestPhaseEnd</span></code></a></p></td>
<td><p>Evaluation data sent to metrics when a test phase is ended.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.OnTrainStepStart" title="avalanche.evaluation.OnTrainStepStart"><code class="xref py py-obj docutils literal notranslate"><span class="pre">OnTrainStepStart</span></code></a></p></td>
<td><p>Evaluation data sent to metrics when a training step is about to start.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.OnTestStepStart" title="avalanche.evaluation.OnTestStepStart"><code class="xref py py-obj docutils literal notranslate"><span class="pre">OnTestStepStart</span></code></a></p></td>
<td><p>Evaluation data sent to metrics when a test step is about to start.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.OnTrainStepEnd" title="avalanche.evaluation.OnTrainStepEnd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">OnTrainStepEnd</span></code></a></p></td>
<td><p>Evaluation data sent to metrics when a training step completes.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.OnTestStepEnd" title="avalanche.evaluation.OnTestStepEnd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">OnTestStepEnd</span></code></a></p></td>
<td><p>Evaluation data sent to metrics when a test step completes.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.OnTrainEpochStart" title="avalanche.evaluation.OnTrainEpochStart"><code class="xref py py-obj docutils literal notranslate"><span class="pre">OnTrainEpochStart</span></code></a></p></td>
<td><p>Evaluation data sent to metrics when a training epoch is about to start.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.OnTrainEpochEnd" title="avalanche.evaluation.OnTrainEpochEnd"><code class="xref py py-obj docutils literal notranslate"><span class="pre">OnTrainEpochEnd</span></code></a></p></td>
<td><p>Evaluation data sent to metrics when a training epoch completes.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.OnTrainIteration" title="avalanche.evaluation.OnTrainIteration"><code class="xref py py-obj docutils literal notranslate"><span class="pre">OnTrainIteration</span></code></a></p></td>
<td><p>Evaluation data sent to metrics when a training iteration (on a minibatch)</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.OnTestIteration" title="avalanche.evaluation.OnTestIteration"><code class="xref py py-obj docutils literal notranslate"><span class="pre">OnTestIteration</span></code></a></p></td>
<td><p>Evaluation data sent to metrics when a test iteration (on a minibatch)</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.Metric" title="avalanche.evaluation.Metric"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Metric</span></code></a></p></td>
<td><p>Protocol definition of a metric.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.AlternativeValues" title="avalanche.evaluation.AlternativeValues"><code class="xref py py-obj docutils literal notranslate"><span class="pre">AlternativeValues</span></code></a></p></td>
<td><p>A container for alternative representations of the same metric value.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#avalanche.evaluation.MetricTypes" title="avalanche.evaluation.MetricTypes"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MetricTypes</span></code></a></p></td>
<td><p>Generic enumeration.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#avalanche.evaluation.MetricValue" title="avalanche.evaluation.MetricValue"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MetricValue</span></code></a></p></td>
<td><p>The result of a Metric.</p></td>
</tr>
</tbody>
</table>
<dl class="py class">
<dt id="avalanche.evaluation.EvalData">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">EvalData</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">step_counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">training_step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">training_task_label</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/evaluation_data/#EvalData"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.EvalData" title="Permalink to this definition">¶</a></dt>
<dd><p>The base evaluation data class.</p>
<p>This class is the root class for all the strategy-related events that can be
captured in order to compute relevant metrics.</p>
<p>This class only defines a step counter, the training step ID and the
training task label.</p>
<p>Initialize self.  See help(type(self)) for accurate signature.</p>
<dl class="py attribute">
<dt id="avalanche.evaluation.EvalData.train_phase">
<code class="sig-name descname">train_phase</code><em class="property"> :bool = True</em><a class="headerlink" href="#avalanche.evaluation.EvalData.train_phase" title="Permalink to this definition">¶</a></dt>
<dd><p>If True, this event refers to the training phase.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.EvalData.step_counter">
<code class="sig-name descname">step_counter</code><em class="property"> :int</em><a class="headerlink" href="#avalanche.evaluation.EvalData.step_counter" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of training steps encountered so far.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.EvalData.training_step_id">
<code class="sig-name descname">training_step_id</code><em class="property"> :int</em><a class="headerlink" href="#avalanche.evaluation.EvalData.training_step_id" title="Permalink to this definition">¶</a></dt>
<dd><p>The training step ID. May be different from the step counter.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.EvalData.training_task_label">
<code class="sig-name descname">training_task_label</code><em class="property"> :int</em><a class="headerlink" href="#avalanche.evaluation.EvalData.training_task_label" title="Permalink to this definition">¶</a></dt>
<dd><p>The training task label.</p>
</dd></dl>

<dl class="py method">
<dt id="avalanche.evaluation.EvalData.test_phase">
<em class="property">property </em><code class="sig-name descname">test_phase</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="headerlink" href="#avalanche.evaluation.EvalData.test_phase" title="Permalink to this definition">¶</a></dt>
<dd><p>If True, this event refers to the test phase.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.EvalTestData">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">EvalTestData</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">step_counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">training_step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">training_task_label</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">test_step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">test_task_label</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/evaluation_data/#EvalTestData"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.EvalTestData" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="evaluation_data/#avalanche.evaluation.evaluation_data.EvalData" title="avalanche.evaluation.evaluation_data.EvalData"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.evaluation_data.EvalData</span></code></a></p>
<p>The base evaluation data class for test phase related events.</p>
<p>This class is the root class for all the strategy-related events of the
test phase.</p>
<p>This class contains all the fields of the EvalData base class, the
test step ID and the test task label.</p>
<p>Initialize self.  See help(type(self)) for accurate signature.</p>
<dl class="py attribute">
<dt id="avalanche.evaluation.EvalTestData.train_phase">
<code class="sig-name descname">train_phase</code><em class="property"> :bool = False</em><a class="headerlink" href="#avalanche.evaluation.EvalTestData.train_phase" title="Permalink to this definition">¶</a></dt>
<dd><p>If True, this event refers to the training phase.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.EvalTestData.test_step_id">
<code class="sig-name descname">test_step_id</code><em class="property"> :int</em><a class="headerlink" href="#avalanche.evaluation.EvalTestData.test_step_id" title="Permalink to this definition">¶</a></dt>
<dd><p>The test step ID.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.EvalTestData.test_task_label">
<code class="sig-name descname">test_task_label</code><em class="property"> :int</em><a class="headerlink" href="#avalanche.evaluation.EvalTestData.test_task_label" title="Permalink to this definition">¶</a></dt>
<dd><p>The test task label.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.OnTrainPhaseStart">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">OnTrainPhaseStart</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">step_counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">training_task_label</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/evaluation_data/#OnTrainPhaseStart"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.OnTrainPhaseStart" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="evaluation_data/#avalanche.evaluation.evaluation_data.EvalData" title="avalanche.evaluation.evaluation_data.EvalData"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.evaluation_data.EvalData</span></code></a></p>
<p>Evaluation data sent to metrics when a training phase is about to start.</p>
<p>Beware that a training phase may involve running the training procedure
on multiple training steps.</p>
<p>The step_counter here refers to the counter as it is before starting the
training phase.</p>
<p>Initialize self.  See help(type(self)) for accurate signature.</p>
</dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.OnTestPhaseStart">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">OnTestPhaseStart</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">step_counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">training_task_label</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">test_step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">test_task_label</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/evaluation_data/#OnTestPhaseStart"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.OnTestPhaseStart" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="evaluation_data/#avalanche.evaluation.evaluation_data.EvalTestData" title="avalanche.evaluation.evaluation_data.EvalTestData"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.evaluation_data.EvalTestData</span></code></a></p>
<p>Evaluation data sent to metrics when a test phase is about to start.</p>
<p>Beware that a test phase usually involves running the test procedure
on multiple test steps.</p>
<p>Initialize self.  See help(type(self)) for accurate signature.</p>
</dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.OnTrainPhaseEnd">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">OnTrainPhaseEnd</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">step_counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">training_task_label</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/evaluation_data/#OnTrainPhaseEnd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.OnTrainPhaseEnd" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="evaluation_data/#avalanche.evaluation.evaluation_data.EvalData" title="avalanche.evaluation.evaluation_data.EvalData"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.evaluation_data.EvalData</span></code></a></p>
<p>Evaluation data sent to metrics when a training phase is ended.</p>
<p>This means that all training steps have completed and the strategy is
about to switch to the test phase.</p>
<p>The step_counter here refers to the counter as it is after all training
steps have completed.</p>
<p>Initialize self.  See help(type(self)) for accurate signature.</p>
</dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.OnTestPhaseEnd">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">OnTestPhaseEnd</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">step_counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">training_task_label</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">test_step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">test_task_label</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/evaluation_data/#OnTestPhaseEnd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.OnTestPhaseEnd" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="evaluation_data/#avalanche.evaluation.evaluation_data.EvalTestData" title="avalanche.evaluation.evaluation_data.EvalTestData"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.evaluation_data.EvalTestData</span></code></a></p>
<p>Evaluation data sent to metrics when a test phase is ended.</p>
<p>This means that all test steps have completed and the strategy is
about to switch to the training phase.</p>
<p>Initialize self.  See help(type(self)) for accurate signature.</p>
</dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.OnTrainStepStart">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">OnTrainStepStart</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">step_counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">training_task_label</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/evaluation_data/#OnTrainStepStart"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.OnTrainStepStart" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="evaluation_data/#avalanche.evaluation.evaluation_data.EvalData" title="avalanche.evaluation.evaluation_data.EvalData"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.evaluation_data.EvalData</span></code></a></p>
<p>Evaluation data sent to metrics when a training step is about to start.</p>
<p>Initialize self.  See help(type(self)) for accurate signature.</p>
</dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.OnTestStepStart">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">OnTestStepStart</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">step_counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">training_task_label</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">test_step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">test_task_label</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/evaluation_data/#OnTestStepStart"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.OnTestStepStart" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="evaluation_data/#avalanche.evaluation.evaluation_data.EvalTestData" title="avalanche.evaluation.evaluation_data.EvalTestData"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.evaluation_data.EvalTestData</span></code></a></p>
<p>Evaluation data sent to metrics when a test step is about to start.</p>
<p>Beware that this type of events also cover the “test epoch start”
checkpoint, as a test step only involves running a single epoch on the test
dataset.</p>
<p>Initialize self.  See help(type(self)) for accurate signature.</p>
</dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.OnTrainStepEnd">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">OnTrainStepEnd</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">step_counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">training_task_label</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/evaluation_data/#OnTrainStepEnd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.OnTrainStepEnd" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="evaluation_data/#avalanche.evaluation.evaluation_data.EvalData" title="avalanche.evaluation.evaluation_data.EvalData"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.evaluation_data.EvalData</span></code></a></p>
<p>Evaluation data sent to metrics when a training step completes.</p>
<p>Initialize self.  See help(type(self)) for accurate signature.</p>
</dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.OnTestStepEnd">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">OnTestStepEnd</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">step_counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">training_task_label</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">test_step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">test_task_label</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/evaluation_data/#OnTestStepEnd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.OnTestStepEnd" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="evaluation_data/#avalanche.evaluation.evaluation_data.EvalTestData" title="avalanche.evaluation.evaluation_data.EvalTestData"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.evaluation_data.EvalTestData</span></code></a></p>
<p>Evaluation data sent to metrics when a test step completes.</p>
<p>Beware that this type of events also cover the “test epoch end”
checkpoint, as a test step only involves running a single epoch on the test
dataset.</p>
<p>Initialize self.  See help(type(self)) for accurate signature.</p>
</dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.OnTrainEpochStart">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">OnTrainEpochStart</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">step_counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">training_task_label</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/evaluation_data/#OnTrainEpochStart"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.OnTrainEpochStart" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="evaluation_data/#avalanche.evaluation.evaluation_data.EvalData" title="avalanche.evaluation.evaluation_data.EvalData"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.evaluation_data.EvalData</span></code></a></p>
<p>Evaluation data sent to metrics when a training epoch is about to start.</p>
<p>Beware that the equivalent “Test” event doesn’t exist.</p>
<p>Initialize self.  See help(type(self)) for accurate signature.</p>
<dl class="py attribute">
<dt id="avalanche.evaluation.OnTrainEpochStart.epoch">
<code class="sig-name descname">epoch</code><em class="property"> :int</em><a class="headerlink" href="#avalanche.evaluation.OnTrainEpochStart.epoch" title="Permalink to this definition">¶</a></dt>
<dd><p>The epoch that is about to start (first epoch = 0).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.OnTrainEpochEnd">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">OnTrainEpochEnd</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">step_counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">training_task_label</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/evaluation_data/#OnTrainEpochEnd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.OnTrainEpochEnd" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="evaluation_data/#avalanche.evaluation.evaluation_data.EvalData" title="avalanche.evaluation.evaluation_data.EvalData"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.evaluation_data.EvalData</span></code></a></p>
<p>Evaluation data sent to metrics when a training epoch completes.</p>
<p>Beware that the equivalent “Test” event doesn’t exist.</p>
<p>Initialize self.  See help(type(self)) for accurate signature.</p>
<dl class="py attribute">
<dt id="avalanche.evaluation.OnTrainEpochEnd.epoch">
<code class="sig-name descname">epoch</code><em class="property"> :int</em><a class="headerlink" href="#avalanche.evaluation.OnTrainEpochEnd.epoch" title="Permalink to this definition">¶</a></dt>
<dd><p>The epoch that just completed (first epoch = 0).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.OnTrainIteration">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">OnTrainIteration</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">step_counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">training_task_label</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">epoch</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">iteration</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">ground_truth</span><span class="p">:</span> <span class="n">Tensor</span></em>, <em class="sig-param"><span class="n">prediction_logits</span><span class="p">:</span> <span class="n">Tensor</span></em>, <em class="sig-param"><span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/evaluation_data/#OnTrainIteration"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.OnTrainIteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="evaluation_data/#avalanche.evaluation.evaluation_data.EvalData" title="avalanche.evaluation.evaluation_data.EvalData"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.evaluation_data.EvalData</span></code></a></p>
<p>Evaluation data sent to metrics when a training iteration (on a minibatch)
completes.</p>
<p>Initialize self.  See help(type(self)) for accurate signature.</p>
<dl class="py attribute">
<dt id="avalanche.evaluation.OnTrainIteration.epoch">
<code class="sig-name descname">epoch</code><em class="property"> :int</em><a class="headerlink" href="#avalanche.evaluation.OnTrainIteration.epoch" title="Permalink to this definition">¶</a></dt>
<dd><p>The current epoch.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.OnTrainIteration.iteration">
<code class="sig-name descname">iteration</code><em class="property"> :int</em><a class="headerlink" href="#avalanche.evaluation.OnTrainIteration.iteration" title="Permalink to this definition">¶</a></dt>
<dd><p>The iteration that just completed (first iteration = 0).</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.OnTrainIteration.ground_truth">
<code class="sig-name descname">ground_truth</code><em class="property"> :Tensor</em><a class="headerlink" href="#avalanche.evaluation.OnTrainIteration.ground_truth" title="Permalink to this definition">¶</a></dt>
<dd><p>A Tensor describing the ground truth for the current minibatch.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.OnTrainIteration.prediction_logits">
<code class="sig-name descname">prediction_logits</code><em class="property"> :Tensor</em><a class="headerlink" href="#avalanche.evaluation.OnTrainIteration.prediction_logits" title="Permalink to this definition">¶</a></dt>
<dd><p>A Tensor describing the prediction logits for the current minibatch.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.OnTrainIteration.loss">
<code class="sig-name descname">loss</code><em class="property"> :Tensor</em><a class="headerlink" href="#avalanche.evaluation.OnTrainIteration.loss" title="Permalink to this definition">¶</a></dt>
<dd><p>A Tensor describing the loss for the current minibatch.</p>
<p>Metrics should be able to handle different reduction types.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.OnTestIteration">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">OnTestIteration</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">step_counter</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">training_task_label</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">test_step_id</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">test_task_label</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">iteration</span><span class="p">:</span> <span class="n">int</span></em>, <em class="sig-param"><span class="n">ground_truth</span><span class="p">:</span> <span class="n">Tensor</span></em>, <em class="sig-param"><span class="n">prediction_logits</span><span class="p">:</span> <span class="n">Tensor</span></em>, <em class="sig-param"><span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/evaluation_data/#OnTestIteration"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.OnTestIteration" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="evaluation_data/#avalanche.evaluation.evaluation_data.EvalTestData" title="avalanche.evaluation.evaluation_data.EvalTestData"><code class="xref py py-class docutils literal notranslate"><span class="pre">avalanche.evaluation.evaluation_data.EvalTestData</span></code></a></p>
<p>Evaluation data sent to metrics when a test iteration (on a minibatch)
completes.</p>
<p>Initialize self.  See help(type(self)) for accurate signature.</p>
<dl class="py attribute">
<dt id="avalanche.evaluation.OnTestIteration.iteration">
<code class="sig-name descname">iteration</code><em class="property"> :int</em><a class="headerlink" href="#avalanche.evaluation.OnTestIteration.iteration" title="Permalink to this definition">¶</a></dt>
<dd><p>The iteration that just completed (first iteration = 0).</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.OnTestIteration.ground_truth">
<code class="sig-name descname">ground_truth</code><em class="property"> :Tensor</em><a class="headerlink" href="#avalanche.evaluation.OnTestIteration.ground_truth" title="Permalink to this definition">¶</a></dt>
<dd><p>A Tensor describing the ground truth for the current minibatch.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.OnTestIteration.prediction_logits">
<code class="sig-name descname">prediction_logits</code><em class="property"> :Tensor</em><a class="headerlink" href="#avalanche.evaluation.OnTestIteration.prediction_logits" title="Permalink to this definition">¶</a></dt>
<dd><p>A Tensor describing the prediction logits for the current minibatch.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.OnTestIteration.loss">
<code class="sig-name descname">loss</code><em class="property"> :Tensor</em><a class="headerlink" href="#avalanche.evaluation.OnTestIteration.loss" title="Permalink to this definition">¶</a></dt>
<dd><p>A Tensor describing the loss for the current minibatch.</p>
<p>Metrics should be able to handle different reduction types.</p>
</dd></dl>

</dd></dl>

<dl class="py data">
<dt id="avalanche.evaluation.MetricResult">
<code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">MetricResult</code><a class="headerlink" href="#avalanche.evaluation.MetricResult" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.Metric">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">Metric</code><a class="reference internal" href="../../../_modules/avalanche/evaluation/metric_definitions/#Metric"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.Metric" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">typing_extensions.Protocol</span></code></p>
<p>Protocol definition of a metric.</p>
<p>A metric simply accepts an evaluation data object containing relevant
information retrieved from the strategy and optionally outputs values
to be logged.</p>
<p>Create and return a new object.  See help(type) for accurate signature.</p>
<dl class="py method">
<dt id="avalanche.evaluation.Metric.__call__">
<code class="sig-name descname">__call__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">eval_data</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#avalanche.evaluation.EvalData" title="avalanche.evaluation.EvalData">EvalData</a></span></em><span class="sig-paren">)</span> &#x2192; <a class="reference internal" href="#avalanche.evaluation.MetricResult" title="avalanche.evaluation.MetricResult">MetricResult</a><a class="reference internal" href="../../../_modules/avalanche/evaluation/metric_definitions/#Metric.__call__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.Metric.__call__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.AlternativeValues">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">AlternativeValues</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">alternatives</span><span class="p">:</span> <span class="n">MetricType</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/metric_definitions/#AlternativeValues"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.AlternativeValues" title="Permalink to this definition">¶</a></dt>
<dd><p>A container for alternative representations of the same metric value.</p>
<p>Initialize self.  See help(type(self)) for accurate signature.</p>
<dl class="py method">
<dt id="avalanche.evaluation.AlternativeValues.best_supported_value">
<code class="sig-name descname">best_supported_value</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">supported_types</span><span class="p">:</span> <span class="n">type</span></em><span class="sig-paren">)</span> &#x2192; Optional<span class="p">[</span>MetricType<span class="p">]</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/metric_definitions/#AlternativeValues.best_supported_value"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.AlternativeValues.best_supported_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Retrieves a supported representation for this metric value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>supported_types</strong> – A list of supported value types.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The best supported representation. Returns None if no supported
representation is found.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.MetricTypes">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">MetricTypes</code><a class="reference internal" href="../../../_modules/avalanche/evaluation/metric_definitions/#MetricTypes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.MetricTypes" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>Generic enumeration.</p>
<p>Derive from this class to define new enumerations.</p>
<p>Create and return a new object.  See help(type) for accurate signature.</p>
<dl class="py attribute">
<dt id="avalanche.evaluation.MetricTypes.OTHER">
<code class="sig-name descname">OTHER</code><a class="headerlink" href="#avalanche.evaluation.MetricTypes.OTHER" title="Permalink to this definition">¶</a></dt>
<dd><p>Value used to flag a metric type that doesn’t fit in any of the other
standard types.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.MetricTypes.ACCURACY">
<code class="sig-name descname">ACCURACY</code><a class="headerlink" href="#avalanche.evaluation.MetricTypes.ACCURACY" title="Permalink to this definition">¶</a></dt>
<dd><p>Used to flag accuracy values.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.MetricTypes.LOSS">
<code class="sig-name descname">LOSS</code><a class="headerlink" href="#avalanche.evaluation.MetricTypes.LOSS" title="Permalink to this definition">¶</a></dt>
<dd><p>Used to flag loss values.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.MetricTypes.FORGETTING">
<code class="sig-name descname">FORGETTING</code><a class="headerlink" href="#avalanche.evaluation.MetricTypes.FORGETTING" title="Permalink to this definition">¶</a></dt>
<dd><p>Used to flag values representing accuracy losses.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.MetricTypes.CONFUSION_MATRIX">
<code class="sig-name descname">CONFUSION_MATRIX</code><a class="headerlink" href="#avalanche.evaluation.MetricTypes.CONFUSION_MATRIX" title="Permalink to this definition">¶</a></dt>
<dd><p>Used to flag confusion matrices.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.MetricTypes.ELAPSED_TIME">
<code class="sig-name descname">ELAPSED_TIME</code><a class="headerlink" href="#avalanche.evaluation.MetricTypes.ELAPSED_TIME" title="Permalink to this definition">¶</a></dt>
<dd><p>Used to flag values describing an elapsed time (usually in seconds).</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.MetricTypes.CPU_USAGE">
<code class="sig-name descname">CPU_USAGE</code><a class="headerlink" href="#avalanche.evaluation.MetricTypes.CPU_USAGE" title="Permalink to this definition">¶</a></dt>
<dd><p>Used to flag values describing the CPU usage.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.MetricTypes.GPU_USAGE">
<code class="sig-name descname">GPU_USAGE</code><a class="headerlink" href="#avalanche.evaluation.MetricTypes.GPU_USAGE" title="Permalink to this definition">¶</a></dt>
<dd><p>Used to flag values describing the GPU usage.</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.MetricTypes.RAM_USAGE">
<code class="sig-name descname">RAM_USAGE</code><a class="headerlink" href="#avalanche.evaluation.MetricTypes.RAM_USAGE" title="Permalink to this definition">¶</a></dt>
<dd><p>Used to flag values describing the RAM usage (usually in MiB).</p>
</dd></dl>

<dl class="py attribute">
<dt id="avalanche.evaluation.MetricTypes.STORAGE_USAGE">
<code class="sig-name descname">STORAGE_USAGE</code><a class="headerlink" href="#avalanche.evaluation.MetricTypes.STORAGE_USAGE" title="Permalink to this definition">¶</a></dt>
<dd><p>Used to flag values describing the storage occupation (usually in MiB).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="avalanche.evaluation.MetricValue">
<em class="property">class </em><code class="sig-prename descclassname">avalanche.evaluation.</code><code class="sig-name descname">MetricValue</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">origin</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#avalanche.evaluation.Metric" title="avalanche.evaluation.Metric">Metric</a></span></em>, <em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">metric_type</span><span class="p">:</span> <span class="n"><a class="reference internal" href="#avalanche.evaluation.MetricTypes" title="avalanche.evaluation.MetricTypes">MetricTypes</a></span></em>, <em class="sig-param"><span class="n">value</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>MetricType<span class="p">, </span><a class="reference internal" href="#avalanche.evaluation.AlternativeValues" title="avalanche.evaluation.AlternativeValues">AlternativeValues</a><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">x_plot</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../_modules/avalanche/evaluation/metric_definitions/#MetricValue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#avalanche.evaluation.MetricValue" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>The result of a Metric.</p>
<p>A result has a name, a value and a “x” position in which the metric value
should be plotted.</p>
<p>The “value” field can also be an instance of “AlternativeValues”, in which
case it means that alternative representations exist for this value. For
instance, the Confusion Matrix can be represented both as a Tensor and as
an Image. It’s up to the Logger, according to its capabilities, decide which
representation to use.</p>
<p>Creates an instance of MetricValue.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>origin</strong> – The originating Metric instance.</p></li>
<li><p><strong>name</strong> – The display name of this value.</p></li>
<li><p><strong>metric_type</strong> – The type of this metric value, as a element
from the  MetricTypes enumeration.</p></li>
<li><p><strong>value</strong> – </p></li>
<li><p><strong>x_plot</strong> – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, ContinualAI Research

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>